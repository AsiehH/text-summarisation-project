{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4520aebd",
   "metadata": {},
   "source": [
    "# Part 0 - Data preparation\n",
    "\n",
    "In this notebook we will download the arXiv dataset and save it to S3. We will also do some light data preprocessing by only keeping the columns we need, filtering out reviews that are too short, and limiting the size of the datasets.\n",
    "\n",
    "To read more, please check out https://towardsdatascience.com/setting-up-a-text-summarisation-project-introduction-526622eea4a8."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daa779b",
   "metadata": {},
   "source": [
    "First of all we want to make sure that the relevent libraries are installed on this machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4a2ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"sagemaker>=2.48.0\" \"transformers==4.6.1\" \"datasets[s3]==1.6.2\" --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afccee70",
   "metadata": {},
   "source": [
    "We will download the dataset directly from the Kaggle website so we need to install the Kaggle Python package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ee46f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833a4b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KAGGLE_USERNAME'] = \"<your-kaggle-username>\"\n",
    "os.environ['KAGGLE_KEY'] = \"<your-kaggle-api-key>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec24586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle\n",
    "kaggle.api.authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d6d840",
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle.api.dataset_download_files('Cornell-University/arxiv', path=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2371d9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip arxiv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562f9f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"arxiv_dataset\", data_dir='.', split='train', ignore_verifications=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57d8f99",
   "metadata": {},
   "source": [
    "The original dataset is too long, so we shuffle it and limit the number of articles to 25,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6a0a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset = dataset.select(range(25000))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d836ff7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0820fe52",
   "metadata": {},
   "outputs": [],
   "source": [
    " # only keep columns that are required\n",
    "df = df[['abstract', 'title']]\n",
    "df = df.rename(columns={\"abstract\": \"text\", \"title\": \"summary\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561d042c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace(r'\\n',' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1c791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9442eed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83516d63",
   "metadata": {},
   "source": [
    "## Filtering the dataset\n",
    "\n",
    "We want to discard reviews and titles that are too short, so that our model can produce more interesting summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c355d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_summary = 5\n",
    "cutoff_text = 20\n",
    "df = df[(df['summary'].apply(lambda x: len(x.split()) >= cutoff_summary)) & (df['text'].apply(lambda x: len(x.split()) >= cutoff_text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2cccb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc588f20",
   "metadata": {},
   "source": [
    "## Limiting the size of the datasets and splitting\n",
    "\n",
    "We want to limit the size of the datasets so that training of the model can finish in a reasonable amount of time. This is a decision that we might want to revisit in the experimentation phase if we want to increase the performance of the model. We then split the dataset into test (80%), validation (10%), and test (10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeddb089",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(20000, random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a710f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# split the dataset into train, val, and test\n",
    "df_train, df_val, df_test = np.split(df.sample(frac=1), [int(0.8*len(df)), int((0.9)*len(df))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0088b69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('data/train.csv', index=False)\n",
    "df_val.to_csv('data/val.csv', index=False)\n",
    "df_test.to_csv('data/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1ed45c",
   "metadata": {},
   "source": [
    "## Save the data as CSV files and upload them to S3\n",
    "\n",
    "We need to upload the data to S3 in order to train the model at a later point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db34ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a51a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp data/train.csv s3://$bucket/summarization/data/train.csv\n",
    "!aws s3 cp data/val.csv s3://$bucket/summarization/data/val.csv\n",
    "!aws s3 cp data/test.csv s3://$bucket/summarization/data/test.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p37",
   "language": "python",
   "name": "conda_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
