{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7afe787",
   "metadata": {},
   "source": [
    "# Part 4b - Model testing locally\n",
    "\n",
    "Instead of deploying the model to a Sagemaker endpoint where it runs all the time we can also just download the model from S3 and run it locally in our notebook. This is very similar to what we did when we ran the zero-shot model. However, in this case we will not have the luxury to have the pipeline pre- and post-process the data for us. So we will do it ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188adc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sess.default_bucket()\n",
    "client = boto3.client('sagemaker')\n",
    "\n",
    "print(f\"IAM role arn used for running training: {role}\")\n",
    "print(f\"S3 bucket used for storing artifacts: {sess.default_bucket()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327992ca",
   "metadata": {},
   "source": [
    "We locate the S3 location of the model, download, and extract it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8d7bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job = client.list_training_jobs()['TrainingJobSummaries'][0]['TrainingJobName']\n",
    "model_data = sess.describe_training_job(training_job)['ModelArtifacts']['S3ModelArtifacts']\n",
    "model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2400e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c870aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $model_data model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc8ba2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xf model/model.tar.gz -C model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727f25ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r model/checkpoint*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ab1369",
   "metadata": {},
   "source": [
    "Now we load the model into a model object so we can create summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcf3e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8879ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('./model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6884c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained('./model/').to('cpu').eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddcda78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_test = pd.read_csv('data/test.csv')\n",
    "ref_summaries = list(df_test['summary'])\n",
    "texts = list(df_test['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d7adf1",
   "metadata": {},
   "source": [
    "Let's test it to see if it worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d826688",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097bb9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(texts[0], truncation=True, padding='longest', return_tensors=\"pt\").input_ids.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7ddcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(input_ids, max_length=20)\n",
    "predictions = tokenizer.batch_decode(output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db46f965",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958a0e32",
   "metadata": {},
   "source": [
    "Now let's do it for all texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4868feaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_summaries = []\n",
    "\n",
    "for i, text in enumerate(texts):\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    input_ids = tokenizer(text, truncation=True, padding='longest', return_tensors=\"pt\").input_ids.to('cpu')\n",
    "    output = model.generate(input_ids, max_length=20)\n",
    "    predictions = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "    candidate_summaries.append(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594d6301",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"model-summaries.txt\", \"w\")\n",
    "for s in candidate_summaries:\n",
    "    file.write(s + \"\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47eb4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c340ca9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rouge_scores(candidates, references):\n",
    "    result = metric.compute(predictions=candidates, references=references, use_stemmer=True)\n",
    "    result = {key: round(value.mid.fmeasure * 100, 1) for key, value in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5102807",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_rouge_scores(candidate_summaries, ref_summaries)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p37",
   "language": "python",
   "name": "conda_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
