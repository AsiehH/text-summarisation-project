{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbe3b18e",
   "metadata": {},
   "source": [
    "# Part 3 - Training (aka *fine-tuning*) a Transformer model\n",
    "\n",
    "In this part we will finally train our very own Transformers model. We saw that the zer-shot model didn't produce great results, and that's probably because the model was trained on summarising news articles, not academic papers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b834a24f",
   "metadata": {},
   "source": [
    "These lines of code are typical setup for Sagemaker, we require them for training jobs: https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98f33fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IAM role arn used for running training: arn:aws:iam::905847418383:role/service-role/AmazonSageMaker-ExecutionRole-20211005T160629\n",
      "S3 bucket used for storing artifacts: sagemaker-us-east-1-905847418383\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "print(f\"IAM role arn used for running training: {role}\")\n",
    "print(f\"S3 bucket used for storing artifacts: {sess.default_bucket()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9ae815",
   "metadata": {},
   "source": [
    "We are in the great position that we don't have to write our own training script. Instead we will use a script from the transformers library in Github: https://github.com/huggingface/transformers/blob/v4.6.1/examples/pytorch/summarization/run_summarization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "175eccf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "git_config = {'repo': 'https://github.com/huggingface/transformers.git','branch': 'v4.6.1'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2e59f6",
   "metadata": {},
   "source": [
    "These rae the parameters for training, and this is one of the most important levers we can leverage once we are in the experimentation phase. Changing these parameters can influence the model performance and there will be a component of trial & error to find the best model. Also check out https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html for automated hyperparameter tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4da10fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={'per_device_train_batch_size': 4,\n",
    "                 'per_device_eval_batch_size': 4,\n",
    "                 'model_name_or_path': 'sshleifer/distilbart-cnn-12-6',\n",
    "                 'train_file': '/opt/ml/input/data/datasets/train.csv',\n",
    "                 'validation_file': '/opt/ml/input/data/datasets/val.csv',\n",
    "                 'do_train': True,\n",
    "                 'do_eval': True,\n",
    "                 'do_predict': False,\n",
    "                 'predict_with_generate': True,\n",
    "                 'output_dir': '/opt/ml/model',\n",
    "                 'num_train_epochs': 3,\n",
    "                 'learning_rate': 5e-5,\n",
    "                 'seed': 7,\n",
    "                 'fp16': True,\n",
    "                 'val_max_target_length': 20,\n",
    "                 'text_column': 'text',\n",
    "                 'summary_column': 'summary',\n",
    "                 }\n",
    "\n",
    "# configuration for running training on smdistributed Data Parallel\n",
    "distribution = {'smdistributed':{'dataparallel':{ 'enabled': True }}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7e60d03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "      entry_point='run_summarization.py',\n",
    "      source_dir='./examples/pytorch/summarization',\n",
    "      git_config=git_config,\n",
    "      instance_type='ml.p3.16xlarge',\n",
    "      instance_count=2,\n",
    "      transformers_version='4.6',\n",
    "      pytorch_version='1.7',\n",
    "      py_version='py36',\n",
    "      role=role,\n",
    "      hyperparameters = hyperparameters,\n",
    "      distribution = distribution\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5853e25",
   "metadata": {},
   "source": [
    "This will kick off the training job which should take ~45 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c1b4db3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into '/tmp/tmpx7p5b1co'...\n",
      "Note: switching to 'v4.6.1'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by switching back to a branch.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -c with the switch command. Example:\n",
      "\n",
      "  git switch -c <new-branch-name>\n",
      "\n",
      "Or undo this operation with:\n",
      "\n",
      "  git switch -\n",
      "\n",
      "Turn off this advice by setting config variable advice.detachedHead to false\n",
      "\n",
      "HEAD is now at fb27b276e Release: v4.6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-01 09:15:54 Starting - Starting the training job...\n",
      "2021-12-01 09:16:04 Starting - Launching requested ML instancesProfilerReport-1638350145: InProgress\n",
      ".........\n",
      "2021-12-01 09:17:49 Starting - Preparing the instances for training.........\n",
      "2021-12-01 09:19:12 Downloading - Downloading input data...\n",
      "2021-12-01 09:19:51 Training - Downloading the training image............\n",
      "2021-12-01 09:21:56 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-12-01 09:21:56,897 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-12-01 09:21:56,975 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2021-12-01 09:21:57,448 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2021-12-01 09:21:57,526 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-12-01 09:21:59,999 sagemaker_pytorch_container.training INFO     Invoking SMDataParallel\u001b[0m\n",
      "\u001b[34m2021-12-01 09:21:59,999 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-12-01 09:22:00,335 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[35m2021-12-01 09:22:00,547 sagemaker_pytorch_container.training INFO     Invoking SMDataParallel\u001b[0m\n",
      "\u001b[35m2021-12-01 09:22:00,547 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2021-12-01 09:22:01,004 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets>=1.1.3 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (1.6.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (0.1.91)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (3.17.1)\u001b[0m\n",
      "\u001b[34mCollecting rouge-score\n",
      "  Downloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\u001b[0m\n",
      "\u001b[34mCollecting nltk\n",
      "  Downloading nltk-3.6.5-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34mCollecting py7zr\n",
      "  Downloading py7zr-0.16.3-py3-none-any.whl (67 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.3 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 7)) (1.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (1.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<0.1.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (0.0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (2.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm<4.50.0,>=4.27 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (4.49.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (0.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (2021.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (0.70.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (20.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (2.25.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=1.0.0<4.0.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (4.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (4.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from torch>=1.3->-r requirements.txt (line 7)) (3.10.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from huggingface-hub<0.1.0->datasets>=1.1.3->-r requirements.txt (line 1)) (3.0.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (2020.12.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (3.0.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: datasets>=1.1.3 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (1.6.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (0.1.91)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: protobuf in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (3.17.1)\u001b[0m\n",
      "\u001b[35mCollecting rouge-score\n",
      "  Downloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\u001b[0m\n",
      "\u001b[35mCollecting nltk\n",
      "  Downloading nltk-3.6.5-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[35mCollecting py7zr\n",
      "  Downloading py7zr-0.16.3-py3-none-any.whl (67 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: torch>=1.3 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 7)) (1.7.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (20.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.9 in /opt/conda/lib/python3.6/site-packages (from protobuf->-r requirements.txt (line 3)) (1.16.0)\u001b[0m\n",
      "\u001b[34mCollecting absl-py\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\u001b[0m\n",
      "\u001b[34mCollecting regex>=2021.8.3\n",
      "  Downloading regex-2021.11.10-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (748 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from nltk->-r requirements.txt (line 5)) (7.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from nltk->-r requirements.txt (line 5)) (1.0.1)\u001b[0m\n",
      "\u001b[34mCollecting pyzstd>=0.14.4\n",
      "  Downloading pyzstd-0.15.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\u001b[0m\n",
      "\u001b[34mCollecting pycryptodomex>=3.6.6\n",
      "  Downloading pycryptodomex-3.11.0-cp35-abi3-manylinux2010_x86_64.whl (1.9 MB)\u001b[0m\n",
      "\u001b[34mCollecting pybcj>=0.5.0\n",
      "  Downloading pybcj-0.5.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyppmd>=0.17.0\n",
      "  Downloading pyppmd-0.17.3-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (125 kB)\u001b[0m\n",
      "\u001b[34mCollecting brotli>=1.0.9\n",
      "  Downloading Brotli-1.0.9-cp36-cp36m-manylinux1_x86_64.whl (357 kB)\u001b[0m\n",
      "\u001b[34mCollecting multivolumefile>=0.2.3\n",
      "  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\u001b[0m\n",
      "\u001b[34mCollecting texttable\n",
      "  Downloading texttable-1.6.4-py2.py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->datasets>=1.1.3->-r requirements.txt (line 1)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->datasets>=1.1.3->-r requirements.txt (line 1)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets>=1.1.3->-r requirements.txt (line 1)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets>=1.1.3->-r requirements.txt (line 1)) (2021.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: xxhash in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (2.0.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: huggingface-hub<0.1.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (0.0.8)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: fsspec in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (2021.5.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tqdm<4.50.0,>=4.27 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (4.49.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: dill in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (0.3.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (2.25.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (1.1.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (0.8)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (0.70.11.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (4.0.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyarrow>=1.0.0<4.0.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (4.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from torch>=1.3->-r requirements.txt (line 7)) (3.10.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from huggingface-hub<0.1.0->datasets>=1.1.3->-r requirements.txt (line 1)) (3.0.12)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (2020.12.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (1.25.11)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (2.10)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (3.0.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six>=1.9 in /opt/conda/lib/python3.6/site-packages (from protobuf->-r requirements.txt (line 3)) (1.16.0)\u001b[0m\n",
      "\u001b[35mCollecting absl-py\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from nltk->-r requirements.txt (line 5)) (1.0.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from nltk->-r requirements.txt (line 5)) (7.1.2)\u001b[0m\n",
      "\u001b[35mCollecting regex>=2021.8.3\n",
      "  Downloading regex-2021.11.10-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (748 kB)\u001b[0m\n",
      "\u001b[35mCollecting pycryptodomex>=3.6.6\n",
      "  Downloading pycryptodomex-3.11.0-cp35-abi3-manylinux2010_x86_64.whl (1.9 MB)\u001b[0m\n",
      "\u001b[35mCollecting pyppmd>=0.17.0\n",
      "  Downloading pyppmd-0.17.3-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (125 kB)\u001b[0m\n",
      "\u001b[35mCollecting texttable\n",
      "  Downloading texttable-1.6.4-py2.py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[35mCollecting pybcj>=0.5.0\n",
      "  Downloading pybcj-0.5.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47 kB)\u001b[0m\n",
      "\u001b[35mCollecting brotli>=1.0.9\n",
      "  Downloading Brotli-1.0.9-cp36-cp36m-manylinux1_x86_64.whl (357 kB)\u001b[0m\n",
      "\u001b[35mCollecting multivolumefile>=0.2.3\n",
      "  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\u001b[0m\n",
      "\u001b[35mCollecting pyzstd>=0.14.4\n",
      "  Downloading pyzstd-0.15.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->datasets>=1.1.3->-r requirements.txt (line 1)) (3.4.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->datasets>=1.1.3->-r requirements.txt (line 1)) (2.4.7)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets>=1.1.3->-r requirements.txt (line 1)) (2.8.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets>=1.1.3->-r requirements.txt (line 1)) (2021.1)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: regex, texttable, pyzstd, pyppmd, pycryptodomex, pybcj, nltk, multivolumefile, brotli, absl-py, rouge-score, py7zr\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2021.4.4\n",
      "    Uninstalling regex-2021.4.4:\n",
      "      Successfully uninstalled regex-2021.4.4\u001b[0m\n",
      "\u001b[35mInstalling collected packages: regex, texttable, pyzstd, pyppmd, pycryptodomex, pybcj, nltk, multivolumefile, brotli, absl-py, rouge-score, py7zr\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2021.4.4\n",
      "    Uninstalling regex-2021.4.4:\n",
      "      Successfully uninstalled regex-2021.4.4\u001b[0m\n",
      "\u001b[34mSuccessfully installed absl-py-1.0.0 brotli-1.0.9 multivolumefile-0.2.3 nltk-3.6.5 py7zr-0.16.3 pybcj-0.5.0 pycryptodomex-3.11.0 pyppmd-0.17.3 pyzstd-0.15.0 regex-2021.11.10 rouge-score-0.0.4 texttable-1.6.4\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2021-12-01 09:22:07,341 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2021-12-01 09:22:07,341 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2021-12-01 09:22:07,344 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2021-12-01 09:22:07,346 sagemaker-training-toolkit INFO     Cannot connect to host algo-2 at port 22. Retrying...\u001b[0m\n",
      "\u001b[34m2021-12-01 09:22:07,346 sagemaker-training-toolkit INFO     Connection closed\u001b[0m\n",
      "\u001b[35mSuccessfully installed absl-py-1.0.0 brotli-1.0.9 multivolumefile-0.2.3 nltk-3.6.5 py7zr-0.16.3 pybcj-0.5.0 pycryptodomex-3.11.0 pyppmd-0.17.3 pyzstd-0.15.0 regex-2021.11.10 rouge-score-0.0.4 texttable-1.6.4\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35m2021-12-01 09:22:08,058 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[35m2021-12-01 09:22:08,059 sagemaker-training-toolkit INFO     Waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[35m2021-12-01 09:22:08,067 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[35m2021-12-01 09:22:08,137 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[35m2021-12-01 09:22:08,138 sagemaker-training-toolkit INFO     Can connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-12-01 09:22:08,138 sagemaker-training-toolkit INFO     MPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[35m2021-12-01 09:22:08,138 sagemaker-training-toolkit INFO     Writing environment variables to /etc/environment for the MPI process.\u001b[0m\n",
      "\u001b[35m2021-12-01 09:22:08,142 sagemaker-training-toolkit INFO     Waiting for MPI process to finish.\u001b[0m\n",
      "\u001b[34m2021-12-01 09:22:08,354 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[34m2021-12-01 09:22:08,429 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[34m2021-12-01 09:22:08,429 sagemaker-training-toolkit INFO     Can connect to host algo-2 at port 22\u001b[0m\n",
      "\u001b[34m2021-12-01 09:22:08,429 sagemaker-training-toolkit INFO     Connection closed\u001b[0m\n",
      "\u001b[34m2021-12-01 09:22:08,429 sagemaker-training-toolkit INFO     Worker algo-2 available for communication\u001b[0m\n",
      "\u001b[34m2021-12-01 09:22:08,429 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2021-12-01 09:22:08,429 sagemaker-training-toolkit INFO     Host: ['algo-1', 'algo-2']\u001b[0m\n",
      "\u001b[34m2021-12-01 09:22:08,430 sagemaker-training-toolkit INFO     instance type: ml.p3.16xlarge\u001b[0m\n",
      "\u001b[34m2021-12-01 09:22:08,510 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_distributed_dataparallel_custom_mpi_options\": \"\",\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": true,\n",
      "        \"sagemaker_instance_type\": \"ml.p3.16xlarge\"\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"datasets\": \"/opt/ml/input/data/datasets\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"per_device_eval_batch_size\": 4,\n",
      "        \"seed\": 7,\n",
      "        \"validation_file\": \"/opt/ml/input/data/datasets/val.csv\",\n",
      "        \"do_predict\": false,\n",
      "        \"text_column\": \"text\",\n",
      "        \"num_train_epochs\": 3,\n",
      "        \"do_eval\": true,\n",
      "        \"output_dir\": \"/opt/ml/model\",\n",
      "        \"fp16\": true,\n",
      "        \"predict_with_generate\": true,\n",
      "        \"do_train\": true,\n",
      "        \"val_max_target_length\": 20,\n",
      "        \"train_file\": \"/opt/ml/input/data/datasets/train.csv\",\n",
      "        \"per_device_train_batch_size\": 4,\n",
      "        \"learning_rate\": 5e-05,\n",
      "        \"model_name_or_path\": \"sshleifer/distilbart-cnn-12-6\",\n",
      "        \"summary_column\": \"summary\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"datasets\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2021-12-01-09-15-45-087\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-905847418383/huggingface-pytorch-training-2021-12-01-09-15-45-087/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_summarization\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_summarization.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"do_eval\":true,\"do_predict\":false,\"do_train\":true,\"fp16\":true,\"learning_rate\":5e-05,\"model_name_or_path\":\"sshleifer/distilbart-cnn-12-6\",\"num_train_epochs\":3,\"output_dir\":\"/opt/ml/model\",\"per_device_eval_batch_size\":4,\"per_device_train_batch_size\":4,\"predict_with_generate\":true,\"seed\":7,\"summary_column\":\"summary\",\"text_column\":\"text\",\"train_file\":\"/opt/ml/input/data/datasets/train.csv\",\"val_max_target_length\":20,\"validation_file\":\"/opt/ml/input/data/datasets/val.csv\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_summarization.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_custom_mpi_options\":\"\",\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"ml.p3.16xlarge\"}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"datasets\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"datasets\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_summarization\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-905847418383/huggingface-pytorch-training-2021-12-01-09-15-45-087/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_custom_mpi_options\":\"\",\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"ml.p3.16xlarge\"},\"channel_input_dirs\":{\"datasets\":\"/opt/ml/input/data/datasets\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"do_eval\":true,\"do_predict\":false,\"do_train\":true,\"fp16\":true,\"learning_rate\":5e-05,\"model_name_or_path\":\"sshleifer/distilbart-cnn-12-6\",\"num_train_epochs\":3,\"output_dir\":\"/opt/ml/model\",\"per_device_eval_batch_size\":4,\"per_device_train_batch_size\":4,\"predict_with_generate\":true,\"seed\":7,\"summary_column\":\"summary\",\"text_column\":\"text\",\"train_file\":\"/opt/ml/input/data/datasets/train.csv\",\"val_max_target_length\":20,\"validation_file\":\"/opt/ml/input/data/datasets/val.csv\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"datasets\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-pytorch-training-2021-12-01-09-15-45-087\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-905847418383/huggingface-pytorch-training-2021-12-01-09-15-45-087/source/sourcedir.tar.gz\",\"module_name\":\"run_summarization\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_summarization.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--do_eval\",\"True\",\"--do_predict\",\"False\",\"--do_train\",\"True\",\"--fp16\",\"True\",\"--learning_rate\",\"5e-05\",\"--model_name_or_path\",\"sshleifer/distilbart-cnn-12-6\",\"--num_train_epochs\",\"3\",\"--output_dir\",\"/opt/ml/model\",\"--per_device_eval_batch_size\",\"4\",\"--per_device_train_batch_size\",\"4\",\"--predict_with_generate\",\"True\",\"--seed\",\"7\",\"--summary_column\",\"summary\",\"--text_column\",\"text\",\"--train_file\",\"/opt/ml/input/data/datasets/train.csv\",\"--val_max_target_length\",\"20\",\"--validation_file\",\"/opt/ml/input/data/datasets/val.csv\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_DATASETS=/opt/ml/input/data/datasets\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=7\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION_FILE=/opt/ml/input/data/datasets/val.csv\u001b[0m\n",
      "\u001b[34mSM_HP_DO_PREDICT=false\u001b[0m\n",
      "\u001b[34mSM_HP_TEXT_COLUMN=text\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=3\u001b[0m\n",
      "\u001b[34mSM_HP_DO_EVAL=true\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_HP_FP16=true\u001b[0m\n",
      "\u001b[34mSM_HP_PREDICT_WITH_GENERATE=true\u001b[0m\n",
      "\u001b[34mSM_HP_DO_TRAIN=true\u001b[0m\n",
      "\u001b[34mSM_HP_VAL_MAX_TARGET_LENGTH=20\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_FILE=/opt/ml/input/data/datasets/train.csv\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=5e-05\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=sshleifer/distilbart-cnn-12-6\u001b[0m\n",
      "\u001b[34mSM_HP_SUMMARY_COLUMN=summary\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1:8,algo-2:8 -np 16 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -mca plm_rsh_num_concurrent 2 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_HOMOGENEOUS=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/opt/conda/lib/python3.6/site-packages/gethostname.cpython-36m-x86_64-linux-gnu.so -x SMDATAPARALLEL_SERVER_ADDR=algo-1 -x SMDATAPARALLEL_SERVER_PORT=7592 -x SAGEMAKER_INSTANCE_TYPE=ml.p3.16xlarge smddprun /opt/conda/bin/python3.6 -m mpi4py run_summarization.py --do_eval True --do_predict False --do_train True --fp16 True --learning_rate 5e-05 --model_name_or_path sshleifer/distilbart-cnn-12-6 --num_train_epochs 3 --output_dir /opt/ml/model --per_device_eval_batch_size 4 --per_device_train_batch_size 4 --predict_with_generate True --seed 7 --summary_column summary --text_column text --train_file /opt/ml/input/data/datasets/train.csv --val_max_target_length 20 --validation_file /opt/ml/input/data/datasets/val.csv\u001b[0m\n",
      "\u001b[35m2021-12-01 09:22:10,149 sagemaker-training-toolkit INFO     Process[es]: [psutil.Process(pid=56, name='orted', status='sleeping', started='09:22:08')]\u001b[0m\n",
      "\u001b[35m2021-12-01 09:22:10,149 sagemaker-training-toolkit INFO     Orted process found [psutil.Process(pid=56, name='orted', status='sleeping', started='09:22:08')]\u001b[0m\n",
      "\u001b[35m2021-12-01 09:22:10,149 sagemaker-training-toolkit INFO     Waiting for orted process [psutil.Process(pid=56, name='orted', status='sleeping', started='09:22:08')]\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:63:63 [0] NCCL INFO Bootstrap : Using [0]eth0:10.2.219.113<0>\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:63:63 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO Bootstrap : Using [0]eth0:10.2.241.86<0>\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:63:63 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:63:63 [0] NCCL INFO NET/Socket : Using [0]eth0:10.2.219.113<0>\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:63:63 [0] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:NCCL version 2.7.8+cuda11.0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO NET/Socket : Using [0]eth0:10.2.241.86<0>\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:NCCL version 2.7.8+cuda11.0\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:58:58 [1] NCCL INFO Bootstrap : Using [0]eth0:10.2.241.86<0>\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:60 [2] NCCL INFO Bootstrap : Using [0]eth0:10.2.241.86<0>\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:62:62 [3] NCCL INFO Bootstrap : Using [0]eth0:10.2.241.86<0>\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:60 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:58:58 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:62:62 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:60 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:58:58 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:62:62 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:58:58 [1] NCCL INFO NET/Socket : Using [0]eth0:10.2.241.86<0>\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:58:58 [1] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:60 [2] NCCL INFO NET/Socket : Using [0]eth0:10.2.241.86<0>\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:60 [2] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:62:62 [3] NCCL INFO NET/Socket : Using [0]eth0:10.2.241.86<0>\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:62:62 [3] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:65:65 [1] NCCL INFO Bootstrap : Using [0]eth0:10.2.219.113<0>\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:67:67 [2] NCCL INFO Bootstrap : Using [0]eth0:10.2.219.113<0>\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:69:69 [3] NCCL INFO Bootstrap : Using [0]eth0:10.2.219.113<0>\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:69:69 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:65:65 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:67:67 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:69:69 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:65:65 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:67:67 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:65:65 [1] NCCL INFO NET/Socket : Using [0]eth0:10.2.219.113<0>\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:65:65 [1] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:69:69 [3] NCCL INFO NET/Socket : Using [0]eth0:10.2.219.113<0>\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:69:69 [3] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:67:67 [2] NCCL INFO NET/Socket : Using [0]eth0:10.2.219.113<0>\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:67:67 [2] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:64:64 [4] NCCL INFO Bootstrap : Using [0]eth0:10.2.241.86<0>\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:66:66 [5] NCCL INFO Bootstrap : Using [0]eth0:10.2.241.86<0>\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:64:64 [4] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:66:66 [5] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:68:68 [7] NCCL INFO Bootstrap : Using [0]eth0:10.2.241.86<0>\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:67:67 [6] NCCL INFO Bootstrap : Using [0]eth0:10.2.241.86<0>\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:68:68 [7] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:67:67 [6] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:66:66 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:64:64 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:75:75 [7] NCCL INFO Bootstrap : Using [0]eth0:10.2.219.113<0>\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:74:74 [6] NCCL INFO Bootstrap : Using [0]eth0:10.2.219.113<0>\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:74:74 [6] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:75:75 [7] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:66:66 [5] NCCL INFO NET/Socket : Using [0]eth0:10.2.241.86<0>\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:66:66 [5] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:64:64 [4] NCCL INFO NET/Socket : Using [0]eth0:10.2.241.86<0>\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:64:64 [4] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:71:71 [4] NCCL INFO Bootstrap : Using [0]eth0:10.2.219.113<0>\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:73:73 [5] NCCL INFO Bootstrap : Using [0]eth0:10.2.219.113<0>\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:73:73 [5] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:71:71 [4] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:75:75 [7] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:67:67 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:68:68 [7] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:74:74 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:73:73 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:74:74 [6] NCCL INFO NET/Socket : Using [0]eth0:10.2.219.113<0>\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:74:74 [6] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:75:75 [7] NCCL INFO NET/Socket : Using [0]eth0:10.2.219.113<0>\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:75:75 [7] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:73:73 [5] NCCL INFO NET/Socket : Using [0]eth0:10.2.219.113<0>\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:73:73 [5] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:71:71 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:68:68 [7] NCCL INFO NET/Socket : Using [0]eth0:10.2.241.86<0>\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:68:68 [7] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:67:67 [6] NCCL INFO NET/Socket : Using [0]eth0:10.2.241.86<0>\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:67:67 [6] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:71:71 [4] NCCL INFO NET/Socket : Using [0]eth0:10.2.219.113<0>\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:71:71 [4] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:60 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:60 [2] NCCL INFO Trees [0] 1/-1/-1->2->3|3->2->1/-1/-1 [1] 1/-1/-1->2->3|3->2->1/-1/-1 [2] 3/-1/-1->2->1|1->2->3/-1/-1 [3] 3/-1/-1->2->1|1->2->3/-1/-1 [4] -1/-1/-1->2->6|6->2->-1/-1/-1 [5] 6/-1/-1->2->0|0->2->6/-1/-1 [6] 1/-1/-1->2->3|3->2->1/-1/-1 [7] 1/-1/-1->2->3|3->2->1/-1/-1 [8] 3/-1/-1->2->1|1->2->3/-1/-1 [9] 3/-1/-1->2->1|1->2->3/-1/-1 [10] -1/-1/-1->2->6|6->2->-1/-1/-1 [11] 6/-1/-1->2->0|0->2->6/-1/-1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:62:62 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:62:62 [3] NCCL INFO Trees [0] 2/-1/-1->3->0|0->3->2/-1/-1 [1] 2/-1/-1->3->0|0->3->2/-1/-1 [2] -1/-1/-1->3->2|2->3->-1/-1/-1 [3] -1/-1/-1->3->2|2->3->-1/-1/-1 [4] 7/-1/-1->3->1|1->3->7/-1/-1 [5] 1/-1/-1->3->7|7->3->1/-1/-1 [6] 2/-1/-1->3->0|0->3->2/-1/-1 [7] 2/-1/-1->3->0|0->3->2/-1/-1 [8] -1/-1/-1->3->2|2->3->-1/-1/-1 [9] -1/-1/-1->3->2|2->3->-1/-1/-1 [10] 7/-1/-1->3->1|1->3->7/-1/-1 [11] 1/-1/-1->3->7|7->3->1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:64:64 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:64:64 [4] NCCL INFO Trees [0] -1/-1/-1->4->7|7->4->-1/-1/-1 [1] -1/-1/-1->4->7|7->4->-1/-1/-1 [2] 7/-1/-1->4->0|0->4->7/-1/-1 [3] 7/-1/-1->4->0|0->4->7/-1/-1 [4] 6/-1/-1->4->5|5->4->6/-1/-1 [5] 5/-1/-1->4->6|6->4->5/-1/-1 [6] -1/-1/-1->4->7|7->4->-1/-1/-1 [7] -1/-1/-1->4->7|7->4->-1/-1/-1 [8] 7/-1/-1->4->0|0->4->7/-1/-1 [9] 7/-1/-1->4->0|0->4->7/-1/-1 [10] 6/-1/-1->4->5|5->4->6/-1/-1 [11] 5/-1/-1->4->6|6->4->5/-1/-1\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:67:67 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:66:66 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:66:66 [5] NCCL INFO Trees [0] 6/-1/-1->5->1|1->5->6/-1/-1 [1] 6/-1/-1->5->1|1->5->6/-1/-1 [2] 1/-1/-1->5->6|6->5->1/-1/-1 [3] 1/-1/-1->5->6|6->5->1/-1/-1 [4] 4/-1/-1->5->7|7->5->4/-1/-1 [5] 7/-1/-1->5->4|4->5->7/-1/-1 [6] 6/-1/-1->5->1|1->5->6/-1/-1 [7] 6/-1/-1->5->1|1->5->6/-1/-1 [8] 1/-1/-1->5->6|6->5->1/-1/-1 [9] 1/-1/-1->5->6|6->5->1/-1/-1 [10] 4/-1/-1->5->7|7->5->4/-1/-1 [11] 7/-1/-1->5->4|4->5->7/-1/-1\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:67:67 [6] NCCL INFO Trees [0] 7/-1/-1->6->5|5->6->7/-1/-1 [1] 7/-1/-1->6->5|5->6->7/-1/-1 [2] 5/-1/-1->6->7|7->6->5/-1/-1 [3] 5/-1/-1->6->7|7->6->5/-1/-1 [4] 2/-1/-1->6->4|4->6->2/-1/-1 [5] 4/-1/-1->6->2|2->6->4/-1/-1 [6] 7/-1/-1->6->5|5->6->7/-1/-1 [7] 7/-1/-1->6->5|5->6->7/-1/-1 [8] 5/-1/-1->6->7|7->6->5/-1/-1 [9] 5/-1/-1->6->7|7->6->5/-1/-1 [10] 2/-1/-1->6->4|4->6->2/-1/-1 [11] 4/-1/-1->6->2|2->6->4/-1/-1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:68:68 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:68:68 [7] NCCL INFO Trees [0] 4/-1/-1->7->6|6->7->4/-1/-1 [1] 4/-1/-1->7->6|6->7->4/-1/-1 [2] 6/-1/-1->7->4|4->7->6/-1/-1 [3] 6/-1/-1->7->4|4->7->6/-1/-1 [4] 5/-1/-1->7->3|3->7->5/-1/-1 [5] 3/-1/-1->7->5|5->7->3/-1/-1 [6] 4/-1/-1->7->6|6->7->4/-1/-1 [7] 4/-1/-1->7->6|6->7->4/-1/-1 [8] 6/-1/-1->7->4|4->7->6/-1/-1 [9] 6/-1/-1->7->4|4->7->6/-1/-1 [10] 5/-1/-1->7->3|3->7->5/-1/-1 [11] 3/-1/-1->7->5|5->7->3/-1/-1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO Channel 00/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:58:58 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO Channel 01/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO Channel 02/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO Channel 03/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:58:58 [1] NCCL INFO Trees [0] 5/-1/-1->1->2|2->1->5/-1/-1 [1] 5/-1/-1->1->2|2->1->5/-1/-1 [2] 2/-1/-1->1->5|5->1->2/-1/-1 [3] 2/-1/-1->1->5|5->1->2/-1/-1 [4] 3/-1/-1->1->0|0->1->3/-1/-1 [5] -1/-1/-1->1->3|3->1->-1/-1/-1 [6] 5/-1/-1->1->2|2->1->5/-1/-1 [7] 5/-1/-1->1->2|2->1->5/-1/-1 [8] 2/-1/-1->1->5|5->1->2/-1/-1 [9] 2/-1/-1->1->5|5->1->2/-1/-1 [10] 3/-1/-1->1->0|0->1->3/-1/-1 [11] -1/-1/-1->1->3|3->1->-1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO Channel 04/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO Channel 05/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO Channel 06/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO Channel 07/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO Channel 08/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO Channel 09/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO Channel 10/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO Channel 11/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO Trees [0] 3/-1/-1->0->-1|-1->0->3/-1/-1 [1] 3/-1/-1->0->-1|-1->0->3/-1/-1 [2] 4/-1/-1->0->-1|-1->0->4/-1/-1 [3] 4/-1/-1->0->-1|-1->0->4/-1/-1 [4] 1/-1/-1->0->-1|-1->0->1/-1/-1 [5] 2/-1/-1->0->-1|-1->0->2/-1/-1 [6] 3/-1/-1->0->-1|-1->0->3/-1/-1 [7] 3/-1/-1->0->-1|-1->0->3/-1/-1 [8] 4/-1/-1->0->-1|-1->0->4/-1/-1 [9] 4/-1/-1->0->-1|-1->0->4/-1/-1 [10] 1/-1/-1->0->-1|-1->0->1/-1/-1 [11] 2/-1/-1->0->-1|-1->0->2/-1/-1\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:60 [2] NCCL INFO Channel 00 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:62:62 [3] NCCL INFO Channel 00 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:67:67 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:67:67 [2] NCCL INFO Trees [0] 1/-1/-1->2->3|3->2->1/-1/-1 [1] 1/-1/-1->2->3|3->2->1/-1/-1 [2] 3/-1/-1->2->1|1->2->3/-1/-1 [3] 3/-1/-1->2->1|1->2->3/-1/-1 [4] -1/-1/-1->2->6|6->2->-1/-1/-1 [5] 6/-1/-1->2->0|0->2->6/-1/-1 [6] 1/-1/-1->2->3|3->2->1/-1/-1 [7] 1/-1/-1->2->3|3->2->1/-1/-1 [8] 3/-1/-1->2->1|1->2->3/-1/-1 [9] 3/-1/-1->2->1|1->2->3/-1/-1 [10] -1/-1/-1->2->6|6->2->-1/-1/-1 [11] 6/-1/-1->2->0|0->2->6/-1/-1\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:69:69 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:69:69 [3] NCCL INFO Trees [0] 2/-1/-1->3->0|0->3->2/-1/-1 [1] 2/-1/-1->3->0|0->3->2/-1/-1 [2] -1/-1/-1->3->2|2->3->-1/-1/-1 [3] -1/-1/-1->3->2|2->3->-1/-1/-1 [4] 7/-1/-1->3->1|1->3->7/-1/-1 [5] 1/-1/-1->3->7|7->3->1/-1/-1 [6] 2/-1/-1->3->0|0->3->2/-1/-1 [7] 2/-1/-1->3->0|0->3->2/-1/-1 [8] -1/-1/-1->3->2|2->3->-1/-1/-1 [9] -1/-1/-1->3->2|2->3->-1/-1/-1 [10] 7/-1/-1->3->1|1->3->7/-1/-1 [11] 1/-1/-1->3->7|7->3->1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:71:71 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:71:71 [4] NCCL INFO Trees [0] -1/-1/-1->4->7|7->4->-1/-1/-1 [1] -1/-1/-1->4->7|7->4->-1/-1/-1 [2] 7/-1/-1->4->0|0->4->7/-1/-1 [3] 7/-1/-1->4->0|0->4->7/-1/-1 [4] 6/-1/-1->4->5|5->4->6/-1/-1 [5] 5/-1/-1->4->6|6->4->5/-1/-1 [6] -1/-1/-1->4->7|7->4->-1/-1/-1 [7] -1/-1/-1->4->7|7->4->-1/-1/-1 [8] 7/-1/-1->4->0|0->4->7/-1/-1 [9] 7/-1/-1->4->0|0->4->7/-1/-1 [10] 6/-1/-1->4->5|5->4->6/-1/-1 [11] 5/-1/-1->4->6|6->4->5/-1/-1\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:73:73 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:73:73 [5] NCCL INFO Trees [0] 6/-1/-1->5->1|1->5->6/-1/-1 [1] 6/-1/-1->5->1|1->5->6/-1/-1 [2] 1/-1/-1->5->6|6->5->1/-1/-1 [3] 1/-1/-1->5->6|6->5->1/-1/-1 [4] 4/-1/-1->5->7|7->5->4/-1/-1 [5] 7/-1/-1->5->4|4->5->7/-1/-1 [6] 6/-1/-1->5->1|1->5->6/-1/-1 [7] 6/-1/-1->5->1|1->5->6/-1/-1 [8] 1/-1/-1->5->6|6->5->1/-1/-1 [9] 1/-1/-1->5->6|6->5->1/-1/-1 [10] 4/-1/-1->5->7|7->5->4/-1/-1 [11] 7/-1/-1->5->4|4->5->7/-1/-1\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:63:63 [0] NCCL INFO Channel 00/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:63:63 [0] NCCL INFO Channel 01/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:63:63 [0] NCCL INFO Channel 02/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:63:63 [0] NCCL INFO Channel 03/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:75:75 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:75:75 [7] NCCL INFO Trees [0] 4/-1/-1->7->6|6->7->4/-1/-1 [1] 4/-1/-1->7->6|6->7->4/-1/-1 [2] 6/-1/-1->7->4|4->7->6/-1/-1 [3] 6/-1/-1->7->4|4->7->6/-1/-1 [4] 5/-1/-1->7->3|3->7->5/-1/-1 [5] 3/-1/-1->7->5|5->7->3/-1/-1 [6] 4/-1/-1->7->6|6->7->4/-1/-1 [7] 4/-1/-1->7->6|6->7->4/-1/-1 [8] 6/-1/-1->7->4|4->7->6/-1/-1 [9] 6/-1/-1->7->4|4->7->6/-1/-1 [10] 5/-1/-1->7->3|3->7->5/-1/-1 [11] 3/-1/-1->7->5|5->7->3/-1/-1\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:74:74 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:74:74 [6] NCCL INFO Trees [0] 7/-1/-1->6->5|5->6->7/-1/-1 [1] 7/-1/-1->6->5|5->6->7/-1/-1 [2] 5/-1/-1->6->7|7->6->5/-1/-1 [3] 5/-1/-1->6->7|7->6->5/-1/-1 [4] 2/-1/-1->6->4|4->6->2/-1/-1 [5] 4/-1/-1->6->2|2->6->4/-1/-1 [6] 7/-1/-1->6->5|5->6->7/-1/-1 [7] 7/-1/-1->6->5|5->6->7/-1/-1 [8] 5/-1/-1->6->7|7->6->5/-1/-1 [9] 5/-1/-1->6->7|7->6->5/-1/-1 [10] 2/-1/-1->6->4|4->6->2/-1/-1 [11] 4/-1/-1->6->2|2->6->4/-1/-1\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:65:65 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:63:63 [0] NCCL INFO Channel 04/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:63:63 [0] NCCL INFO Channel 05/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:63:63 [0] NCCL INFO Channel 06/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:63:63 [0] NCCL INFO Channel 07/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:63:63 [0] NCCL INFO Channel 08/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:63:63 [0] NCCL INFO Channel 09/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:63:63 [0] NCCL INFO Channel 10/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:63:63 [0] NCCL INFO Channel 11/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:65:65 [1] NCCL INFO Trees [0] 5/-1/-1->1->2|2->1->5/-1/-1 [1] 5/-1/-1->1->2|2->1->5/-1/-1 [2] 2/-1/-1->1->5|5->1->2/-1/-1 [3] 2/-1/-1->1->5|5->1->2/-1/-1 [4] 3/-1/-1->1->0|0->1->3/-1/-1 [5] -1/-1/-1->1->3|3->1->-1/-1/-1 [6] 5/-1/-1->1->2|2->1->5/-1/-1 [7] 5/-1/-1->1->2|2->1->5/-1/-1 [8] 2/-1/-1->1->5|5->1->2/-1/-1 [9] 2/-1/-1->1->5|5->1->2/-1/-1 [10] 3/-1/-1->1->0|0->1->3/-1/-1 [11] -1/-1/-1->1->3|3->1->-1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:63:63 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:63:63 [0] NCCL INFO Trees [0] 3/-1/-1->0->-1|-1->0->3/-1/-1 [1] 3/-1/-1->0->-1|-1->0->3/-1/-1 [2] 4/-1/-1->0->-1|-1->0->4/-1/-1 [3] 4/-1/-1->0->-1|-1->0->4/-1/-1 [4] 1/-1/-1->0->-1|-1->0->1/-1/-1 [5] 2/-1/-1->0->-1|-1->0->2/-1/-1 [6] 3/-1/-1->0->-1|-1->0->3/-1/-1 [7] 3/-1/-1->0->-1|-1->0->3/-1/-1 [8] 4/-1/-1->0->-1|-1->0->4/-1/-1 [9] 4/-1/-1->0->-1|-1->0->4/-1/-1 [10] 1/-1/-1->0->-1|-1->0->1/-1/-1 [11] 2/-1/-1->0->-1|-1->0->2/-1/-1\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:64:64 [4] NCCL INFO Channel 00 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:66:66 [5] NCCL INFO Channel 00 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:67:67 [6] NCCL INFO Channel 00 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:68:68 [7] NCCL INFO Channel 00 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:58:58 [1] NCCL INFO Channel 00 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO Channel 00 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:67:67 [2] NCCL INFO Channel 00 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:69:69 [3] NCCL INFO Channel 00 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:71:71 [4] NCCL INFO Channel 00 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:73:73 [5] NCCL INFO Channel 00 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:74:74 [6] NCCL INFO Channel 00 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:75:75 [7] NCCL INFO Channel 00 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:65:65 [1] NCCL INFO Channel 00 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:63:63 [0] NCCL INFO Channel 00 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:64:64 [4] NCCL INFO Channel 00 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:60 [2] NCCL INFO Channel 00 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:62:62 [3] NCCL INFO Channel 00 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:66:66 [5] NCCL INFO Channel 00 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:67:67 [6] NCCL INFO Channel 00 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:68:68 [7] NCCL INFO Channel 00 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:58:58 [1] NCCL INFO Channel 00 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:71:71 [4] NCCL INFO Channel 00 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:67:67 [2] NCCL INFO Channel 00 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:69:69 [3] NCCL INFO Channel 00 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:74:74 [6] NCCL INFO Channel 00 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:73:73 [5] NCCL INFO Channel 00 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:65:65 [1] NCCL INFO Channel 00 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:75:75 [7] NCCL INFO Channel 00 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:64:64 [4] NCCL INFO Channel 01 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO Channel 01 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:60 [2] NCCL INFO Channel 01 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:62:62 [3] NCCL INFO Channel 01 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:66:66 [5] NCCL INFO Channel 01 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:68:68 [7] NCCL INFO Channel 01 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:67:67 [6] NCCL INFO Channel 01 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:58:58 [1] NCCL INFO Channel 01 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:71:71 [4] NCCL INFO Channel 01 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:63:63 [0] NCCL INFO Channel 01 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:67:67 [2] NCCL INFO Channel 01 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:69:69 [3] NCCL INFO Channel 01 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:74:74 [6] NCCL INFO Channel 01 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:75:75 [7] NCCL INFO Channel 01 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:73:73 [5] NCCL INFO Channel 01 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:65:65 [1] NCCL INFO Channel 01 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:64:64 [4] NCCL INFO Channel 01 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:60 [2] NCCL INFO Channel 01 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:62:62 [3] NCCL INFO Channel 01 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:66:66 [5] NCCL INFO Channel 01 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:68:68 [7] NCCL INFO Channel 01 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:58:58 [1] NCCL INFO Channel 01 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:67:67 [6] NCCL INFO Channel 01 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:71:71 [4] NCCL INFO Channel 01 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:67:67 [2] NCCL INFO Channel 01 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:69:69 [3] NCCL INFO Channel 01 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:74:74 [6] NCCL INFO Channel 01 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:73:73 [5] NCCL INFO Channel 01 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:65:65 [1] NCCL INFO Channel 01 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:75:75 [7] NCCL INFO Channel 01 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:64:64 [4] NCCL INFO Channel 02 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO Channel 02 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:60 [2] NCCL INFO Channel 02 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:62:62 [3] NCCL INFO Channel 02 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:66:66 [5] NCCL INFO Channel 02 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:68:68 [7] NCCL INFO Channel 02 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:71:71 [4] NCCL INFO Channel 02 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:67:67 [6] NCCL INFO Channel 02 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:58:58 [1] NCCL INFO Channel 02 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:63:63 [0] NCCL INFO Channel 02 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:67:67 [2] NCCL INFO Channel 02 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:69:69 [3] NCCL INFO Channel 02 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:74:74 [6] NCCL INFO Channel 02 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:73:73 [5] NCCL INFO Channel 02 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:65:65 [1] NCCL INFO Channel 02 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:75:75 [7] NCCL INFO Channel 02 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:62:62 [3] NCCL INFO Channel 02 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:64:64 [4] NCCL INFO Channel 02 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:60 [2] NCCL INFO Channel 02 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:66:66 [5] NCCL INFO Channel 02 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:68:68 [7] NCCL INFO Channel 02 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:58:58 [1] NCCL INFO Channel 02 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:67:67 [6] NCCL INFO Channel 02 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:69:69 [3] NCCL INFO Channel 02 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:71:71 [4] NCCL INFO Channel 02 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:67:67 [2] NCCL INFO Channel 02 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:74:74 [6] NCCL INFO Channel 02 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:65:65 [1] NCCL INFO Channel 02 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:75:75 [7] NCCL INFO Channel 02 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:73:73 [5] NCCL INFO Channel 02 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:62:62 [3] NCCL INFO Channel 03 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO Channel 03 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:64:64 [4] NCCL INFO Channel 03 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:60 [2] NCCL INFO Channel 03 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:69:69 [3] NCCL INFO Channel 03 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:68:68 [7] NCCL INFO Channel 03 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:66:66 [5] NCCL INFO Channel 03 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:63:63 [0] NCCL INFO Channel 03 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:67:67 [6] NCCL INFO Channel 03 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:58:58 [1] NCCL INFO Channel 03 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:71:71 [4] NCCL INFO Channel 03 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:67:67 [2] NCCL INFO Channel 03 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:74:74 [6] NCCL INFO Channel 03 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:75:75 [7] NCCL INFO Channel 03 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:65:65 [1] NCCL INFO Channel 03 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:62:62 [3] NCCL INFO Channel 03 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:73:73 [5] NCCL INFO Channel 03 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:64:64 [4] NCCL INFO Channel 03 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:60 [2] NCCL INFO Channel 03 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:69:69 [3] NCCL INFO Channel 03 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:68:68 [7] NCCL INFO Channel 03 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:66:66 [5] NCCL INFO Channel 03 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:58:58 [1] NCCL INFO Channel 03 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:67:67 [6] NCCL INFO Channel 03 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:71:71 [4] NCCL INFO Channel 03 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:74:74 [6] NCCL INFO Channel 03 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:67:67 [2] NCCL INFO Channel 03 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:65:65 [1] NCCL INFO Channel 03 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:75:75 [7] NCCL INFO Channel 03 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:62:62 [3] NCCL INFO Channel 04 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:73:73 [5] NCCL INFO Channel 03 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO Channel 04 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:64:64 [4] NCCL INFO Channel 04 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:60 [2] NCCL INFO Channel 04 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:69:69 [3] NCCL INFO Channel 04 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:68:68 [7] NCCL INFO Channel 04 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:66:66 [5] NCCL INFO Channel 04 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:67:67 [6] NCCL INFO Channel 04 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:58:58 [1] NCCL INFO Channel 04 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:63:63 [0] NCCL INFO Channel 04 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:71:71 [4] NCCL INFO Channel 04 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:67:67 [2] NCCL INFO Channel 04 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:74:74 [6] NCCL INFO Channel 04 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:65:65 [1] NCCL INFO Channel 04 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:75:75 [7] NCCL INFO Channel 04 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:73:73 [5] NCCL INFO Channel 04 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:60 [2] NCCL INFO Channel 04 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:64:64 [4] NCCL INFO Channel 04 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:62:62 [3] NCCL INFO Channel 04 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:68:68 [7] NCCL INFO Channel 04 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:66:66 [5] NCCL INFO Channel 04 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:67:67 [6] NCCL INFO Channel 04 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:58:58 [1] NCCL INFO Channel 04 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:67:67 [2] NCCL INFO Channel 04 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:69:69 [3] NCCL INFO Channel 04 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:71:71 [4] NCCL INFO Channel 04 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:74:74 [6] NCCL INFO Channel 04 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:65:65 [1] NCCL INFO Channel 04 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:75:75 [7] NCCL INFO Channel 04 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:73:73 [5] NCCL INFO Channel 04 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:60 [2] NCCL INFO Channel 05 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:62:62 [3] NCCL INFO Channel 05 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:64:64 [4] NCCL INFO Channel 05 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO Channel 05 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:67:67 [2] NCCL INFO Channel 05 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:68:68 [7] NCCL INFO Channel 05 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:66:66 [5] NCCL INFO Channel 05 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:67:67 [6] NCCL INFO Channel 05 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:58:58 [1] NCCL INFO Channel 05 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:63:63 [0] NCCL INFO Channel 05 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:69:69 [3] NCCL INFO Channel 05 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:71:71 [4] NCCL INFO Channel 05 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:74:74 [6] NCCL INFO Channel 05 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:65:65 [1] NCCL INFO Channel 05 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:75:75 [7] NCCL INFO Channel 05 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:73:73 [5] NCCL INFO Channel 05 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:60 [2] NCCL INFO Channel 05 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:58:58 [1] NCCL INFO Channel 05 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:62:62 [3] NCCL INFO Channel 05 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:64:64 [4] NCCL INFO Channel 05 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:68:68 [7] NCCL INFO Channel 05 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:66:66 [5] NCCL INFO Channel 05 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:67:67 [6] NCCL INFO Channel 05 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:67:67 [2] NCCL INFO Channel 05 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:65:65 [1] NCCL INFO Channel 05 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:71:71 [4] NCCL INFO Channel 05 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:69:69 [3] NCCL INFO Channel 05 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:74:74 [6] NCCL INFO Channel 05 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:75:75 [7] NCCL INFO Channel 05 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:73:73 [5] NCCL INFO Channel 05 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO Channel 06 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:58:58 [1] NCCL INFO Channel 06 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:62:62 [3] NCCL INFO Channel 06 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:60 [2] NCCL INFO Channel 06 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:64:64 [4] NCCL INFO Channel 06 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:63:63 [0] NCCL INFO Channel 06 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:68:68 [7] NCCL INFO Channel 06 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:66:66 [5] NCCL INFO Channel 06 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:65:65 [1] NCCL INFO Channel 06 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:67:67 [6] NCCL INFO Channel 06 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:67:67 [2] NCCL INFO Channel 06 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:69:69 [3] NCCL INFO Channel 06 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:71:71 [4] NCCL INFO Channel 06 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:74:74 [6] NCCL INFO Channel 06 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:75:75 [7] NCCL INFO Channel 06 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:73:73 [5] NCCL INFO Channel 06 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:64:64 [4] NCCL INFO Channel 06 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:58:58 [1] NCCL INFO Channel 06 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:62:62 [3] NCCL INFO Channel 06 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:60 [2] NCCL INFO Channel 06 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:68:68 [7] NCCL INFO Channel 06 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:66:66 [5] NCCL INFO Channel 06 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:71:71 [4] NCCL INFO Channel 06 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:67:67 [6] NCCL INFO Channel 06 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:67:67 [2] NCCL INFO Channel 06 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:65:65 [1] NCCL INFO Channel 06 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:69:69 [3] NCCL INFO Channel 06 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:74:74 [6] NCCL INFO Channel 06 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:75:75 [7] NCCL INFO Channel 06 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:73:73 [5] NCCL INFO Channel 06 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:64:64 [4] NCCL INFO Channel 07 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO Channel 07 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:58:58 [1] NCCL INFO Channel 07 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:62:62 [3] NCCL INFO Channel 07 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:60 [2] NCCL INFO Channel 07 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:68:68 [7] NCCL INFO Channel 07 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:66:66 [5] NCCL INFO Channel 07 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:71:71 [4] NCCL INFO Channel 07 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:67:67 [6] NCCL INFO Channel 07 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:63:63 [0] NCCL INFO Channel 07 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:67:67 [2] NCCL INFO Channel 07 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:69:69 [3] NCCL INFO Channel 07 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:74:74 [6] NCCL INFO Channel 07 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:65:65 [1] NCCL INFO Channel 07 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:75:75 [7] NCCL INFO Channel 07 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:64:64 [4] NCCL INFO Channel 07 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:73:73 [5] NCCL INFO Channel 07 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:58:58 [1] NCCL INFO Channel 07 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:62:62 [3] NCCL INFO Channel 07 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:60 [2] NCCL INFO Channel 07 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:66:66 [5] NCCL INFO Channel 07 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:68:68 [7] NCCL INFO Channel 07 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:71:71 [4] NCCL INFO Channel 07 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:67:67 [6] NCCL INFO Channel 07 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:67:67 [2] NCCL INFO Channel 07 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:69:69 [3] NCCL INFO Channel 07 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:65:65 [1] NCCL INFO Channel 07 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:74:74 [6] NCCL INFO Channel 07 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:75:75 [7] NCCL INFO Channel 07 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:73:73 [5] NCCL INFO Channel 07 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:64:64 [4] NCCL INFO Channel 08 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO Channel 08 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:58:58 [1] NCCL INFO Channel 08 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:62:62 [3] NCCL INFO Channel 08 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:60 [2] NCCL INFO Channel 08 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:68:68 [7] NCCL INFO Channel 08 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:71:71 [4] NCCL INFO Channel 08 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:66:66 [5] NCCL INFO Channel 08 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:67:67 [6] NCCL INFO Channel 08 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:63:63 [0] NCCL INFO Channel 08 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:67:67 [2] NCCL INFO Channel 08 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:69:69 [3] NCCL INFO Channel 08 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:65:65 [1] NCCL INFO Channel 08 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:74:74 [6] NCCL INFO Channel 08 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:75:75 [7] NCCL INFO Channel 08 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:73:73 [5] NCCL INFO Channel 08 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:62:62 [3] NCCL INFO Channel 08 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:64:64 [4] NCCL INFO Channel 08 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:58:58 [1] NCCL INFO Channel 08 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:60 [2] NCCL INFO Channel 08 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:68:68 [7] NCCL INFO Channel 08 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:66:66 [5] NCCL INFO Channel 08 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:69:69 [3] NCCL INFO Channel 08 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:67:67 [6] NCCL INFO Channel 08 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:71:71 [4] NCCL INFO Channel 08 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:67:67 [2] NCCL INFO Channel 08 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:65:65 [1] NCCL INFO Channel 08 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:74:74 [6] NCCL INFO Channel 08 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:62:62 [3] NCCL INFO Channel 09 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:75:75 [7] NCCL INFO Channel 08 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO Channel 09 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:73:73 [5] NCCL INFO Channel 08 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:64:64 [4] NCCL INFO Channel 09 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:58:58 [1] NCCL INFO Channel 09 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:69:69 [3] NCCL INFO Channel 09 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:60 [2] NCCL INFO Channel 09 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:68:68 [7] NCCL INFO Channel 09 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:66:66 [5] NCCL INFO Channel 09 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:63:63 [0] NCCL INFO Channel 09 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:67:67 [6] NCCL INFO Channel 09 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:67:67 [2] NCCL INFO Channel 09 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:71:71 [4] NCCL INFO Channel 09 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:74:74 [6] NCCL INFO Channel 09 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:65:65 [1] NCCL INFO Channel 09 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:62:62 [3] NCCL INFO Channel 09 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:75:75 [7] NCCL INFO Channel 09 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:73:73 [5] NCCL INFO Channel 09 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:64:64 [4] NCCL INFO Channel 09 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:58:58 [1] NCCL INFO Channel 09 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:69:69 [3] NCCL INFO Channel 09 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:60 [2] NCCL INFO Channel 09 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:68:68 [7] NCCL INFO Channel 09 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:66:66 [5] NCCL INFO Channel 09 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:67:67 [6] NCCL INFO Channel 09 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:67:67 [2] NCCL INFO Channel 09 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:71:71 [4] NCCL INFO Channel 09 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:74:74 [6] NCCL INFO Channel 09 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:65:65 [1] NCCL INFO Channel 09 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:75:75 [7] NCCL INFO Channel 09 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:62:62 [3] NCCL INFO Channel 10 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:73:73 [5] NCCL INFO Channel 09 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO Channel 10 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:69:69 [3] NCCL INFO Channel 10 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:64:64 [4] NCCL INFO Channel 10 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:58:58 [1] NCCL INFO Channel 10 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:60 [2] NCCL INFO Channel 10 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:68:68 [7] NCCL INFO Channel 10 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:66:66 [5] NCCL INFO Channel 10 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:67:67 [6] NCCL INFO Channel 10 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:63:63 [0] NCCL INFO Channel 10 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:67:67 [2] NCCL INFO Channel 10 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:71:71 [4] NCCL INFO Channel 10 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:74:74 [6] NCCL INFO Channel 10 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:65:65 [1] NCCL INFO Channel 10 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:75:75 [7] NCCL INFO Channel 10 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:73:73 [5] NCCL INFO Channel 10 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:60 [2] NCCL INFO Channel 10 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:62:62 [3] NCCL INFO Channel 10 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:58:58 [1] NCCL INFO Channel 10 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:64:64 [4] NCCL INFO Channel 10 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:67:67 [2] NCCL INFO Channel 10 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:68:68 [7] NCCL INFO Channel 10 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:66:66 [5] NCCL INFO Channel 10 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:67:67 [6] NCCL INFO Channel 10 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:69:69 [3] NCCL INFO Channel 10 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:71:71 [4] NCCL INFO Channel 10 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:74:74 [6] NCCL INFO Channel 10 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:65:65 [1] NCCL INFO Channel 10 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:75:75 [7] NCCL INFO Channel 10 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:73:73 [5] NCCL INFO Channel 10 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:60 [2] NCCL INFO Channel 11 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO Channel 11 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:62:62 [3] NCCL INFO Channel 11 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:58:58 [1] NCCL INFO Channel 11 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:64:64 [4] NCCL INFO Channel 11 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:67:67 [2] NCCL INFO Channel 11 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:66:66 [5] NCCL INFO Channel 11 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:68:68 [7] NCCL INFO Channel 11 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:67:67 [6] NCCL INFO Channel 11 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:63:63 [0] NCCL INFO Channel 11 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:69:69 [3] NCCL INFO Channel 11 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:74:74 [6] NCCL INFO Channel 11 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:71:71 [4] NCCL INFO Channel 11 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:65:65 [1] NCCL INFO Channel 11 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:75:75 [7] NCCL INFO Channel 11 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:73:73 [5] NCCL INFO Channel 11 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:58:58 [1] NCCL INFO Channel 11 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:60 [2] NCCL INFO Channel 11 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:62:62 [3] NCCL INFO Channel 11 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:64:64 [4] NCCL INFO Channel 11 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:66:66 [5] NCCL INFO Channel 11 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:68:68 [7] NCCL INFO Channel 11 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:67:67 [2] NCCL INFO Channel 11 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:65:65 [1] NCCL INFO Channel 11 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:58:58 [1] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:67:67 [6] NCCL INFO Channel 11 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:58:58 [1] NCCL INFO comm 0x55e6ed8514e0 rank 1 nranks 8 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:69:69 [3] NCCL INFO Channel 11 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:71:71 [4] NCCL INFO Channel 11 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:74:74 [6] NCCL INFO Channel 11 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO comm 0x5585648d1ef0 rank 0 nranks 8 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:75:75 [7] NCCL INFO Channel 11 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:62:62 [3] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:62:62 [3] NCCL INFO comm 0x5612f399bc40 rank 3 nranks 8 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:60 [2] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:64:64 [4] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:60 [2] NCCL INFO comm 0x56499e4f1ec0 rank 2 nranks 8 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:64:64 [4] NCCL INFO comm 0x563bbaf0fe70 rank 4 nranks 8 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:66:66 [5] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:73:73 [5] NCCL INFO Channel 11 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:66:66 [5] NCCL INFO comm 0x55cc9b6e72b0 rank 5 nranks 8 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:68:68 [7] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:68:68 [7] NCCL INFO comm 0x55a915093e60 rank 7 nranks 8 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:67:67 [6] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:63:63 [0] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:65:65 [1] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:63:63 [0] NCCL INFO comm 0x5646e4a74470 rank 0 nranks 8 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:67:67 [6] NCCL INFO comm 0x55dfd19f43a0 rank 6 nranks 8 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:65:65 [1] NCCL INFO comm 0x55912c612520 rank 1 nranks 8 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:67:67 [2] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:67:67 [2] NCCL INFO comm 0x55720c4f6a60 rank 2 nranks 8 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:69:69 [3] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:69:69 [3] NCCL INFO comm 0x559cc46951c0 rank 3 nranks 8 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:71:71 [4] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:74:74 [6] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:71:71 [4] NCCL INFO comm 0x563c26d02210 rank 4 nranks 8 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:74:74 [6] NCCL INFO comm 0x55621de37ce0 rank 6 nranks 8 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:75:75 [7] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:75:75 [7] NCCL INFO comm 0x56141ffc7b20 rank 7 nranks 8 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:73:73 [5] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:73:73 [5] NCCL INFO comm 0x5613dc8b3240 rank 5 nranks 8 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:66:66 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:66:66 [5] NCCL INFO Trees [0] 6/-1/-1->5->1|1->5->6/-1/-1 [1] 6/-1/-1->5->1|1->5->6/-1/-1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:68:68 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:68:68 [7] NCCL INFO Trees [0] 4/-1/-1->7->6|6->7->4/-1/-1 [1] 4/-1/-1->7->6|6->7->4/-1/-1\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:67:67 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:67:67 [6] NCCL INFO Trees [0] 7/-1/-1->6->5|5->6->7/-1/-1 [1] 7/-1/-1->6->5|5->6->7/-1/-1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:62:62 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:62:62 [3] NCCL INFO Trees [0] 2/8/-1->3->0|0->3->2/8/-1 [1] 2/-1/-1->3->0|0->3->2/-1/-1\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:60 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:60 [2] NCCL INFO Trees [0] 1/-1/-1->2->3|3->2->1/-1/-1 [1] 1/-1/-1->2->3|3->2->1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:64:64 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:64:64 [4] NCCL INFO Trees [0] -1/-1/-1->4->7|7->4->-1/-1/-1 [1] -1/-1/-1->4->7|7->4->-1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:58:58 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:58:58 [1] NCCL INFO Trees [0] 5/-1/-1->1->2|2->1->5/-1/-1 [1] 5/-1/-1->1->2|2->1->5/-1/-1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO Channel 00/02 :    0   3   2   1   5   6   7   4   8  11  10   9  13  14  15  12\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO Channel 01/02 :    0   3   2   1   5   6   7   4   8  11  10   9  13  14  15  12\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO Trees [0] 3/-1/-1->0->-1|-1->0->3/-1/-1 [1] 3/-1/-1->0->11|11->0->3/-1/-1\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:67:67 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:67:67 [2] NCCL INFO Trees [0] 9/-1/-1->10->11|11->10->9/-1/-1 [1] 9/-1/-1->10->11|11->10->9/-1/-1\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:63:63 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:63:63 [0] NCCL INFO Trees [0] 11/-1/-1->8->3|3->8->11/-1/-1 [1] 11/-1/-1->8->-1|-1->8->11/-1/-1\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:71:71 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:71:71 [4] NCCL INFO Trees [0] -1/-1/-1->12->15|15->12->-1/-1/-1 [1] -1/-1/-1->12->15|15->12->-1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:74:74 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:74:74 [6] NCCL INFO Trees [0] 15/-1/-1->14->13|13->14->15/-1/-1 [1] 15/-1/-1->14->13|13->14->15/-1/-1\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:65:65 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:65:65 [1] NCCL INFO Trees [0] 13/-1/-1->9->10|10->9->13/-1/-1 [1] 13/-1/-1->9->10|10->9->13/-1/-1\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:75:75 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:75:75 [7] NCCL INFO Trees [0] 12/-1/-1->15->14|14->15->12/-1/-1 [1] 12/-1/-1->15->14|14->15->12/-1/-1\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:69:69 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:69:69 [3] NCCL INFO Trees [0] 10/-1/-1->11->8|8->11->10/-1/-1 [1] 10/0/-1->11->8|8->11->10/0/-1\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:73:73 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:73:73 [5] NCCL INFO Trees [0] 14/-1/-1->13->9|9->13->14/-1/-1 [1] 14/-1/-1->13->9|9->13->14/-1/-1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:68:68 [7] NCCL INFO Channel 00 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:66:66 [5] NCCL INFO Channel 00 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:67:67 [6] NCCL INFO Channel 00 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:62:62 [3] NCCL INFO Channel 00 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:65:65 [1] NCCL INFO Channel 00 : 9[180] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:58:58 [1] NCCL INFO Channel 00 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:60 [2] NCCL INFO Channel 00 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:74:74 [6] NCCL INFO Channel 00 : 14[1d0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:67:67 [2] NCCL INFO Channel 00 : 10[190] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:69:69 [3] NCCL INFO Channel 00 : 11[1a0] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:75:75 [7] NCCL INFO Channel 00 : 15[1e0] -> 12[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:73:73 [5] NCCL INFO Channel 00 : 13[1c0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO Channel 00 : 12[1b0] -> 0[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:63:63 [0] NCCL INFO Channel 00 : 4[1b0] -> 8[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:63:63 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:64:64 [4] NCCL INFO Channel 00 : 4[1b0] -> 8[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:71:71 [4] NCCL INFO Channel 00 : 12[1b0] -> 0[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO Channel 00 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:63:63 [0] NCCL INFO Channel 00 : 8[170] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:66:66 [5] NCCL INFO Channel 00 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:68:68 [7] NCCL INFO Channel 00 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:65:65 [1] NCCL INFO Channel 00 : 9[180] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:74:74 [6] NCCL INFO Channel 00 : 14[1d0] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:60 [2] NCCL INFO Channel 00 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:67:67 [6] NCCL INFO Channel 00 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:58:58 [1] NCCL INFO Channel 00 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:67:67 [2] NCCL INFO Channel 00 : 10[190] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:75:75 [7] NCCL INFO Channel 00 : 15[1e0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:73:73 [5] NCCL INFO Channel 00 : 13[1c0] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:64:64 [4] NCCL INFO Channel 00 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:71:71 [4] NCCL INFO Channel 00 : 12[1b0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:69:69 [3] NCCL INFO Channel 00 : 11[1a0] -> 8[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:62:62 [3] NCCL INFO Channel 00 : 8[170] -> 3[1a0] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:62:62 [3] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:63:63 [0] NCCL INFO Channel 00 : 8[170] -> 3[1a0] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:66:66 [5] NCCL INFO Channel 01 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:68:68 [7] NCCL INFO Channel 01 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:62:62 [3] NCCL INFO Channel 00 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:67:67 [6] NCCL INFO Channel 01 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:60 [2] NCCL INFO Channel 01 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:58:58 [1] NCCL INFO Channel 01 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:65:65 [1] NCCL INFO Channel 01 : 9[180] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:74:74 [6] NCCL INFO Channel 01 : 14[1d0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:67:67 [2] NCCL INFO Channel 01 : 10[190] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:75:75 [7] NCCL INFO Channel 01 : 15[1e0] -> 12[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:73:73 [5] NCCL INFO Channel 01 : 13[1c0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:64:64 [4] NCCL INFO Channel 01 : 4[1b0] -> 8[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:69:69 [3] NCCL INFO Channel 01 : 11[1a0] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:71:71 [4] NCCL INFO Channel 01 : 12[1b0] -> 0[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:66:66 [5] NCCL INFO Channel 01 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:68:68 [7] NCCL INFO Channel 01 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:67:67 [6] NCCL INFO Channel 01 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO Channel 01 : 12[1b0] -> 0[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:58:58 [1] NCCL INFO Channel 01 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:63:63 [0] NCCL INFO Channel 00 : 3[1a0] -> 8[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:63:63 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:65:65 [1] NCCL INFO Channel 01 : 9[180] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:66:66 [5] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:74:74 [6] NCCL INFO Channel 01 : 14[1d0] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:66:66 [5] NCCL INFO comm 0x55cc9e3ee6e0 rank 5 nranks 16 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:67:67 [6] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO Channel 01 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:67:67 [6] NCCL INFO comm 0x55dfd46fb7d0 rank 6 nranks 16 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:62:62 [3] NCCL INFO Channel 00 : 3[1a0] -> 8[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:75:75 [7] NCCL INFO Channel 01 : 15[1e0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:73:73 [5] NCCL INFO Channel 01 : 13[1c0] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:67:67 [2] NCCL INFO Channel 01 : 10[190] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:65:65 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:65:65 [1] NCCL INFO comm 0x55912f319950 rank 9 nranks 16 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:74:74 [6] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:74:74 [6] NCCL INFO comm 0x556220b3f110 rank 14 nranks 16 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:71:71 [4] NCCL INFO Channel 01 : 12[1b0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:73:73 [5] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:73:73 [5] NCCL INFO comm 0x5613df5ba670 rank 13 nranks 16 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:62:62 [3] NCCL INFO Channel 01 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:71:71 [4] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:75:75 [7] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:71:71 [4] NCCL INFO comm 0x563c29a09640 rank 12 nranks 16 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:75:75 [7] NCCL INFO comm 0x561422ccef50 rank 15 nranks 16 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:60 [2] NCCL INFO Channel 01 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:62:62 [3] NCCL INFO Channel 01 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:58:58 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:58:58 [1] NCCL INFO comm 0x55e6f0558910 rank 1 nranks 16 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:63:63 [0] NCCL INFO Channel 01 : 4[1b0] -> 8[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:63:63 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:60 [2] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:60 [2] NCCL INFO comm 0x5649a11f92f0 rank 2 nranks 16 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:62:62 [3] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:62:62 [3] NCCL INFO comm 0x5612f66a3070 rank 3 nranks 16 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:63:63 [0] NCCL INFO Channel 01 : 8[170] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO Channel 01 : 0[170] -> 11[1a0] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:64:64 [4] NCCL INFO Channel 01 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:67:67 [2] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:67:67 [2] NCCL INFO comm 0x55720f1fde90 rank 10 nranks 16 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:68:68 [7] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:64:64 [4] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:68:68 [7] NCCL INFO comm 0x55a917d9b290 rank 7 nranks 16 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:64:64 [4] NCCL INFO comm 0x563bbdc17620 rank 4 nranks 16 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:69:69 [3] NCCL INFO Channel 01 : 0[170] -> 11[1a0] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:69:69 [3] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:69:69 [3] NCCL INFO Channel 01 : 11[1a0] -> 8[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:63:63 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:63:63 [0] NCCL INFO comm 0x5646e777b8a0 rank 8 nranks 16 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:69:69 [3] NCCL INFO Channel 01 : 11[1a0] -> 0[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO Channel 01 : 11[1a0] -> 0[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:56 [0] NCCL INFO comm 0x5585675d9320 rank 0 nranks 16 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:69:69 [3] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:69:69 [3] NCCL INFO comm 0x559cc739c5f0 rank 11 nranks 16 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:12/01/2021 09:22:23 - WARNING - __main__ -   Process rank: 7, device: cuda:7, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:12/01/2021 09:22:23 - WARNING - __main__ -   Process rank: 5, device: cuda:5, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Running smdistributed.dataparallel v1.0.0\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:12/01/2021 09:22:23 - WARNING - datasets.builder -   Using custom data configuration default-a529624d7fa7afc1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-a529624d7fa7afc1/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:12/01/2021 09:22:23 - WARNING - __main__ -   Process rank: 4, device: cuda:4, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:12/01/2021 09:22:23 - WARNING - datasets.builder -   Using custom data configuration default-a529624d7fa7afc1\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:12/01/2021 09:22:23 - WARNING - __main__ -   Process rank: 6, device: cuda:6, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:12/01/2021 09:22:23 - WARNING - __main__ -   Process rank: 7, device: cuda:7, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:12/01/2021 09:22:23 - WARNING - datasets.builder -   Using custom data configuration default-a529624d7fa7afc1\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:12/01/2021 09:22:23 - WARNING - __main__ -   Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:12/01/2021 09:22:23 - WARNING - __main__ -   Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:12/01/2021 09:22:23 - WARNING - __main__ -   Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:12/01/2021 09:22:23 - WARNING - __main__ -   Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:12/01/2021 09:22:23 - WARNING - datasets.builder -   Using custom data configuration default-bddcbce47998da25\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:12/01/2021 09:22:23 - WARNING - __main__ -   Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-bddcbce47998da25/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:12/01/2021 09:22:23 - WARNING - __main__ -   Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:12/01/2021 09:22:23 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='/opt/ml/model', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=4, per_device_eval_batch_size=4, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, logging_dir='runs/Dec01_09-22-15_algo-1', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=500, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=None, no_cuda=False, seed=7, fp16=True, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=0, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/opt/ml/model', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, mp_parameters='', sortish_sampler=False, predict_with_generate=True)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:12/01/2021 09:22:23 - WARNING - __main__ -   Process rank: 4, device: cuda:4, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:12/01/2021 09:22:23 - WARNING - datasets.builder -   Using custom data configuration default-bddcbce47998da25\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:12/01/2021 09:22:23 - WARNING - __main__ -   Process rank: 5, device: cuda:5, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:12/01/2021 09:22:23 - WARNING - __main__ -   Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:12/01/2021 09:22:23 - WARNING - __main__ -   Process rank: 6, device: cuda:6, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:12/01/2021 09:22:23 - WARNING - __main__ -   Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:12/01/2021 09:22:23 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='/opt/ml/model', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=4, per_device_eval_batch_size=4, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, logging_dir='runs/Dec01_09-22-16_algo-2', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=500, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=None, no_cuda=False, seed=7, fp16=True, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=0, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/opt/ml/model', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, mp_parameters='', sortish_sampler=False, predict_with_generate=True)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:12/01/2021 09:22:23 - WARNING - datasets.builder -   Using custom data configuration default-a529624d7fa7afc1\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:12/01/2021 09:22:23 - WARNING - datasets.builder -   Using custom data configuration default-a529624d7fa7afc1\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:12/01/2021 09:22:23 - WARNING - datasets.builder -   Using custom data configuration default-bddcbce47998da25\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:12/01/2021 09:22:23 - WARNING - datasets.builder -   Using custom data configuration default-bddcbce47998da25\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:12/01/2021 09:22:23 - WARNING - datasets.builder -   Using custom data configuration default-bddcbce47998da25\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:12/01/2021 09:22:23 - WARNING - datasets.builder -   Using custom data configuration default-a529624d7fa7afc1\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:12/01/2021 09:22:23 - WARNING - datasets.builder -   Using custom data configuration default-bddcbce47998da25\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:12/01/2021 09:22:23 - WARNING - datasets.builder -   Using custom data configuration default-bddcbce47998da25\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:12/01/2021 09:22:23 - WARNING - datasets.builder -   Using custom data configuration default-a529624d7fa7afc1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:12/01/2021 09:22:23 - WARNING - datasets.builder -   Using custom data configuration default-a529624d7fa7afc1\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:12/01/2021 09:22:23 - WARNING - datasets.builder -   Using custom data configuration default-bddcbce47998da25\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-a529624d7fa7afc1/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:12/01/2021 09:22:23 - WARNING - datasets.builder -   Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-a529624d7fa7afc1/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:12/01/2021 09:22:23 - WARNING - datasets.builder -   Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-a529624d7fa7afc1/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:12/01/2021 09:22:23 - WARNING - datasets.builder -   Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-a529624d7fa7afc1/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:12/01/2021 09:22:23 - WARNING - datasets.builder -   Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-a529624d7fa7afc1/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:12/01/2021 09:22:23 - WARNING - datasets.builder -   Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-a529624d7fa7afc1/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:12/01/2021 09:22:23 - WARNING - datasets.builder -   Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-a529624d7fa7afc1/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:loading configuration file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/adac95cf641be69365b3dd7fe00d4114b3c7c77fb0572931db31a92d4995053b.a50597c2c8b540e8d07e03ca4d58bf615a365f134fb10ca988f4f67881789178\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Model config BartConfig {\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"_num_labels\": 3,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"activation_dropout\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"activation_function\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"add_bias_logits\": false,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"add_final_layer_norm\": false,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    \"BartForConditionalGeneration\"\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  ],\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"attention_dropout\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"classif_dropout\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"classifier_dropout\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"d_model\": 1024,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"decoder_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"decoder_ffn_dim\": 4096,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"decoder_layerdrop\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"decoder_layers\": 6,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"dropout\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"encoder_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"encoder_ffn_dim\": 4096,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"encoder_layerdrop\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"encoder_layers\": 12,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"extra_pos_embeddings\": 2,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"force_bos_token_to_be_generated\": true,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"id2label\": {\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    \"0\": \"LABEL_0\",\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    \"1\": \"LABEL_1\",\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    \"2\": \"LABEL_2\"\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  },\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"init_std\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"is_encoder_decoder\": true,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"label2id\": {\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    \"LABEL_0\": 0,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    \"LABEL_1\": 1,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    \"LABEL_2\": 2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  },\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"max_position_embeddings\": 1024,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"model_type\": \"bart\",\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"normalize_before\": false,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"normalize_embedding\": true,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"num_hidden_layers\": 12,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"output_past\": true,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"prefix\": \" \",\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"replacing_rate\": 0,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"scale_embedding\": false,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"static_position_embeddings\": false,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"student_decoder_layers\": null,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"student_encoder_layers\": null,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"task_specific_params\": {\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    \"summarization\": {\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:      \"early_stopping\": true,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:      \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:      \"max_length\": 142,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:      \"min_length\": 56,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:      \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:      \"num_beams\": 4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    }\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  },\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"vocab_size\": 50264\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:}\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:12/01/2021 09:22:23 - WARNING - datasets.builder -   Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-a529624d7fa7afc1/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:loading configuration file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/adac95cf641be69365b3dd7fe00d4114b3c7c77fb0572931db31a92d4995053b.a50597c2c8b540e8d07e03ca4d58bf615a365f134fb10ca988f4f67881789178\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Model config BartConfig {\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"_num_labels\": 3,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"activation_dropout\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"activation_function\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"add_bias_logits\": false,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"add_final_layer_norm\": false,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    \"BartForConditionalGeneration\"\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  ],\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"attention_dropout\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"classif_dropout\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"classifier_dropout\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"d_model\": 1024,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"decoder_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"decoder_ffn_dim\": 4096,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"decoder_layerdrop\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"decoder_layers\": 6,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"dropout\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"encoder_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"encoder_ffn_dim\": 4096,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"encoder_layerdrop\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"encoder_layers\": 12,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"extra_pos_embeddings\": 2,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"force_bos_token_to_be_generated\": true,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"id2label\": {\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    \"0\": \"LABEL_0\",\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    \"1\": \"LABEL_1\",\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    \"2\": \"LABEL_2\"\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  },\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"init_std\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"is_encoder_decoder\": true,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"label2id\": {\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    \"LABEL_0\": 0,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    \"LABEL_1\": 1,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    \"LABEL_2\": 2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  },\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"max_position_embeddings\": 1024,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"model_type\": \"bart\",\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"normalize_before\": false,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"normalize_embedding\": true,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"num_hidden_layers\": 12,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"output_past\": true,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"prefix\": \" \",\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"replacing_rate\": 0,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"scale_embedding\": false,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"static_position_embeddings\": false,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"student_decoder_layers\": null,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"student_encoder_layers\": null,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"task_specific_params\": {\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    \"summarization\": {\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:      \"early_stopping\": true,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:      \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:      \"max_length\": 142,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:      \"min_length\": 56,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:      \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:      \"num_beams\": 4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:    }\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  },\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  \"vocab_size\": 50264\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:}\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-bddcbce47998da25/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:12/01/2021 09:22:23 - WARNING - datasets.builder -   Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-bddcbce47998da25/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:12/01/2021 09:22:23 - WARNING - datasets.builder -   Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-bddcbce47998da25/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:12/01/2021 09:22:23 - WARNING - datasets.builder -   Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-bddcbce47998da25/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:12/01/2021 09:22:23 - WARNING - datasets.builder -   Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-bddcbce47998da25/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:12/01/2021 09:22:23 - WARNING - datasets.builder -   Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-bddcbce47998da25/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:12/01/2021 09:22:23 - WARNING - datasets.builder -   Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-bddcbce47998da25/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:12/01/2021 09:22:23 - WARNING - datasets.builder -   Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-bddcbce47998da25/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:loading configuration file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/adac95cf641be69365b3dd7fe00d4114b3c7c77fb0572931db31a92d4995053b.a50597c2c8b540e8d07e03ca4d58bf615a365f134fb10ca988f4f67881789178\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:Model config BartConfig {\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"_num_labels\": 3,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"activation_dropout\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"activation_function\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"add_bias_logits\": false,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"add_final_layer_norm\": false,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    \"BartForConditionalGeneration\"\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  ],\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"attention_dropout\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"classif_dropout\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"classifier_dropout\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"d_model\": 1024,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"decoder_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"decoder_ffn_dim\": 4096,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"decoder_layerdrop\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"decoder_layers\": 6,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"dropout\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"encoder_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"encoder_ffn_dim\": 4096,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"encoder_layerdrop\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"encoder_layers\": 12,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"extra_pos_embeddings\": 2,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"force_bos_token_to_be_generated\": true,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"id2label\": {\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    \"0\": \"LABEL_0\",\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    \"1\": \"LABEL_1\",\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    \"2\": \"LABEL_2\"\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  },\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"init_std\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"is_encoder_decoder\": true,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"label2id\": {\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    \"LABEL_0\": 0,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    \"LABEL_1\": 1,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    \"LABEL_2\": 2\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  },\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"max_position_embeddings\": 1024,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"model_type\": \"bart\",\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"normalize_before\": false,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"normalize_embedding\": true,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"num_hidden_layers\": 12,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"output_past\": true,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"prefix\": \" \",\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"replacing_rate\": 0,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"scale_embedding\": false,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"static_position_embeddings\": false,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"student_decoder_layers\": null,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"student_encoder_layers\": null,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"task_specific_params\": {\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    \"summarization\": {\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:      \"early_stopping\": true,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:      \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:      \"max_length\": 142,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:      \"min_length\": 56,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:      \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:      \"num_beams\": 4\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    }\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  },\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"vocab_size\": 50264\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:}\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:loading configuration file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/adac95cf641be69365b3dd7fe00d4114b3c7c77fb0572931db31a92d4995053b.a50597c2c8b540e8d07e03ca4d58bf615a365f134fb10ca988f4f67881789178\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:Model config BartConfig {\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"_num_labels\": 3,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"activation_dropout\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"activation_function\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"add_bias_logits\": false,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"add_final_layer_norm\": false,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    \"BartForConditionalGeneration\"\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  ],\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"attention_dropout\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"classif_dropout\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"classifier_dropout\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"d_model\": 1024,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"decoder_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"decoder_ffn_dim\": 4096,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"decoder_layerdrop\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"decoder_layers\": 6,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"dropout\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"encoder_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"encoder_ffn_dim\": 4096,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"encoder_layerdrop\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"encoder_layers\": 12,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"extra_pos_embeddings\": 2,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"force_bos_token_to_be_generated\": true,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"id2label\": {\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    \"0\": \"LABEL_0\",\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    \"1\": \"LABEL_1\",\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    \"2\": \"LABEL_2\"\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  },\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"init_std\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"is_encoder_decoder\": true,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"label2id\": {\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    \"LABEL_0\": 0,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    \"LABEL_1\": 1,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    \"LABEL_2\": 2\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  },\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"max_position_embeddings\": 1024,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"model_type\": \"bart\",\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"normalize_before\": false,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"normalize_embedding\": true,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"num_hidden_layers\": 12,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"output_past\": true,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"prefix\": \" \",\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"replacing_rate\": 0,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"scale_embedding\": false,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"static_position_embeddings\": false,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"student_decoder_layers\": null,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"student_encoder_layers\": null,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"task_specific_params\": {\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    \"summarization\": {\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:      \"early_stopping\": true,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:      \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:      \"max_length\": 142,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:      \"min_length\": 56,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:      \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:      \"num_beams\": 4\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    }\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  },\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  \"vocab_size\": 50264\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:}\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/9951e68693b9a7c583ae677e9cb53c02715d9bd0311a78706401372653cdea0a.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/7588c8d398d659b230a038240cc023f67b6848117d2999f06ab625af7bfc7ec1.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/tokenizer.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/f5316f64f9716436994a7ad76a354dc20ecb2dd74eb61d278f103a9c8b80291f.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/9951e68693b9a7c583ae677e9cb53c02715d9bd0311a78706401372653cdea0a.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/7588c8d398d659b230a038240cc023f67b6848117d2999f06ab625af7bfc7ec1.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/tokenizer.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/f5316f64f9716436994a7ad76a354dc20ecb2dd74eb61d278f103a9c8b80291f.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:loading weights file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/b336fa0b874ea92e3e22f07a7e6f8fa9da01221759c33abeb2679d6d98fe7755.585965cf7e82e4536033cd21d76c486af3d6b1c2a34b3a847840d4e7fe9d8844\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:loading weights file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/b336fa0b874ea92e3e22f07a7e6f8fa9da01221759c33abeb2679d6d98fe7755.585965cf7e82e4536033cd21d76c486af3d6b1c2a34b3a847840d4e7fe9d8844\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:All model checkpoint weights were used when initializing BartForConditionalGeneration.\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:All the weights of BartForConditionalGeneration were initialized from the model checkpoint at sshleifer/distilbart-cnn-12-6.\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:All model checkpoint weights were used when initializing BartForConditionalGeneration.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:All the weights of BartForConditionalGeneration were initialized from the model checkpoint at sshleifer/distilbart-cnn-12-6.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Using amp fp16 backend\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:813 [0] NCCL INFO Launch mode Parallel\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:Using amp fp16 backend\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:***** Running training *****\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  Num examples = 16000\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  Num Epochs = 3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  Instantaneous batch size per device = 4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  Total train batch size (w. parallel, distributed & accumulation) = 64\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  Total optimization steps = 750\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:***** Running training *****\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  Num examples = 16000\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  Num Epochs = 3\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  Instantaneous batch size per device = 4\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  Total train batch size (w. parallel, distributed & accumulation) = 64\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  Total optimization steps = 750\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:04.996 algo-1:68 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:04.996 algo-1:64 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:04.996 algo-1:67 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:04.997 algo-1:60 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:04.998 algo-1:62 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.000 algo-1:56 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.022 algo-1:58 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.031 algo-1:66 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.045 algo-2:74 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.045 algo-2:67 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.045 algo-2:63 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.045 algo-2:75 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.045 algo-2:71 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.045 algo-2:69 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.045 algo-2:73 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.092 algo-2:65 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.134 algo-1:68 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.134 algo-1:62 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.134 algo-1:58 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.135 algo-1:68 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.135 algo-1:62 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.135 algo-1:62 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.136 algo-1:58 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.136 algo-1:68 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.136 algo-1:58 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.136 algo-1:62 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.136 algo-1:62 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.137 algo-1:66 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.137 algo-1:67 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.137 algo-1:68 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.137 algo-1:58 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.137 algo-1:68 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.138 algo-1:58 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.138 algo-1:67 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.139 algo-1:56 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.139 algo-1:66 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.139 algo-1:64 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.139 algo-1:60 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.140 algo-1:66 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.140 algo-1:67 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.140 algo-1:64 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.141 algo-1:60 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.141 algo-1:66 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.141 algo-1:67 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.141 algo-1:66 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.141 algo-1:67 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.141 algo-1:56 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.141 algo-1:60 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.142 algo-1:56 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.143 algo-1:64 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.143 algo-1:60 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.143 algo-1:60 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.144 algo-1:56 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.144 algo-1:56 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.145 algo-1:64 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.145 algo-1:64 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.187 algo-2:74 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.187 algo-2:75 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.188 algo-2:71 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.188 algo-2:69 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.188 algo-2:67 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.188 algo-2:63 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.188 algo-2:73 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.188 algo-2:74 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.188 algo-2:71 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.188 algo-2:69 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.188 algo-2:63 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.188 algo-2:73 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.189 algo-2:63 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.189 algo-2:74 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.189 algo-2:71 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.189 algo-2:69 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.189 algo-2:73 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.189 algo-2:75 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.189 algo-2:67 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.189 algo-2:75 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.190 algo-2:73 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.190 algo-2:63 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.190 algo-2:67 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.190 algo-2:69 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.190 algo-2:74 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.190 algo-2:63 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.190 algo-2:71 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.190 algo-2:73 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.190 algo-2:69 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.190 algo-2:71 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.190 algo-2:74 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.191 algo-2:75 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.191 algo-2:75 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.191 algo-2:67 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.191 algo-2:67 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.246 algo-2:65 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.248 algo-2:65 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.249 algo-2:65 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.252 algo-2:65 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.252 algo-2:65 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.333 algo-1:60 INFO hook.py:591] name:module.model.shared.weight count_params:51471360\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.333 algo-1:60 INFO hook.py:591] name:module.model.encoder.embed_positions.weight count_params:1050624\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.333 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.333 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.333 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.333 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.334 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.334 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.334 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.334 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.334 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.334 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.335 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.0.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.335 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.0.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.335 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.0.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.335 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.0.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.335 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.0.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.335 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.0.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.335 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.336 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.336 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.336 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.336 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.336 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.337 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.337 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.337 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.337 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.337 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.1.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.337 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.1.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.337 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.1.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.337 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.1.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.338 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.1.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.338 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.1.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.338 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.338 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.338 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.338 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.338 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.338 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.339 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.339 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.339 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.339 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.339 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.2.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.339 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.2.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.339 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.2.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.339 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.2.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.340 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.2.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.340 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.2.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.340 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.340 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.340 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.340 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.341 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.341 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.341 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.341 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.341 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.341 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.341 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.3.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.341 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.3.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.341 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.3.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.342 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.3.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.342 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.3.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.342 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.3.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.342 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.342 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.342 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.342 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.342 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.343 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.343 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.343 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.343 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.343 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.344 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.4.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.344 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.4.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.344 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.4.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.344 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.4.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.344 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.4.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.344 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.4.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.344 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.344 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.345 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.345 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.345 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.345 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.345 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.345 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.345 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.345 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.346 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.5.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.346 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.5.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.346 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.5.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.346 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.5.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.346 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.5.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.346 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.5.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.346 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.347 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.347 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.347 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.347 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.347 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.347 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.347 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.347 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.348 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.348 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.6.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.348 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.6.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.348 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.6.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.348 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.6.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.348 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.6.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.348 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.6.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.349 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.349 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.349 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.349 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.349 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.349 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.349 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.350 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.350 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.350 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.350 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.7.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.350 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.7.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.350 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.7.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.350 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.7.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.351 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.7.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.351 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.7.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.351 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.351 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.351 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.351 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.351 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.351 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.351 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.352 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.352 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.352 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.352 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.8.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.352 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.8.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.353 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.8.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.353 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.8.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.353 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.8.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.353 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.8.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.353 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.353 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.354 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.354 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.354 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.354 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.354 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.354 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.355 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.355 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.355 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.9.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.355 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.9.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.355 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.9.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.355 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.9.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.356 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.9.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.356 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.9.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.356 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.356 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.356 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.356 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.357 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.357 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.357 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.357 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.357 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.357 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.358 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.10.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.358 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.10.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.358 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.10.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.358 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.10.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.358 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.10.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.358 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.10.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.359 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.359 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.359 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.359 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.359 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.359 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.360 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.360 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.360 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.360 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.360 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.11.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.361 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.11.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.361 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.11.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.361 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.11.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.361 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.11.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.361 algo-1:60 INFO hook.py:591] name:module.model.encoder.layers.11.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.361 algo-1:60 INFO hook.py:591] name:module.model.encoder.layernorm_embedding.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.362 algo-1:60 INFO hook.py:591] name:module.model.encoder.layernorm_embedding.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.362 algo-1:60 INFO hook.py:591] name:module.model.decoder.embed_positions.weight count_params:1050624\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.362 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.362 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.362 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.362 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.362 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.362 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.363 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.363 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.363 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.363 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.363 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.363 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.363 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.363 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.363 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.364 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.364 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.364 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.364 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.364 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.364 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.0.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.364 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.0.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.364 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.0.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.364 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.0.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.364 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.0.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.365 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.0.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.365 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.365 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.365 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.365 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.365 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.365 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.365 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.365 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.365 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.365 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.366 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.366 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.366 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.366 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.366 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.366 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.366 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.366 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.366 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.366 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.366 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.1.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.367 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.1.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.367 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.1.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.367 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.1.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.367 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.1.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.367 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.1.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.367 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.367 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.367 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.367 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.368 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.368 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.368 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.368 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.368 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.368 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.368 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.368 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.369 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.369 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.369 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.369 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.369 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.369 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.369 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.369 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.370 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.2.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.370 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.2.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.370 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.2.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.370 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.2.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.370 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.2.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.370 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.2.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.370 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.370 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.370 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.370 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.371 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.371 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.371 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.371 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.371 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.371 algo-1:66 INFO hook.py:591] name:module.model.shared.weight count_params:51471360\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.371 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.371 algo-1:66 INFO hook.py:591] name:module.model.encoder.embed_positions.weight count_params:1050624\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.371 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.371 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.371 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.371 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.371 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.371 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.372 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.372 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.372 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.372 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.372 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.372 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.372 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.372 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.372 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.372 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.372 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.372 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.372 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.3.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.372 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.3.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.373 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.373 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.3.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.373 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.3.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.373 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.373 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.3.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.373 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.0.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.373 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.3.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.373 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.0.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.373 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.373 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.0.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.373 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.373 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.373 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.0.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.373 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.373 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.0.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.373 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.373 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.0.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.373 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.374 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.374 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.374 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.374 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.374 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.374 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.374 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.374 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.374 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.374 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.374 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.374 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.374 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.374 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.374 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.374 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.374 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.375 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.375 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.375 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.1.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.375 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.375 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.1.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.375 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.375 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.1.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.375 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.1.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.375 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.375 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.1.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.375 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.375 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.1.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.375 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.375 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.375 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.375 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.376 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.4.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.376 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.376 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.4.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.376 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.376 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.4.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.376 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.376 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.4.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.376 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.376 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.4.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.376 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.376 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.376 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.4.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.376 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.377 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.377 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.2.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.377 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.377 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.2.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.377 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.377 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.2.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.377 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.377 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.2.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.377 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.377 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.2.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.377 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.377 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.2.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.377 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.378 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.378 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.378 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.378 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.378 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.378 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.378 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.378 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.378 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.378 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.378 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.378 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.378 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.378 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.379 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.379 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.379 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.379 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.379 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.379 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.379 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.3.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.379 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.379 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.3.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.379 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.379 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.3.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.379 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.379 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.3.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.380 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.5.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.380 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.3.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.380 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.5.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.380 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.3.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.380 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.380 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.5.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.380 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.380 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.5.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.380 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.380 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.380 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.5.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.380 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.380 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.380 algo-1:60 INFO hook.py:591] name:module.model.decoder.layers.5.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.381 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.381 algo-1:60 INFO hook.py:591] name:module.model.decoder.layernorm_embedding.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.381 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.381 algo-1:60 INFO hook.py:591] name:module.model.decoder.layernorm_embedding.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.381 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.381 algo-1:60 INFO hook.py:593] Total Trainable Params: 305511424\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.381 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-12-01 09:23:05.381 algo-1:60 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.381 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.4.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.381 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.4.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.382 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.4.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.382 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.4.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.382 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.4.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.382 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.4.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.382 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.382 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.382 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.382 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.383 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.383 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.383 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.383 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.383 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.383 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.383 algo-1:64 INFO hook.py:591] name:module.model.shared.weight count_params:51471360\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.383 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.5.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.384 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.5.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.384 algo-1:64 INFO hook.py:591] name:module.model.encoder.embed_positions.weight count_params:1050624\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.384 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.5.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.384 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.5.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.384 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.384 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.5.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.384 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.384 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.5.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.384 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.384 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.384 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.384 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.385 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.385 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.385 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.385 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.385 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.385 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.385 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.385 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.385 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.385 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.385 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.385 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.386 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.386 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.6.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.386 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.6.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.386 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.386 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.6.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.386 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.0.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.386 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.6.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.386 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.0.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.386 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.0.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.386 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.6.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.386 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.6.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.387 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.0.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.387 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.387 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.0.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.387 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.387 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.0.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.387 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.387 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.387 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.387 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.387 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.387 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.387 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.388 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.388 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.388 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.388 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.388 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.388 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.388 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.388 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.388 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.7.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.389 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.389 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.7.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.389 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.7.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.389 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.389 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.7.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.389 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.7.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.389 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.389 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.7.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.389 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.389 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.1.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.389 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.390 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.390 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.1.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.390 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.390 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.1.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.390 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.390 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.1.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.390 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.390 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.1.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.390 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.390 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.390 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.1.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.390 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.391 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.391 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.391 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.8.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.391 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.391 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.8.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.391 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.391 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.8.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.391 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.391 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.8.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.391 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.391 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.8.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.391 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.391 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.391 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.8.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.392 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.392 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.392 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.392 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.392 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.392 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.392 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.2.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.392 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.392 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.392 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.2.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.392 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.392 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.2.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.392 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.392 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.2.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.392 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.393 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.393 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.2.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.393 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.393 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.2.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.393 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.393 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.393 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.9.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.393 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.393 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.393 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.9.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.393 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.393 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.393 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.9.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.393 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.9.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.393 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.394 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.9.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.394 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.9.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.394 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.394 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.394 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.394 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.394 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.394 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.394 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.3.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.394 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.3.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.394 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.394 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.3.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.394 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.394 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.3.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.394 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.395 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.395 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.3.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.395 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.3.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.395 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.395 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.395 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.395 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.395 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.395 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.395 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.395 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.10.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.395 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.10.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.395 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.395 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.395 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.10.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.395 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.395 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.10.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.396 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.396 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.396 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.10.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.396 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.396 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.10.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.396 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.4.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.396 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.396 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.4.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.396 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.4.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.396 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.396 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.4.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.396 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.4.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.396 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.4.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.396 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.396 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.397 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.397 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.397 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.397 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.397 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.397 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.397 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.397 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.397 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.397 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.397 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.397 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.397 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.397 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.397 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.398 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.398 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.11.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.398 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.11.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.398 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.5.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.398 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.5.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.398 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.11.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.398 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.5.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.398 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.11.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.398 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.5.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.398 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.5.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.398 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.11.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.398 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.5.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.398 algo-1:66 INFO hook.py:591] name:module.model.encoder.layers.11.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.398 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.398 algo-1:66 INFO hook.py:591] name:module.model.encoder.layernorm_embedding.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.398 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.398 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.398 algo-1:66 INFO hook.py:591] name:module.model.encoder.layernorm_embedding.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.398 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.399 algo-1:66 INFO hook.py:591] name:module.model.decoder.embed_positions.weight count_params:1050624\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.399 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.399 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.399 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.395 algo-2:73 INFO hook.py:591] name:module.model.shared.weight count_params:51471360\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.395 algo-2:73 INFO hook.py:591] name:module.model.encoder.embed_positions.weight count_params:1050624\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.396 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.399 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.396 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.399 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.399 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.396 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.396 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.399 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.396 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.399 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.399 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.396 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.396 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.399 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.399 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.396 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.399 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.6.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.396 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.399 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.399 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.6.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.400 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.6.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.397 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.400 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.400 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.6.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.400 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.6.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.397 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.0.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.400 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.6.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.397 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.0.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.400 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.397 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.0.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.400 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.400 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.397 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.0.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.400 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.400 algo-1:58 INFO hook.py:591] name:module.model.shared.weight count_params:51471360\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.397 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.0.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.397 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.0.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.400 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.400 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.400 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.400 algo-1:58 INFO hook.py:591] name:module.model.encoder.embed_positions.weight count_params:1050624\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.400 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.400 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.400 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.400 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.400 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.401 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.401 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.401 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.398 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.401 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.398 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.398 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.398 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.401 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.401 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.398 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.401 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.401 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.7.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.401 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.401 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.398 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.401 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.7.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.401 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.401 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.7.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.401 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.398 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.401 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.401 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.401 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.7.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.398 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.401 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.7.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.401 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.398 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.401 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.7.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.401 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.398 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.401 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.401 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.399 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.1.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.402 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.399 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.1.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.402 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.402 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.399 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.1.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.402 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.399 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.1.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.402 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.0.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.399 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.1.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.402 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.402 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.402 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.402 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.0.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.402 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.399 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.1.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.402 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.0.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.399 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.402 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.402 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.399 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.402 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.0.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.402 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.402 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.402 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.402 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.399 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.402 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.0.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.402 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.0.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.402 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.0.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.399 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.399 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.402 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.8.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.402 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.0.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.400 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.402 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.0.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.403 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.0.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.400 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.400 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.403 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.403 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.8.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.403 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.0.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.403 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.0.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.403 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.8.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.403 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.400 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.400 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.403 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.403 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.8.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.403 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.403 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.400 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.2.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.403 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.8.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.400 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.2.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.403 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.403 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.403 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.8.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.403 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.400 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.2.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.403 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.403 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.400 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.2.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.403 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.403 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.401 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.2.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.403 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.403 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.401 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.2.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.404 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.401 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.404 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.404 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.401 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.404 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.404 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.404 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.401 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.404 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.404 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.404 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.401 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.404 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.404 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.404 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.404 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.401 algo-2:69 INFO hook.py:591] name:module.model.shared.weight count_params:51471360\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.401 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.404 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.404 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.1.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.401 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.404 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.401 algo-2:69 INFO hook.py:591] name:module.model.encoder.embed_positions.weight count_params:1050624\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.401 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.404 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.1.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.404 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.401 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.401 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.404 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.404 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.1.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.402 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.404 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.402 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.405 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.1.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.402 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.402 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.405 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.405 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.9.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.402 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.3.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.402 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.402 algo-2:71 INFO hook.py:591] name:module.model.shared.weight count_params:51471360\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.405 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.1.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.402 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.3.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.405 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.405 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.1.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.402 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.405 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.9.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.402 algo-2:67 INFO hook.py:591] name:module.model.shared.weight count_params:51471360\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.402 algo-2:71 INFO hook.py:591] name:module.model.encoder.embed_positions.weight count_params:1050624\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.402 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.3.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.405 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.405 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.9.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.405 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.402 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.402 algo-2:67 INFO hook.py:591] name:module.model.encoder.embed_positions.weight count_params:1050624\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.402 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.405 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.405 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.9.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.402 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.3.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.405 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.9.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.402 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.405 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.405 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.402 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.402 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.3.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.405 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.9.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.405 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.402 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.402 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.405 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.405 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.402 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.3.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.405 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.402 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.402 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.405 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.402 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.402 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.405 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.405 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.402 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.402 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.402 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.402 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.402 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.406 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.406 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.402 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.0.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.402 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.402 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.406 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.406 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.406 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.403 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.406 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.403 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.0.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.406 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.403 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.403 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.406 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.403 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.403 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.0.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.406 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.406 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.406 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.403 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.403 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.403 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.403 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.0.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.403 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.406 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.1.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.406 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.406 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.2.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.403 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.403 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.0.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.403 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.406 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.1.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.403 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.406 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.406 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.403 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.403 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.403 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.406 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.2.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.406 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.1.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.403 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.0.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.406 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.2.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.403 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.406 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.10.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.403 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.407 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.10.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.406 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.2.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.403 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.403 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.403 algo-2:65 INFO hook.py:591] name:module.model.shared.weight count_params:51471360\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.403 algo-2:65 INFO hook.py:591] name:module.model.encoder.embed_positions.weight count_params:1050624\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.403 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.403 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.407 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.1.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.407 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.10.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.403 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.4.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.403 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.4.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.403 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.0.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.407 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.10.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.403 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.0.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.403 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.407 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.10.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.403 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.0.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.407 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.2.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.407 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.10.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.407 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.1.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.403 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.403 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.0.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.407 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.1.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.403 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.4.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.407 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.2.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.403 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.403 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.0.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.407 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.403 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.403 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.4.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.403 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.0.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.403 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.0.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.407 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.403 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.0.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.407 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.407 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.403 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.407 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.403 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.403 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.4.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.403 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.0.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.407 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.407 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.403 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.403 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.4.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.403 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.0.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.403 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.0.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.407 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.407 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.407 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.403 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.403 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.407 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.404 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.407 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.404 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.408 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.408 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.408 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.408 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.408 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.404 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.408 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.408 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.404 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.408 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.408 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.408 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.11.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.408 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.404 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.404 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.404 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.404 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.0.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.408 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.404 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.404 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.408 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.404 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.408 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.11.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.404 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.408 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.408 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.11.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.408 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.404 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.404 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.404 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.404 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.408 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.408 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.11.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.408 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.11.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.408 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.404 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.404 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.404 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.408 algo-1:64 INFO hook.py:591] name:module.model.encoder.layers.11.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.404 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.408 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.3.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.408 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.404 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.404 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.409 algo-1:64 INFO hook.py:591] name:module.model.encoder.layernorm_embedding.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.404 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.1.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.409 algo-1:64 INFO hook.py:591] name:module.model.encoder.layernorm_embedding.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.404 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.404 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.409 algo-1:64 INFO hook.py:591] name:module.model.decoder.embed_positions.weight count_params:1050624\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.404 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.1.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.409 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.3.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.404 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.409 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.409 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.404 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.404 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.404 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.1.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.409 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.409 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.409 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.3.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.404 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.404 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.404 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.409 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.404 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.404 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.404 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.1.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.409 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.3.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.404 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.1.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.409 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.404 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.1.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.409 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.404 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.404 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.1.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.409 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.3.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.404 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.1.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.404 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.1.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.409 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.409 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.404 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.409 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.404 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.404 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.404 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.409 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.3.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.409 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.409 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.404 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.1.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.404 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.409 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.404 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.1.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.405 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.1.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.404 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.5.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.409 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.404 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.404 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.405 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.409 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.405 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.5.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.410 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.405 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.405 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.410 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.405 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.405 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.1.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.410 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.405 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.1.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.405 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.405 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.5.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.405 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.410 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.405 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.0.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.405 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.405 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.410 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.405 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.5.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.405 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.1.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.405 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.5.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.405 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.405 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.0.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.410 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.410 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.410 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.405 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.405 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.405 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.5.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.410 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.405 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.1.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.405 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.405 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.405 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.1.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.405 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.405 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.410 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.410 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.410 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.410 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.405 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.1.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.405 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.405 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.410 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.405 algo-2:63 INFO hook.py:591] name:module.model.shared.weight count_params:51471360\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.405 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.410 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.405 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.405 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.405 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.410 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.405 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.405 algo-2:63 INFO hook.py:591] name:module.model.encoder.embed_positions.weight count_params:1050624\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.410 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.405 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.2.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.405 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.0.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.405 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.410 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.405 algo-2:74 INFO hook.py:591] name:module.model.shared.weight count_params:51471360\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.411 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.405 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.405 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.2.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.411 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.405 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.405 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.2.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.411 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.2.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.405 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.405 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.2.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.405 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.411 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.405 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.411 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.405 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.2.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.411 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.411 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.2.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.405 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.406 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.411 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.406 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.2.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.411 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.2.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.411 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.4.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.411 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.406 algo-2:74 INFO hook.py:591] name:module.model.encoder.embed_positions.weight count_params:1050624\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.406 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.411 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.2.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.406 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.406 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.406 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.411 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.4.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.406 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.2.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.406 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.411 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.411 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.2.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.411 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.4.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.406 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.2.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.406 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.2.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.411 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.0.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.411 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.0.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.411 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.2.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.411 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.4.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.406 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.406 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.411 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.0.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.411 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.4.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.406 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.412 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.4.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.412 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.412 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.0.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.406 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.406 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.412 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.412 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.0.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.412 algo-1:56 INFO hook.py:591] name:module.model.shared.weight count_params:51471360\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.412 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.406 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.406 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.2.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.412 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.406 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.412 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.0.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.406 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.412 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.412 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.412 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.412 algo-1:56 INFO hook.py:591] name:module.model.encoder.embed_positions.weight count_params:1050624\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.412 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.406 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.406 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.412 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.412 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.406 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.406 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.412 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.412 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.412 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.413 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.406 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.406 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.6.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.413 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.406 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.406 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.413 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.406 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.0.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.406 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.413 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.413 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.413 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.406 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.2.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.406 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.413 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.413 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.413 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.413 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.406 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.6.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.413 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.413 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.406 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.413 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.413 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.413 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.413 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.413 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.406 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.0.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.413 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.413 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.406 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.413 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.413 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.5.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.406 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.2.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.406 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.413 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.413 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.5.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.406 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.6.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.413 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.414 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.414 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.5.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.414 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.406 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.0.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.406 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.6.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.406 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.414 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.414 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.5.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.414 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.406 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.414 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.414 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.414 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.5.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.414 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.414 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.406 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.414 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.5.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.414 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.414 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.414 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.406 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.406 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.414 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.414 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.414 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.414 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.414 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.414 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.406 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.6.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.406 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.414 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.406 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.414 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.0.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.414 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.406 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.406 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.414 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.406 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.406 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.414 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.415 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.415 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.0.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.415 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.406 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.406 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.406 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.2.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.406 algo-2:75 INFO hook.py:591] name:module.model.shared.weight count_params:51471360\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.415 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.415 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.415 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.406 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.6.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.406 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.406 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.406 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.415 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.0.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.415 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.406 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.3.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.406 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.3.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.406 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.406 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.415 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.415 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.3.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.415 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.415 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.407 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.3.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.415 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.0.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.415 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.3.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.406 algo-2:75 INFO hook.py:591] name:module.model.encoder.embed_positions.weight count_params:1050624\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.407 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.407 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.2.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.415 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.415 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.415 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.3.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.407 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.407 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.415 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.3.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.415 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.6.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.415 algo-1:62 INFO hook.py:591] name:module.model.shared.weight count_params:51471360\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.415 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.0.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.407 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.415 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.415 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.6.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.407 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.415 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.0.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.415 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.407 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.3.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.415 algo-1:62 INFO hook.py:591] name:module.model.encoder.embed_positions.weight count_params:1050624\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.407 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.415 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.3.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.415 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.6.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.407 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.416 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.407 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.407 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.0.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.416 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.6.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.416 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.6.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.416 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.416 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.407 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.3.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.407 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.407 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.407 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.2.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.416 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.1.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.416 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.3.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.416 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.6.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.407 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.3.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.416 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.416 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.407 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.407 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.0.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.407 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.407 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.2.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.416 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.416 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.1.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.416 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.407 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.0.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.416 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.416 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.416 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.407 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.407 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.407 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.2.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.416 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.416 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.416 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.1.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.407 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.416 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.407 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.407 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.416 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.1.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.416 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.416 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.407 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.416 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.416 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.1.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.407 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.3.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.416 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.416 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.416 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.407 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.2.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.417 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.407 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.417 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.1.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.417 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.417 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.417 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.417 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.417 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.417 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.417 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.417 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.417 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.417 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.417 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.407 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.0.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.407 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.0.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.417 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.417 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.417 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.417 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.407 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.417 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.407 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.417 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.417 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.407 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.0.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.407 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.0.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.417 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.407 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.3.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.407 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.417 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.407 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.407 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.3.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.407 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.3.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.417 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.417 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.417 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.407 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.0.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.407 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.417 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.417 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.407 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.0.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.407 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.417 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.0.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.417 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.7.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.417 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.407 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.407 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.3.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.417 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.418 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.417 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.407 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.407 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.407 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.0.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.418 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.0.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.418 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.7.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.418 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.7.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.418 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.418 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.418 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.0.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.407 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.407 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.418 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.418 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.1.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.418 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.407 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.3.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.418 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.418 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.0.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.407 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.0.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.418 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.407 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.407 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.407 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.407 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.418 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.7.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.418 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.418 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.0.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.418 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.1.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.418 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.1.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.418 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.7.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.418 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.418 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.0.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.418 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.407 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.0.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.418 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.418 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.7.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.418 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.418 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.1.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.418 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.418 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.407 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.407 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.418 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.418 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.418 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.418 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.418 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.407 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.418 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.407 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.418 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.418 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.1.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.419 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.407 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.419 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.4.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.419 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.407 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.407 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.407 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.419 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.419 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.1.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.419 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.4.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.407 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.419 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.419 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.407 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.407 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.407 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.419 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.407 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.419 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.4.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.419 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.407 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.419 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.407 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.408 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.407 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.7.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.408 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.7.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.419 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.4.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.419 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.419 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.419 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.419 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.419 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.4.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.419 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.419 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.407 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.4.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.408 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.4.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.419 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.4.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.419 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.419 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.419 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.408 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.408 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.419 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.419 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.408 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.419 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.419 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.1.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.419 algo-1:68 INFO hook.py:591] name:module.model.shared.weight count_params:51471360\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.419 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.408 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.419 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.419 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.1.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.408 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.7.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.419 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.408 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.4.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.419 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.408 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.408 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.0.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.408 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.420 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.1.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.420 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.420 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.420 algo-1:68 INFO hook.py:591] name:module.model.encoder.embed_positions.weight count_params:1050624\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.408 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.408 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.420 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.420 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.408 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.7.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.408 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.4.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.420 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.1.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.420 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.420 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.420 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.8.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.420 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.420 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.420 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.1.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.408 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.0.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.408 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.420 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.420 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.420 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.1.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.408 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.408 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.420 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.8.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.420 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.420 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.420 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.420 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.8.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.420 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.420 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.420 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.8.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.420 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.2.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.420 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.408 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.420 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.420 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.408 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.408 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.408 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.7.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.408 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.4.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.420 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.420 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.420 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.8.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.420 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.420 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.2.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.420 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.408 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.0.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.408 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.408 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.421 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.421 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.8.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.421 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.2.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.408 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.421 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.421 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.408 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.421 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.421 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.408 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.7.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.421 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.2.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.421 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.408 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.4.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.421 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.421 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.2.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.421 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.421 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.421 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.421 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.408 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.0.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.408 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.0.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.421 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.2.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.421 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.421 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.421 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.2.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.408 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.408 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.408 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.421 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.421 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.421 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.408 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.408 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.408 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.408 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.408 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.421 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.421 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.421 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.408 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.0.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.421 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.421 algo-1:67 INFO hook.py:591] name:module.model.shared.weight count_params:51471360\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.421 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.2.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.421 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.421 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.421 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.421 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.421 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.408 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.421 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.2.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.421 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.421 algo-1:67 INFO hook.py:591] name:module.model.encoder.embed_positions.weight count_params:1050624\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.421 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.421 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.421 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.421 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.421 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.2.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.421 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.421 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.422 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.422 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.408 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.4.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.408 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.422 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.422 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.422 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.422 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.2.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.422 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.422 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.422 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.422 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.408 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.422 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.422 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.0.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.422 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.422 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.5.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.422 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.2.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.408 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.408 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.4.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.408 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.422 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.408 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.3.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.422 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.0.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.422 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.5.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.422 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.422 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.422 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.422 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.2.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.422 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.9.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.422 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.5.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.408 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.408 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.422 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.408 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.422 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.0.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.422 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.9.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.422 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.5.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.408 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.408 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.4.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.408 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.4.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.422 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.422 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.2.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.422 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.422 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.422 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.9.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.422 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.5.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.408 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.422 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.422 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.422 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.2.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.422 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.0.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.422 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.408 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.3.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.423 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.0.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.423 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.2.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.408 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.423 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.423 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.423 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.423 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.9.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.423 algo-1:66 INFO hook.py:591] name:module.model.decoder.layers.5.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.408 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.423 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.0.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.423 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.423 algo-1:66 INFO hook.py:591] name:module.model.decoder.layernorm_embedding.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.423 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.9.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.408 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.1.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.423 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.2.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.423 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.408 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.423 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.408 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.4.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.423 algo-1:66 INFO hook.py:591] name:module.model.decoder.layernorm_embedding.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.423 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.2.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.408 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.408 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.423 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.9.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.408 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.1.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.423 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.423 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.423 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.423 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.423 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.423 algo-1:66 INFO hook.py:593] Total Trainable Params: 305511424\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.423 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.408 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.408 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.423 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.423 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.423 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.423 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.423 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.408 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.1.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-12-01 09:23:05.423 algo-1:66 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.408 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.4.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.423 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.423 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.408 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.3.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.423 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.423 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.408 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.423 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.423 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.0.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.423 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.423 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.423 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.424 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.424 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.424 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.0.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.408 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.408 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.408 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.424 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.424 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.408 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.424 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.408 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.408 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.424 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.424 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.408 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.1.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.424 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.424 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.424 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.0.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.424 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.424 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.424 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.424 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.408 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.424 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.408 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.424 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.408 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.3.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.424 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.408 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.408 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.424 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.0.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.424 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.3.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.424 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.408 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.424 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.424 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.3.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.424 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.408 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.409 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.408 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.409 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.5.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.408 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.1.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.409 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.1.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.424 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.408 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.408 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.3.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.409 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.424 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.409 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.409 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.424 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.3.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.424 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.424 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.3.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.409 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.5.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.425 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.409 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.409 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.425 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.424 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.0.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.409 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.409 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.1.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.409 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.3.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.409 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.425 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.3.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.425 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.409 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.409 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.409 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.409 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.425 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.3.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.425 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.0.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.425 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.425 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.10.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.409 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.5.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.409 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.409 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.425 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.1.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.409 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.5.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.409 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.409 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.425 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.10.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.425 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.10.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.425 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.3.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.425 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.3.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.425 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.3.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.425 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.3.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.425 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.1.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.425 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.1.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.409 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.409 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.409 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.1.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.409 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.1.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.409 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.5.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.425 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.3.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.425 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.3.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.425 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.0.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.409 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.409 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.8.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.409 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.409 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.409 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.425 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.10.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.425 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.1.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.425 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.3.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.425 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.425 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.425 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.10.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.425 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.3.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.425 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.1.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.425 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.10.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.425 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.425 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.3.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.409 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.425 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.1.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.409 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.5.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.409 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.425 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.409 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.425 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.425 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.409 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.409 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.425 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.425 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.409 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.1.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.426 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.426 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.3.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.409 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.1.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.426 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.426 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.426 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.409 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.409 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.426 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.426 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.409 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.8.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.409 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.409 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.1.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.426 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.426 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.426 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.426 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.409 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.426 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.426 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.3.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.426 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.426 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.409 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.426 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.426 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.3.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.409 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.1.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.426 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.426 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.426 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.426 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.409 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.8.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.426 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.409 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.1.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.426 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.426 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.409 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.5.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.426 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.409 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.8.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.409 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.426 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.426 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.409 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.1.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.409 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.1.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.426 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.426 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.409 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.5.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.426 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.426 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.426 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.409 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.409 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.1.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.409 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.8.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.409 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.426 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.426 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.426 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.426 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.426 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.426 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.426 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.409 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.409 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.1.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.409 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.5.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.409 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.8.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.409 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.426 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.409 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.409 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.427 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.427 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.409 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.1.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.427 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.427 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.409 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.409 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.409 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.5.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.409 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.5.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.427 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.427 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.409 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.409 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.409 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.1.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.427 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.11.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.409 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.409 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.409 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.427 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.1.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.427 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.409 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.1.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.409 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.1.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.427 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.2.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.427 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.427 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.427 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.11.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.427 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.2.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.427 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.1.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.427 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.427 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.11.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.427 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.427 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.409 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.409 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.427 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.11.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.409 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.5.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.427 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.427 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.1.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.409 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.409 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.409 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.409 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.427 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.4.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.427 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.2.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.427 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.11.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.409 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.409 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.1.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.410 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.409 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.427 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.1.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.427 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.427 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.427 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.2.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.427 algo-1:58 INFO hook.py:591] name:module.model.encoder.layers.11.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.427 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.4.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.410 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.427 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.1.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.410 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.427 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.427 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.410 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.410 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.6.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.427 algo-1:58 INFO hook.py:591] name:module.model.encoder.layernorm_embedding.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.427 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.4.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.410 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.410 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.427 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.2.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.428 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.428 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.1.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.428 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.428 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.2.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.428 algo-1:58 INFO hook.py:591] name:module.model.encoder.layernorm_embedding.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.428 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.4.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.428 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.1.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.428 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.4.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.428 algo-1:58 INFO hook.py:591] name:module.model.decoder.embed_positions.weight count_params:1050624\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.428 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.4.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.428 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.428 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.4.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.428 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.428 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.428 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.428 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.428 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.4.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.428 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.428 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.4.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.428 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.410 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.1.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.428 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.428 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.428 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.410 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.410 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.6.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.410 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.410 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.410 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.428 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.428 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.428 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.410 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.2.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.428 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.428 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.4.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.410 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.6.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.428 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.410 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.410 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.428 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.410 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.428 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.410 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.6.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.429 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.429 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.410 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.410 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.429 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.410 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.6.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.429 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.4.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.429 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.410 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.429 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.429 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.429 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.410 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.429 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.4.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.429 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.429 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.4.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.429 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.410 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.410 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.429 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.429 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.4.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.429 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.429 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.410 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.2.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.429 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.4.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.429 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.429 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.4.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.410 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.4.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.429 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.429 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.4.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.429 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.429 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.410 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.6.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.429 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.429 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.429 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.429 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.429 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.4.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.410 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.429 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.429 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.410 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.410 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.429 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.429 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.410 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.430 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.430 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.430 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.410 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.4.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.430 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.410 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.430 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.430 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.410 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.410 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.410 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.2.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.410 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.430 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.430 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.430 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.3.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.410 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.410 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.410 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.430 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.430 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.2.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.430 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.410 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.410 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.2.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.410 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.4.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.410 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.4.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.430 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.3.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.430 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.430 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.410 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.2.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.430 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.410 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.410 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.430 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.2.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.410 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.430 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.430 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.410 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.430 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.430 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.3.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.430 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.410 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.4.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.410 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.410 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.430 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.2.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.430 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.430 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.430 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.3.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.410 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.2.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.410 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.2.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.410 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.430 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.430 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.430 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.430 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.410 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.410 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.2.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.430 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.3.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.430 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.410 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.4.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.410 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.6.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.410 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.430 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.430 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.2.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.410 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.410 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.410 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.430 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.430 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.430 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.3.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.430 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.410 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.430 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.2.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.431 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.410 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.6.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.431 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.431 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.431 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.410 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.9.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.410 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.9.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.431 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.2.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.431 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.0.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.431 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.5.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.410 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.2.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.410 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.6.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.410 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.431 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.0.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.431 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.431 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.431 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.431 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.5.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.410 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.2.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.431 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.431 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.431 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.410 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.410 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.431 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.431 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.0.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.431 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.5.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.410 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.431 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.5.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.410 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.9.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.431 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.431 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.410 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.2.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.410 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.6.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.410 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.6.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.431 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.0.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.431 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.410 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.411 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.9.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.411 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.411 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.431 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.0.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.431 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.5.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.431 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.431 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.411 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.411 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.2.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.431 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.5.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.431 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.431 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.431 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.411 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.411 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.6.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.411 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.411 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.431 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.5.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.411 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.431 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.0.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.432 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.432 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.411 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.2.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.411 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.432 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.432 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.5.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.432 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.432 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.5.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.432 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.411 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.411 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.411 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.411 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.432 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.411 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.7.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.432 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.5.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.432 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.432 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.411 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.411 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.411 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.9.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.411 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.411 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.2.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.432 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.432 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.5.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.432 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.432 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.432 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.411 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.7.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.411 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.7.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.432 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.411 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.2.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.432 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.432 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.411 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.9.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.411 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.411 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.432 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.432 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.5.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.432 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.411 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.7.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.432 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.432 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.432 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.4.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.432 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.411 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.411 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.411 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.2.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.432 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.432 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.432 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.432 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.432 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.411 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.411 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.432 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.3.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.411 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.7.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.433 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.4.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.433 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.5.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.411 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.433 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.411 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.433 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.3.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.411 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.411 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.411 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.411 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.2.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.433 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.433 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.5.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.433 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.433 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.4.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.411 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.411 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.7.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.411 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.2.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.411 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.411 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.433 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.433 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.4.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.433 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.5.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.433 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.433 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.3.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.411 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.433 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.3.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.433 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.433 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.5.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.433 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.411 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.411 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.433 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.411 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.411 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.411 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.411 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.411 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.433 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.3.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.433 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.411 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.411 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.411 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.411 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.411 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.433 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.433 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.411 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.433 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.5.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.433 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.433 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.433 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.3.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.433 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.411 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.411 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.411 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.411 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.433 algo-1:64 INFO hook.py:591] name:module.model.decoder.layers.5.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.433 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.411 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.2.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.433 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.433 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.433 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.411 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.411 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.433 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.433 algo-1:64 INFO hook.py:591] name:module.model.decoder.layernorm_embedding.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.434 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.434 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.434 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.434 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.411 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.411 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.411 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.5.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.411 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.434 algo-1:64 INFO hook.py:591] name:module.model.decoder.layernorm_embedding.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.434 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.411 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.7.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.411 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.7.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.411 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.411 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.411 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.434 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.434 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.434 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.434 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.411 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.411 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.411 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.434 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.434 algo-1:64 INFO hook.py:593] Total Trainable Params: 305511424\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.434 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.434 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.434 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.411 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.2.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.411 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.412 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.434 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.411 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.5.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.412 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.5.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.412 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.412 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.434 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.434 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.434 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.6.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.412 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.412 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.434 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.412 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.5.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.434 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.6.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.434 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.1.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.412 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.412 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.412 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-12-01 09:23:05.434 algo-1:64 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.434 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.412 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.412 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.3.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.434 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.1.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.434 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.434 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.6.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.412 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.412 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.2.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.434 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.1.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.434 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.1.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.412 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.434 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.412 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.10.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.434 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.1.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.412 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.434 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.6.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.434 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.1.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.434 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.434 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.6.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.412 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.7.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.412 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.8.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.412 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.8.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.412 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.5.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.435 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.6.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.435 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.435 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.6.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.435 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.4.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.435 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.412 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.3.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.412 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.3.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.435 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.435 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.412 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.435 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.6.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.435 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.6.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.435 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.412 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.10.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.412 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.435 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.4.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.435 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.435 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.412 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.7.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.412 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.7.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.412 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.5.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.435 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.435 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.435 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.412 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.3.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.435 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.6.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.435 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.412 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.10.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.435 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.4.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.412 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.8.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.412 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.8.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.435 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.412 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.435 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.435 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.435 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.435 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.4.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.435 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.412 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.2.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.412 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.3.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.412 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.7.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.412 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.435 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.435 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.435 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.6.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.435 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.6.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.435 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.435 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.4.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.435 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.435 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.412 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.8.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.412 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.8.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.412 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.412 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.412 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.412 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.3.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.435 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.435 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.412 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.3.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.412 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.3.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.412 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.412 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.412 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.2.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.435 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.435 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.435 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.435 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.4.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.412 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.10.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.412 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.10.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.412 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.3.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.435 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.436 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.436 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.4.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.436 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.436 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.436 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.412 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.412 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.412 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.412 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.3.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.436 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.436 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.436 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.436 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.412 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.2.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.436 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.436 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.4.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.412 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.3.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.436 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.436 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.412 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.436 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.436 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.412 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.436 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.412 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.10.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.436 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.5.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.412 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.2.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.412 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.412 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.436 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.436 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.412 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.3.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.436 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.436 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.5.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.436 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.412 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.436 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.5.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.412 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.3.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.436 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.412 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.436 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.412 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.412 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.436 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.5.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.412 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.436 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.436 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.412 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.3.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.412 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.437 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.5.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.437 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.412 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.412 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.3.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.437 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.412 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.437 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.5.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.437 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.437 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.2.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.412 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.412 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.3.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.412 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.413 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.437 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.412 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.437 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.437 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.437 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.2.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.413 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.413 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.413 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.437 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.413 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.3.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.437 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.7.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.437 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.413 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.413 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.437 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.2.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.437 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.437 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.412 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.413 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.413 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.437 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.2.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.437 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.413 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.437 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.437 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.437 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.413 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.3.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.437 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.7.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.437 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.413 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.437 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.413 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.437 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.2.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.437 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.437 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.413 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.413 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.413 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.8.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.413 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.8.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.437 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.7.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.437 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.413 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.413 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.437 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.437 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.2.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.413 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.413 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.437 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.413 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.413 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.9.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.413 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.9.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.413 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.437 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.437 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.437 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.7.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.413 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.413 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.8.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.437 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.7.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.437 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.437 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.438 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.5.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.438 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.6.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.413 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.413 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.413 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.413 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.9.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.438 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.7.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.438 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.6.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.413 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.438 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.438 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.7.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.413 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.438 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.6.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.413 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.8.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.438 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.7.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.438 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.5.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.413 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.413 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.413 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.9.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.413 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.438 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.6.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.438 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.7.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.413 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.413 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.413 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.413 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.8.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.413 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.438 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.5.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.438 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.438 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.6.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.438 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.6.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.438 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.413 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.413 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.9.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.438 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.438 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.438 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.413 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.8.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.413 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.438 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.438 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.5.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.413 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.9.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.413 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.413 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.438 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.413 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.438 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.438 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.7.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.413 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.438 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.413 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.438 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.5.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.413 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.413 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.413 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.438 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.438 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.413 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.413 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.438 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.438 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.7.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.438 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.438 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.5.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.413 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.413 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.413 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.413 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.6.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.413 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.6.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.439 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.438 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.439 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.7.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.413 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.413 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.413 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.413 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.439 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.439 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.439 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.413 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.413 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.413 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.4.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.413 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.439 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.439 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.439 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.439 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.413 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.439 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.439 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.413 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.439 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.439 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.413 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.11.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.413 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.413 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.439 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.413 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.6.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.413 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.4.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.439 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.439 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.7.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.413 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.413 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.439 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.413 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.11.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.413 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.11.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.439 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.439 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.439 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.439 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.7.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.439 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.7.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.439 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.413 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.413 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.439 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.439 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.7.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.413 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.4.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.439 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.413 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.439 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.414 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.6.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.414 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.439 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.7.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.439 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.439 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.439 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.414 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.439 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.7.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.414 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.439 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.439 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.414 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.439 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.439 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.439 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.414 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.4.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.440 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.414 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.440 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.8.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.440 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.414 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.4.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.414 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.4.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.414 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.11.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.414 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.440 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.440 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.440 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.414 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.11.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.414 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.6.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.440 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.414 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.440 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.8.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.440 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.414 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.4.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.440 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.440 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.440 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.414 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.4.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.414 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.440 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.440 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.440 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.440 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.440 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.8.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.440 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.414 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.4.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.414 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.414 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.6.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.414 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.414 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.4.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.440 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.440 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.414 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.440 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.440 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.440 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.414 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.4.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.440 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.440 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.8.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.414 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.4.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.414 algo-2:73 INFO hook.py:591] name:module.model.encoder.layers.11.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.440 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.8.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.440 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.8.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.440 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.8.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.414 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.9.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.414 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.4.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.440 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.440 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.6.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.414 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.10.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.441 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.8.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.441 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.8.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.414 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.4.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.414 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.414 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.414 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.10.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.441 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.8.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.441 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.8.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.441 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.6.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.414 algo-2:73 INFO hook.py:591] name:module.model.encoder.layernorm_embedding.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.414 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.9.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.441 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.8.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.441 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.441 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.8.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.414 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.4.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.414 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.441 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.8.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.441 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.6.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.414 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.10.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.441 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.441 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.3.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.441 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.8.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.414 algo-2:73 INFO hook.py:591] name:module.model.encoder.layernorm_embedding.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.414 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.9.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.414 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.441 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.8.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.414 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.3.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.414 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.441 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.6.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.441 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.414 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.9.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.441 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.441 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.8.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.414 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.10.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.414 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.414 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.441 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.3.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.441 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.414 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.9.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.414 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.9.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.414 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.10.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.414 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.10.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.441 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.441 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.6.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.414 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.414 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.441 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.441 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.3.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.414 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.441 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.8.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.441 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.6.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.441 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.414 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.441 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.414 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.414 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.441 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.3.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.441 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.441 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.441 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.414 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.441 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.414 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.442 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.442 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.3.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.442 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.442 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.414 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.3.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.414 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.442 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.442 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.442 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.442 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.414 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.442 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.414 algo-2:73 INFO hook.py:591] name:module.model.decoder.embed_positions.weight count_params:1050624\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.442 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.3.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.442 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.414 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.414 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.442 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.414 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.4.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.414 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.414 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.414 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.414 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.4.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.415 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.4.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.442 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.442 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.9.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.415 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.415 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.414 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.415 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.442 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.9.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.442 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.442 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.415 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.415 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.415 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.442 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.442 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.9.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.414 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.3.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.442 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.415 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.415 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.442 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.414 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.415 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.415 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.415 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.442 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.415 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.415 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.415 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.415 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.3.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.442 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.442 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.442 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.415 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.415 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.442 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.9.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.442 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.9.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.415 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.415 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.442 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.442 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.415 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.415 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.442 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.9.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.442 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.442 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.442 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.442 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.415 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.415 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.415 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.442 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.443 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.415 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.415 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.415 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.415 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.415 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.443 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.443 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.443 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.443 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.415 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.415 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.415 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.443 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.443 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.9.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.415 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.415 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.415 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.11.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.443 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.415 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.415 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.415 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.443 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.443 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.9.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.443 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.443 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.415 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.5.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.415 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.5.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.415 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.415 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.443 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.443 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.415 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.11.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.443 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.443 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.9.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.415 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.10.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.443 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.415 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.443 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.7.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.443 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.415 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.5.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.443 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.9.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.443 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.443 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.415 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.3.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.415 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.415 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.443 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.443 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.7.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.415 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.443 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.443 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.9.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.443 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.9.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.415 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.11.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.415 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.10.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.443 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.415 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.415 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.5.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.415 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.443 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.7.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.415 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.5.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.415 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.11.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.415 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.10.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.443 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.9.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.443 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.443 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.9.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.443 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.10.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.415 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.5.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.415 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.10.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.443 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.10.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.415 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.415 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.11.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.444 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.7.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.444 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.415 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.444 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.444 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.10.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.415 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.5.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.444 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.9.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.444 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.10.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.444 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.7.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.415 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.10.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.415 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.7.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.444 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.444 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.444 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.10.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.415 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.5.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.444 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.9.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.415 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.415 algo-2:71 INFO hook.py:591] name:module.model.encoder.layers.11.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.444 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.7.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.415 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.444 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.444 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.10.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.444 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.415 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.415 algo-2:71 INFO hook.py:591] name:module.model.encoder.layernorm_embedding.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.415 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.10.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.444 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.415 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.7.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.415 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.7.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.415 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.415 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.444 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.444 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.444 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.9.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.444 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.444 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.415 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.415 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.3.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.415 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.415 algo-2:71 INFO hook.py:591] name:module.model.encoder.layernorm_embedding.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.415 algo-2:71 INFO hook.py:591] name:module.model.decoder.embed_positions.weight count_params:1050624\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.444 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.444 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.9.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.444 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.444 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.415 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.5.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.444 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.415 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.7.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.415 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.415 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.444 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.415 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.415 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.444 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.444 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.444 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.415 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.7.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.416 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.7.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.415 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.5.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.444 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.444 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.415 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.416 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.445 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.445 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.416 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.416 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.445 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.445 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.445 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.416 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.5.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.445 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.416 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.445 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.416 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.445 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.416 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.3.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.416 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.445 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.445 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.445 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.445 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.416 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.5.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.416 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.5.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.416 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.416 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.416 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.5.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.416 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.5.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.445 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.416 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.445 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.445 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.445 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.445 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.416 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.416 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.445 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.416 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.416 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.416 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.416 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.416 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.445 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.445 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.11.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.445 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.416 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.416 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.416 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.5.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.416 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.5.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.445 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.445 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.416 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.416 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.445 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.4.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.445 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.11.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.445 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.10.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.416 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.445 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.416 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.445 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.416 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.445 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.4.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.445 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.11.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.416 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.0.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.416 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.416 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.445 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.11.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.445 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.10.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.445 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.416 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.416 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.416 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.446 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.11.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.446 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.416 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.416 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.446 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.4.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.416 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.446 algo-1:68 INFO hook.py:591] name:module.model.encoder.layers.11.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.446 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.10.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.446 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.8.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.446 algo-1:68 INFO hook.py:591] name:module.model.encoder.layernorm_embedding.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.416 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.416 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.416 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.0.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.416 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.0.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.446 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.416 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.5.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.446 algo-1:68 INFO hook.py:591] name:module.model.encoder.layernorm_embedding.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.446 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.10.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.416 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.446 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.8.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.416 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.416 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.416 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.446 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.4.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.446 algo-1:68 INFO hook.py:591] name:module.model.decoder.embed_positions.weight count_params:1050624\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.416 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.416 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.0.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.446 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.416 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.11.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.446 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.10.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.446 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.8.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.416 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.446 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.446 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.4.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.416 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.416 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.446 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.8.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.446 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.10.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.446 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.446 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.446 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.8.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.446 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.4.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.446 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.416 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.416 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.0.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.416 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.11.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.446 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.416 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.416 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.6.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.416 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.446 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.8.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.446 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.446 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.10.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.446 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.416 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.0.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.416 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.446 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.446 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.446 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.416 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.416 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.6.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.416 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.6.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.446 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.446 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.10.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.416 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.447 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.447 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.416 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.416 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.447 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.416 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.11.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.416 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.11.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.416 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.416 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.447 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.416 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.447 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.447 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.447 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.10.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.416 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.447 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.416 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.447 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.447 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.447 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.416 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.416 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.447 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.447 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.416 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.416 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.416 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.416 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.447 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.10.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.447 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.416 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.447 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.447 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.416 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.416 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.6.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.417 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.6.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.447 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.447 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.10.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.447 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.447 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.416 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.417 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.8.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.417 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.417 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.447 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.417 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.417 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.11.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.417 algo-2:69 INFO hook.py:591] name:module.model.encoder.layers.11.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.447 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.447 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.447 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.10.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.447 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.417 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.417 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.417 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.447 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.417 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.6.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.417 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.447 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.447 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.417 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.8.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.417 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.417 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.447 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.447 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.417 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.417 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.417 algo-2:69 INFO hook.py:591] name:module.model.encoder.layernorm_embedding.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.447 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.447 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.447 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.417 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.417 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.417 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.417 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.448 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.417 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.0.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.448 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.448 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.417 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.417 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.417 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.417 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.417 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.448 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.448 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.448 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.417 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.417 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.448 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.448 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.448 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.448 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.11.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.448 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.417 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.417 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.0.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.448 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.448 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.417 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.448 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.417 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.8.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.417 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.8.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.448 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.11.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.448 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.448 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.417 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.417 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.6.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.448 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.11.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.417 algo-2:69 INFO hook.py:591] name:module.model.encoder.layernorm_embedding.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.417 algo-2:69 INFO hook.py:591] name:module.model.decoder.embed_positions.weight count_params:1050624\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.417 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.448 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.417 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.0.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.417 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.0.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.448 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.0.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.448 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.9.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.417 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.6.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.417 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.6.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.417 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.8.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.448 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.448 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.417 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.417 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.448 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.11.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.448 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.0.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.417 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.417 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.417 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.417 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.417 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.0.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.449 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.417 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.6.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.417 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.6.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.449 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.449 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.0.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.449 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.0.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.449 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.11.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.449 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.9.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.417 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.417 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.417 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.417 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.6.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.417 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.0.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.417 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.449 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.449 algo-1:56 INFO hook.py:591] name:module.model.encoder.layers.11.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.449 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.9.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.417 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.6.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.417 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.8.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.417 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.449 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.449 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.0.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.417 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.417 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.417 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.417 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.449 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.9.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.417 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.6.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.449 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.0.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.417 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.6.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.417 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.6.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.449 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.449 algo-1:56 INFO hook.py:591] name:module.model.encoder.layernorm_embedding.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.417 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.417 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.417 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.449 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.9.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.449 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.449 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.449 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.417 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.417 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.417 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.4.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.417 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.417 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.417 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.6.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.449 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.9.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.449 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.417 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.6.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.417 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.417 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.449 algo-1:56 INFO hook.py:591] name:module.model.encoder.layernorm_embedding.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.449 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.417 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.417 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.417 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.4.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.417 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.417 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.417 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.7.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.418 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.7.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.449 algo-1:56 INFO hook.py:591] name:module.model.decoder.embed_positions.weight count_params:1050624\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.449 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.417 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.417 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.418 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.417 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.417 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.418 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.417 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.418 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.449 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.449 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.449 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.418 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.418 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.449 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.417 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.417 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.418 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.418 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.449 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.449 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.418 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.449 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.449 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.418 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.418 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.418 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.7.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.418 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.7.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.418 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.4.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.450 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.11.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.450 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.450 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.450 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.418 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.418 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.450 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.450 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.418 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.418 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.418 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.418 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.7.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.418 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.7.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.450 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.450 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.11.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.450 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.418 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.418 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.418 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.418 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.450 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.450 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.450 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.418 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.418 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.418 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.418 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.450 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.450 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.450 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.11.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.418 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.4.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.418 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.450 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.450 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.5.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.450 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.418 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.418 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.450 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.418 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.418 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.418 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.450 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.418 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.418 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.418 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.418 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.418 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.450 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.11.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.450 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.450 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.450 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.5.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.450 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.418 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.418 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.418 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.418 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.4.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.418 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.418 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.418 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.450 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.5.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.450 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.418 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.418 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.418 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.418 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.418 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.1.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.418 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.1.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.450 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.418 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.418 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.418 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.450 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.11.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.450 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.418 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.418 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.418 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.418 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.4.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.450 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.450 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.418 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.1.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.418 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.1.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.418 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.7.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.418 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.451 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.5.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.451 algo-1:62 INFO hook.py:591] name:module.model.encoder.layers.11.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.418 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.418 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.9.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.451 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.418 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.418 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.451 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.418 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.1.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.418 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.7.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.451 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.5.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.451 algo-1:62 INFO hook.py:591] name:module.model.encoder.layernorm_embedding.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.418 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.451 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.418 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.418 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.418 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.4.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.451 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.418 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.451 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.451 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.418 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.1.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.451 algo-1:62 INFO hook.py:591] name:module.model.encoder.layernorm_embedding.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.418 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.7.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.451 algo-1:58 INFO hook.py:591] name:module.model.decoder.layers.5.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.418 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.451 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.418 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.418 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.451 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.1.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.451 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.418 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.9.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.418 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.9.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.418 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.451 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.451 algo-1:58 INFO hook.py:591] name:module.model.decoder.layernorm_embedding.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.418 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.451 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.1.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.451 algo-1:62 INFO hook.py:591] name:module.model.decoder.embed_positions.weight count_params:1050624\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.418 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.418 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.7.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.451 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.1.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.418 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.0.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.419 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.0.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.419 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.9.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.451 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.419 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.451 algo-1:58 INFO hook.py:591] name:module.model.decoder.layernorm_embedding.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.451 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.1.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.451 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.10.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.419 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.1.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.419 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.1.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.451 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.451 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.1.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.419 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.419 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.7.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.451 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.419 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.7.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.451 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.10.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.451 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.1.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.451 algo-1:58 INFO hook.py:593] Total Trainable Params: 305511424\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.419 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.0.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.419 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.451 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.452 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.419 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.8.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.419 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.452 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.419 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.7.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.419 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.0.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-12-01 09:23:05.452 algo-1:58 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.452 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.10.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.419 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.1.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.419 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.8.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.452 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.419 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.9.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.419 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.1.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.419 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.7.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.452 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.419 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.8.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.452 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.452 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.419 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.452 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.10.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.419 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.7.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.452 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.452 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.419 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.452 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.452 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.10.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.419 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.0.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.419 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.0.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.452 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.419 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.1.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.419 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.8.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.419 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.452 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.10.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.419 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.419 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.452 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.452 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.452 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.419 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.7.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.419 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.9.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.419 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.419 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.452 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.419 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.1.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.419 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.419 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.8.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.419 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.8.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.452 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.419 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.419 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.452 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.419 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.7.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.419 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.452 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.452 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.419 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.419 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.419 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.7.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.453 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.419 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.419 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.453 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.419 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.419 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.453 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.453 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.453 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.419 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.419 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.419 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.419 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.419 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.419 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.419 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.453 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.419 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.419 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.419 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.419 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.453 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.453 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.453 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.419 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.419 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.419 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.453 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.453 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.453 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.453 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.419 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.419 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.453 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.453 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.0.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.419 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.453 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.419 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.419 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.419 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.453 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.419 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.453 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.0.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.453 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.419 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.419 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.419 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.419 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.453 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.419 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.453 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.453 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.453 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.0.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.419 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.419 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.453 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.419 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.419 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.419 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.454 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.419 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.419 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.419 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.454 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.0.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.454 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.454 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.419 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.419 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.419 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.420 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.454 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.420 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.419 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.454 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.454 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.0.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.454 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.420 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.420 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.454 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.420 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.454 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.420 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.420 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.420 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.420 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.420 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.420 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.420 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.454 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.454 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.0.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.454 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.11.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.454 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.420 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.420 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.454 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.420 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.420 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.420 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.454 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.420 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.420 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.454 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.454 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.11.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.454 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.420 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.420 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.420 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.454 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.454 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.2.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.420 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.420 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.420 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.454 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.2.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.420 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.8.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.420 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.420 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.420 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.455 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.454 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.420 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.455 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.11.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.455 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.2.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.420 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.420 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.420 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.455 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.420 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.455 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.2.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.420 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.455 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.420 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.455 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.11.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.455 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.2.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.420 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.420 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.455 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.2.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.420 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.8.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.455 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.11.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.420 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.455 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.455 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.420 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.420 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.455 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.455 algo-1:67 INFO hook.py:591] name:module.model.encoder.layers.11.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.420 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.10.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.420 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.420 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.455 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.420 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.455 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.455 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.420 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.420 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.455 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.0.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.455 algo-1:67 INFO hook.py:591] name:module.model.encoder.layernorm_embedding.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.420 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.8.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.420 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.420 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.10.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.455 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.455 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.0.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.455 algo-1:67 INFO hook.py:591] name:module.model.encoder.layernorm_embedding.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.420 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.455 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.420 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.8.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.420 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.2.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.420 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.1.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.456 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.420 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.9.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.420 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.9.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.456 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.456 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.0.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.420 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.8.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.456 algo-1:67 INFO hook.py:591] name:module.model.decoder.embed_positions.weight count_params:1050624\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.456 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.420 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.420 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.5.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.456 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.0.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.456 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.420 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.2.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.420 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.1.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.420 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.9.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.456 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.420 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.2.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.456 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.0.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.456 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.420 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.420 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.456 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.420 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.1.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.420 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.9.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.456 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.456 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.456 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.0.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.420 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.2.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.420 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.5.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.456 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.456 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.420 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.2.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.420 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.2.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.420 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.10.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.456 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.420 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.1.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.456 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.420 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.8.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.456 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.456 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.456 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.420 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.9.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.420 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.2.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.420 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.1.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.456 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.457 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.420 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.2.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.420 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.9.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.457 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.420 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.5.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.420 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.2.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.420 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.2.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.420 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.1.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.421 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.457 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.457 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.457 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.420 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.8.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.457 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.420 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.10.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.420 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.10.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.420 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.421 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.457 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.457 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.457 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.420 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.2.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.421 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.2.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.420 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.457 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.421 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.457 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.421 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.457 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.421 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.421 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.8.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.457 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.457 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.457 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.457 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.421 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.421 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.457 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.421 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.10.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.421 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.421 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.421 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.457 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.457 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.421 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.421 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.457 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.457 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.421 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.421 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.421 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.421 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.421 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.457 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.421 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.8.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.421 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.421 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.457 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.457 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.457 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.421 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.421 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.421 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.421 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.8.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.421 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.458 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.458 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.457 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.421 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.421 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.421 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.458 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.421 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.5.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.458 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.1.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.421 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.421 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.458 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.421 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.458 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.421 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.8.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.421 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.458 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.458 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.1.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.421 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.421 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.421 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.421 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.421 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.458 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.458 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.421 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.5.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.421 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.421 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.421 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.458 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.421 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.458 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.1.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.421 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.8.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.458 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.421 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.421 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.458 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.421 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.5.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.458 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.3.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.421 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.421 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.458 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.1.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.421 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.458 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.3.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.458 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.421 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.421 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.421 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.458 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.421 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.421 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.458 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.1.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.458 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.421 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.421 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.458 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.3.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.421 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.5.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.458 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.458 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.1.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.421 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.421 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.458 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.421 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.459 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.3.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.421 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.421 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.459 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.421 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.421 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.421 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.421 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.421 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.459 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.459 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.3.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.421 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.459 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.421 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.9.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.459 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.459 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.3.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.459 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.459 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.421 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.421 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.421 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.459 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.459 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.459 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.459 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.421 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.9.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.422 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.9.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.421 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.422 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.421 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.422 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.459 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.459 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.459 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.422 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.422 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.422 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.422 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.459 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.0.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.459 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.459 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.422 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.459 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.422 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.422 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.422 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.9.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.422 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.9.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.422 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.422 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.422 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.422 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.422 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.459 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.0.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.459 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.422 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.459 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.459 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.422 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.460 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.422 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.11.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.422 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.11.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.422 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.2.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.422 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.2.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.422 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.2.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.422 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.9.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.422 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.460 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.0.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.460 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.422 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.422 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.422 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.422 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.422 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.10.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.422 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.10.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.422 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.10.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.422 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.10.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.460 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.460 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.460 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.0.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.422 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.422 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.422 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.11.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.460 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.422 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.2.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.422 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.422 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.422 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.422 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.2.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.422 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.2.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.460 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.422 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.10.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.460 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.0.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.460 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.1.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.460 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.422 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.460 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.460 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.422 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.422 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.422 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.11.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.422 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.11.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.422 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.422 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.460 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.1.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.422 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.422 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.422 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.422 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.10.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.422 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.422 algo-2:67 INFO hook.py:591] name:module.model.encoder.layers.11.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.460 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.460 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.0.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.460 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.422 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.422 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.460 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.460 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.1.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.422 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.422 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.422 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.9.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.461 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.460 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.422 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.461 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.461 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.1.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.422 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.422 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.3.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.422 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.3.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.461 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.422 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.422 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.422 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.461 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.461 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.1.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.422 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.422 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.9.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.422 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.9.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.461 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.422 algo-2:67 INFO hook.py:591] name:module.model.encoder.layernorm_embedding.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.422 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.422 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.461 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.461 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.422 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.422 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.422 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.461 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.1.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.422 algo-2:67 INFO hook.py:591] name:module.model.encoder.layernorm_embedding.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.461 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.422 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.3.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.422 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.423 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.422 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.461 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.461 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.461 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.422 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.9.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.423 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.9.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.422 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.461 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.423 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.3.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.423 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.3.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.423 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.423 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.3.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.461 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.461 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.461 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.423 algo-2:67 INFO hook.py:591] name:module.model.decoder.embed_positions.weight count_params:1050624\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.423 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.3.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.461 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.461 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.461 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.423 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.461 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.461 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.461 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.462 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.423 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.462 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.462 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.423 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.3.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.462 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.423 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.3.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.462 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.462 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.423 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.462 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.423 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.423 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.423 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.462 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.462 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.423 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.10.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.462 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.423 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.462 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.462 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.4.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.423 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.423 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.423 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.3.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.423 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.3.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.462 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.4.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.462 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.423 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.9.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.423 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.423 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.462 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.462 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.423 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.423 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.10.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.462 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.4.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.423 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.462 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.423 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.3.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.423 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.462 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.4.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.462 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.423 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.10.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.462 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.423 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.423 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.423 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.423 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.423 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.423 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.423 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.423 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.462 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.4.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.462 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.423 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.423 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.423 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.463 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.2.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.423 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.10.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.423 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.10.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.423 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.10.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.463 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.463 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.463 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.4.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.423 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.423 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.423 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.423 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.423 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.423 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.423 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.423 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.423 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.423 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.423 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.423 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.463 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.2.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.463 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.423 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.423 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.423 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.463 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.423 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.423 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.423 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.423 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.423 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.423 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.463 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.463 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.463 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.423 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.463 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.2.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.463 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.423 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.463 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.2.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.463 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.423 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.463 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.423 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.11.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.423 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.463 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.463 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.2.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.423 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.423 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.463 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.423 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.423 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.423 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.463 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.463 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.2.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.463 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.423 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.423 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.463 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.463 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.423 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.11.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.423 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.11.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.463 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.463 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.423 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.424 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.423 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.423 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.423 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.464 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.464 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.464 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.423 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.424 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.424 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.464 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.424 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.464 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.464 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.424 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.424 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.424 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.424 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.11.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.423 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.424 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.424 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.464 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.464 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.424 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.424 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.6.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.424 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.424 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.464 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.424 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.464 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.464 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.464 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.424 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.3.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.424 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.11.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.424 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.424 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.464 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.464 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.464 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.1.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.464 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.424 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.424 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.6.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.424 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.424 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.464 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.464 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.1.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.424 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.424 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.464 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.424 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.3.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.424 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.3.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.464 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.424 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.424 algo-2:74 INFO hook.py:591] name:module.model.encoder.layers.11.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.464 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.424 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.424 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.424 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.6.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.424 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.10.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.424 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.465 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.465 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.1.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.424 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.3.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.424 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.3.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.424 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.465 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.424 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.424 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.465 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.424 algo-2:74 INFO hook.py:591] name:module.model.encoder.layernorm_embedding.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.424 algo-2:74 INFO hook.py:591] name:module.model.encoder.layernorm_embedding.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.465 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.465 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.2.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.424 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.424 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.424 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.424 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.424 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.4.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.424 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.4.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.424 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.4.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.465 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.1.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.424 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.465 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.2.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.465 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.465 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.424 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.3.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.424 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.424 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.424 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.11.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.424 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.10.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.424 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.10.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.424 algo-2:74 INFO hook.py:591] name:module.model.decoder.embed_positions.weight count_params:1050624\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.465 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.1.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.424 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.424 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.4.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.465 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.465 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.424 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.424 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.465 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.2.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.424 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.465 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.1.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.465 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.465 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.424 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.4.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.424 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.6.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.465 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.2.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.424 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.424 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.465 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.424 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.11.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.424 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.10.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.424 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.424 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.424 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.465 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.424 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.4.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.424 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.465 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.2.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.424 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.10.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.465 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.465 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.424 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.424 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.424 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.6.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.466 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.424 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.466 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.2.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.466 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.424 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.11.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.466 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.424 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.424 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.424 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.424 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.424 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.466 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.5.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.424 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.466 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.466 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.424 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.10.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.425 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.466 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.5.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.466 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.425 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.6.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.425 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.425 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.425 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.466 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.466 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.5.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.466 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.466 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.425 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.0.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.425 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.425 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.4.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.425 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.466 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.5.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.466 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.425 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.11.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.425 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.466 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.425 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.4.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.425 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.425 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.425 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.11.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.466 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.5.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.466 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.425 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.0.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.466 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.466 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.425 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.425 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.6.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.425 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.425 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.466 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.466 algo-1:68 INFO hook.py:591] name:module.model.decoder.layers.5.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.466 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.425 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.425 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.4.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.425 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.0.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.467 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.467 algo-1:68 INFO hook.py:591] name:module.model.decoder.layernorm_embedding.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.425 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.425 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.425 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.467 algo-1:68 INFO hook.py:591] name:module.model.decoder.layernorm_embedding.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.425 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.467 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.467 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.425 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.467 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.425 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.4.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.425 algo-2:75 INFO hook.py:591] name:module.model.encoder.layers.11.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.467 algo-1:68 INFO hook.py:593] Total Trainable Params: 305511424\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.467 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.425 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.425 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.4.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.467 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.425 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.467 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.425 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.425 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.467 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.425 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.425 algo-2:75 INFO hook.py:591] name:module.model.encoder.layernorm_embedding.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-12-01 09:23:05.467 algo-1:68 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.467 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.425 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.0.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.467 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.425 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.425 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.4.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.425 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.425 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.467 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.425 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.425 algo-2:75 INFO hook.py:591] name:module.model.encoder.layernorm_embedding.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.467 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.425 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.425 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.425 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.0.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.425 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.425 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.425 algo-2:75 INFO hook.py:591] name:module.model.decoder.embed_positions.weight count_params:1050624\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.425 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.467 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.467 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.425 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.467 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.425 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.425 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.0.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.425 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.425 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.425 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.468 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.425 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.425 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.425 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.425 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.425 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.468 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.3.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.468 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.425 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.425 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.425 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.468 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.425 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.468 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.3.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.468 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.425 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.425 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.425 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.425 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.425 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.425 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.425 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.425 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.425 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.425 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.468 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.425 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.425 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.425 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.426 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.425 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.425 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.426 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.468 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.3.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.468 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.425 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.425 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.425 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.426 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.426 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.426 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.426 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.468 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.468 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.3.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.468 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.426 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.426 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.426 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.426 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.426 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.426 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.426 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.426 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.468 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.468 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.468 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.3.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.426 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.4.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.426 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.4.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.426 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.426 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.426 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.5.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.426 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.469 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.426 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.426 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.11.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.426 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.469 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.469 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.3.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.426 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.4.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.426 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.4.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.426 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.426 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.5.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.469 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.426 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.469 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.426 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.426 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.11.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.469 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.426 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.5.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.426 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.11.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.426 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.4.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.426 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.469 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.426 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.426 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.426 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.426 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.469 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.469 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.426 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.11.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.426 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.4.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.426 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.5.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.469 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.2.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.426 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.426 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.426 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.0.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.469 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.2.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.469 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.469 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.426 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.426 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.11.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.469 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.426 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.426 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.426 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.5.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.426 algo-2:71 INFO hook.py:591] name:module.model.decoder.layers.5.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.469 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.469 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.2.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.426 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.426 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.0.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.426 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.469 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.426 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.426 algo-2:71 INFO hook.py:591] name:module.model.decoder.layernorm_embedding.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.426 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.426 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.0.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.426 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.0.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.426 algo-2:63 INFO hook.py:591] name:module.model.encoder.layers.11.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.470 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.470 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.2.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.470 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.3.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.426 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.470 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.426 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.426 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.426 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.470 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.2.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.426 algo-2:71 INFO hook.py:591] name:module.model.decoder.layernorm_embedding.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.426 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.470 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.3.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.470 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.426 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.426 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.426 algo-2:71 INFO hook.py:593] Total Trainable Params: 305511424\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.426 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.0.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.426 algo-2:63 INFO hook.py:591] name:module.model.encoder.layernorm_embedding.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.470 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.2.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.426 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.426 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.470 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.3.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.426 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.426 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.0.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-12-01 09:23:05.426 algo-2:71 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.426 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.426 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.470 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.426 algo-2:63 INFO hook.py:591] name:module.model.encoder.layernorm_embedding.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.426 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.470 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.3.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.426 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.427 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.470 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.426 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.426 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.470 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.426 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.427 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.427 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.427 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.427 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.470 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.470 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.3.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.427 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.427 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.427 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.470 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.427 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.427 algo-2:63 INFO hook.py:591] name:module.model.decoder.embed_positions.weight count_params:1050624\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.427 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.427 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.427 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.427 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.427 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.427 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.7.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.427 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.427 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.427 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.427 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.427 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.470 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.3.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.427 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.471 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.427 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.427 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.471 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.427 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.427 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.427 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.427 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.7.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.427 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.427 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.5.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.471 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.427 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.7.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.471 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.471 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.427 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.427 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.427 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.427 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.427 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.427 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.471 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.471 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.427 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.427 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.427 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.5.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.427 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.7.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.427 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.471 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.427 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.1.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.427 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.5.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.427 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.427 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.471 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.427 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.427 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.427 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.471 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.427 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.1.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.427 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.1.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.471 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.471 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.427 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.5.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.427 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.5.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.427 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.427 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.471 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.427 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.7.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.427 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.1.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.427 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.1.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.471 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.471 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.471 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.427 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.427 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.427 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.427 algo-2:73 INFO hook.py:591] name:module.model.decoder.layers.5.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.427 algo-2:73 INFO hook.py:591] name:module.model.decoder.layernorm_embedding.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.427 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.0.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.427 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.427 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.427 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.471 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.427 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.0.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.427 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.0.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.427 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.7.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.427 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.1.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.427 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.428 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.5.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.428 algo-2:73 INFO hook.py:591] name:module.model.decoder.layernorm_embedding.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.472 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.472 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.428 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.427 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.472 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.428 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.5.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.428 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.0.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.472 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.428 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.7.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.472 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.428 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.428 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.428 algo-2:73 INFO hook.py:593] Total Trainable Params: 305511424\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.472 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.428 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.472 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.428 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.5.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.428 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.428 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.5.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.472 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-12-01 09:23:05.428 algo-2:73 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.428 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.428 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.472 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.428 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.0.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.428 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.0.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.428 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.472 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.4.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.428 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.5.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.428 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.428 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.472 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.428 algo-2:69 INFO hook.py:591] name:module.model.decoder.layers.5.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.428 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.472 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.4.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.428 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.472 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.428 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.428 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.428 algo-2:69 INFO hook.py:591] name:module.model.decoder.layernorm_embedding.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.428 algo-2:69 INFO hook.py:591] name:module.model.decoder.layernorm_embedding.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.428 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.428 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.428 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.472 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.428 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.428 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.472 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.4.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.472 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.428 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.428 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.428 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.428 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.428 algo-2:69 INFO hook.py:593] Total Trainable Params: 305511424\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.473 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.472 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.4.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.473 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.428 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-12-01 09:23:05.428 algo-2:69 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.428 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.428 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.473 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.4.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.428 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.428 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.428 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.473 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.428 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.428 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.428 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.428 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.428 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.473 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.4.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.428 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.473 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.428 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.428 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.473 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.428 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.428 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.429 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.429 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.473 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.473 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.429 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.429 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.1.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.429 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.429 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.0.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.429 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.1.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.429 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.473 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.429 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.0.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.429 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.429 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.1.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.473 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.473 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.429 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.429 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.1.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.429 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.473 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.429 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.0.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.473 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.429 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.1.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.473 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.429 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.429 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.1.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.473 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.429 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.429 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.473 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.429 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.429 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.0.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.429 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.473 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.473 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.429 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.429 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.473 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.429 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.0.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.429 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.429 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.474 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.3.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.429 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.429 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.0.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.429 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.474 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.429 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.429 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.429 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.474 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.3.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.429 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.429 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.429 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.429 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.429 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.429 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.429 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.8.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.474 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.429 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.430 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.430 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.430 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.430 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.430 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.430 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.430 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.8.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.474 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.474 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.474 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.3.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.430 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.430 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.430 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.430 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.474 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.3.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.474 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.430 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.1.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.430 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.430 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.430 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.1.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.430 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.474 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.430 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.474 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.430 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.1.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.430 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.8.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.430 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.1.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.474 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.4.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.430 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.2.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.430 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.8.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.430 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.1.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.430 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.474 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.3.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.474 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.430 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.430 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.2.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.430 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.8.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.430 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.2.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.430 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.430 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.475 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.4.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.475 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.3.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.430 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.8.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.430 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.2.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.430 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.1.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.475 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.430 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.475 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.430 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.475 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.4.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.430 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.430 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.2.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.430 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.8.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.475 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.431 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.431 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.2.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.431 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.475 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.4.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.431 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.431 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.431 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.475 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.431 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.431 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.431 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.431 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.431 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.475 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.431 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.475 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.4.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.431 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.431 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.431 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.475 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.431 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.431 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.475 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.431 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.431 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.431 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.475 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.4.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.431 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.431 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.431 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.431 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.431 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.431 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.431 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.475 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.475 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.431 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.431 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.431 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.431 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.431 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.475 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.431 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.431 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.431 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.431 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.431 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.431 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.476 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.431 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.431 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.431 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.431 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.476 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.476 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.431 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.2.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.431 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.432 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.432 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.476 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.432 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.2.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.476 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.432 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.432 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.432 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.432 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.432 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.2.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.476 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.432 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.432 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.432 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.2.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.476 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.432 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.432 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.476 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.476 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.432 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.432 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.432 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.432 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.432 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.432 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.2.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.432 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.432 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.2.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.432 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.476 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.476 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.476 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.432 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.432 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.9.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.432 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.432 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.432 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.432 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.432 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.432 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.9.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.476 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.476 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.432 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.432 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.476 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.432 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.1.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.432 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.2.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.432 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.1.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.432 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.9.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.432 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.432 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.2.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.477 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.432 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.477 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.477 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.432 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.432 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.2.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.432 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.9.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.432 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.432 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.433 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.9.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.432 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.1.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.477 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.5.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.477 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.433 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.2.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.432 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.433 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.477 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.433 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.433 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.433 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.1.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.433 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.2.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.433 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.433 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.9.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.433 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.3.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.433 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.1.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.477 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.477 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.5.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.433 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.2.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.433 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.3.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.433 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.433 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.1.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.433 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.433 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.433 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.3.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.433 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.477 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.477 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.5.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.433 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.3.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.433 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.433 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.9.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.433 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.477 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.433 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.433 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.433 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.477 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.477 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.5.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.433 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.433 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.3.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.433 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.433 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.477 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.433 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.433 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.433 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.433 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.3.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.477 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.5.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.477 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.433 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.433 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.433 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.433 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.433 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.433 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.433 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.433 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.433 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.478 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.478 algo-1:56 INFO hook.py:591] name:module.model.decoder.layers.5.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.478 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.433 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.433 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.433 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.433 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.478 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.478 algo-1:56 INFO hook.py:591] name:module.model.decoder.layernorm_embedding.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.478 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.433 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.433 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.433 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.433 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.433 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.434 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.434 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.478 algo-1:56 INFO hook.py:591] name:module.model.decoder.layernorm_embedding.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.478 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.478 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.434 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.434 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.434 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.434 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.434 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.434 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.434 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.434 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.478 algo-1:56 INFO hook.py:593] Total Trainable Params: 305511424\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.478 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.434 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.434 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.434 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.478 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.434 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.434 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.434 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.434 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.434 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.434 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.434 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.478 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.434 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.434 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.434 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.434 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.478 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-12-01 09:23:05.478 algo-1:56 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.434 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.434 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.3.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.434 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.434 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.478 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.478 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.434 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.434 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.434 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.434 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.434 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.3.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.434 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.3.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.434 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.434 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.479 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.434 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.3.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.434 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.434 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.434 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.435 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.3.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.479 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.4.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.435 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.435 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.2.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.435 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.3.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.435 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.3.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.435 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.3.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.479 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.479 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.4.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.435 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.2.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.435 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.435 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.3.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.479 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.4.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.435 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.2.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.435 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.3.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.435 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.435 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.479 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.435 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.3.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.435 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.435 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.435 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.435 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.435 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.3.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.435 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.435 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.2.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.435 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.2.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.479 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.4.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.435 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.435 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.479 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.5.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.435 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.435 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.435 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.435 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.2.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.435 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.479 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.4.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.435 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.435 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.435 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.435 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.435 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.435 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.435 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.435 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.435 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.435 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.435 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.435 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.435 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.479 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.5.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.480 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.4.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.435 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.435 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.435 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.480 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.5.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.435 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.4.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.435 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.435 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.435 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.4.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.435 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.436 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.480 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.436 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.4.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.436 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.436 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.436 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.436 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.480 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.436 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.480 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.5.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.436 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.4.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.436 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.436 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.436 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.436 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.480 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.436 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.4.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.436 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.10.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.436 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.436 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.436 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.10.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.436 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.436 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.436 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.436 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.4.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.480 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.436 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.10.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.436 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.436 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.480 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.5.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.436 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.436 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.436 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.436 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.436 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.436 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.436 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.436 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.10.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.436 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.10.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.436 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.436 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.480 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.436 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.436 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.480 algo-1:62 INFO hook.py:591] name:module.model.decoder.layers.5.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.436 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.436 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.10.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.436 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.436 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.436 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.437 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.437 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.437 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.4.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.437 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.481 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.437 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.10.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.437 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.4.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.437 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.437 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.437 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.4.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.481 algo-1:62 INFO hook.py:591] name:module.model.decoder.layernorm_embedding.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.437 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.437 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.437 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.481 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.437 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.4.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.437 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.4.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.437 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.437 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.437 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.3.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.437 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.481 algo-1:62 INFO hook.py:591] name:module.model.decoder.layernorm_embedding.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.437 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.4.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.437 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.437 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.437 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.481 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.437 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.437 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.3.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.437 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.437 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.3.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.437 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.437 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.3.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.437 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.3.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.437 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.437 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.481 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.481 algo-1:62 INFO hook.py:593] Total Trainable Params: 305511424\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.437 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.437 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.437 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.437 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.4.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.437 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.3.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.437 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.437 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.437 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.437 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.437 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.4.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.437 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.481 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.437 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.437 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.437 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-12-01 09:23:05.481 algo-1:62 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.437 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.437 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.437 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.437 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.437 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.437 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.4.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.438 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.438 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.4.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.481 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.438 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.438 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.438 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.438 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.438 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.438 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.4.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.438 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.482 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.438 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.438 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.438 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.438 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.438 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.438 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.438 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.438 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.438 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.4.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.438 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.438 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.438 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.482 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.438 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.438 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.438 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.438 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.438 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.438 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.438 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.482 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.438 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.438 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.438 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.438 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.438 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.438 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.438 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.438 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.5.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.438 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.438 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.482 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.438 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.438 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.438 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.438 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.438 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.439 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.439 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.5.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.439 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.439 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.439 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.11.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.439 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.439 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.482 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.439 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.5.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.439 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.439 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.439 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.439 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.5.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.439 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.5.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.483 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.439 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.5.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.439 algo-2:67 INFO hook.py:591] name:module.model.decoder.layers.5.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.439 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.439 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.439 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.4.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.439 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.11.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.439 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.5.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.439 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.5.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.439 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.5.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.439 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.11.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.439 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.439 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.4.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.439 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.5.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.483 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.439 algo-2:67 INFO hook.py:591] name:module.model.decoder.layernorm_embedding.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.439 algo-2:67 INFO hook.py:591] name:module.model.decoder.layernorm_embedding.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.439 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.439 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.439 algo-2:75 INFO hook.py:591] name:module.model.decoder.layers.5.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.439 algo-2:75 INFO hook.py:591] name:module.model.decoder.layernorm_embedding.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.439 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.439 algo-2:75 INFO hook.py:591] name:module.model.decoder.layernorm_embedding.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.483 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.439 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.11.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.439 algo-2:67 INFO hook.py:593] Total Trainable Params: 305511424\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.439 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.4.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.439 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.439 algo-2:75 INFO hook.py:593] Total Trainable Params: 305511424\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.439 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.11.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.483 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.439 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.4.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-12-01 09:23:05.439 algo-2:67 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.439 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-12-01 09:23:05.439 algo-2:75 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.439 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.4.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.439 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.439 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.4.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.440 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.11.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.440 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.440 algo-2:65 INFO hook.py:591] name:module.model.encoder.layers.11.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.440 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.483 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.5.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.440 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.440 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.440 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.440 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.440 algo-2:65 INFO hook.py:591] name:module.model.encoder.layernorm_embedding.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.440 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.440 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.440 algo-2:65 INFO hook.py:591] name:module.model.encoder.layernorm_embedding.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.440 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.440 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.484 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.5.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.440 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.440 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.440 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.440 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.5.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.484 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.5.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.440 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.440 algo-2:65 INFO hook.py:591] name:module.model.decoder.embed_positions.weight count_params:1050624\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.440 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.5.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.440 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.440 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.484 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.5.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.440 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.5.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.440 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.440 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.440 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.5.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.440 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.440 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.441 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.5.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.441 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.441 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.441 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.484 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.5.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.441 algo-2:74 INFO hook.py:591] name:module.model.decoder.layers.5.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.441 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.441 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.441 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.441 algo-2:74 INFO hook.py:591] name:module.model.decoder.layernorm_embedding.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.484 algo-1:67 INFO hook.py:591] name:module.model.decoder.layers.5.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.441 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.441 algo-2:74 INFO hook.py:591] name:module.model.decoder.layernorm_embedding.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.441 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.441 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.441 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.441 algo-2:74 INFO hook.py:593] Total Trainable Params: 305511424\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.441 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.5.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.441 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.484 algo-1:67 INFO hook.py:591] name:module.model.decoder.layernorm_embedding.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.441 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.5.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.441 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.5.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-12-01 09:23:05.441 algo-2:74 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.441 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.5.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.441 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.5.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.441 algo-2:63 INFO hook.py:591] name:module.model.decoder.layers.5.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.485 algo-1:67 INFO hook.py:591] name:module.model.decoder.layernorm_embedding.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.441 algo-2:63 INFO hook.py:591] name:module.model.decoder.layernorm_embedding.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.442 algo-2:63 INFO hook.py:591] name:module.model.decoder.layernorm_embedding.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.442 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.442 algo-2:63 INFO hook.py:593] Total Trainable Params: 305511424\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-12-01 09:23:05.442 algo-2:63 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.442 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.442 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.442 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.0.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.443 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.443 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.443 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.443 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.443 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.443 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.443 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.485 algo-1:67 INFO hook.py:593] Total Trainable Params: 305511424\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.444 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.444 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.445 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.0.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.445 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.0.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.446 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.0.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.446 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.0.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.446 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.0.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.446 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.0.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.446 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.0.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.446 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.446 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-12-01 09:23:05.485 algo-1:67 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.447 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.447 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.447 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.447 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.448 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.448 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.448 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.449 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.1.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.449 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.449 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.450 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.450 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.450 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.451 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.451 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.451 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.451 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.451 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.1.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.451 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.1.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.451 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.1.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.452 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.1.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.452 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.1.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.452 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.1.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.452 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.1.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.452 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.452 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.452 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.452 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.453 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.453 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.453 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.453 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.454 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.454 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.2.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.454 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.454 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.454 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.454 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.454 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.454 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.454 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.455 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.455 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.455 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.2.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.455 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.2.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.455 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.2.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.455 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.2.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.456 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.2.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.456 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.2.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.456 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.2.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.456 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.456 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.456 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.456 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.456 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.457 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.457 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.457 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.457 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.457 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.3.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.457 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.457 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.458 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.458 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.458 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.458 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.459 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.459 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.459 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.460 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.3.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.460 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.3.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.461 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.3.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.461 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.3.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.461 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.3.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.461 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.3.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.461 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.3.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.462 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.462 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.462 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.462 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.462 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.462 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.463 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.463 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.463 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.463 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.4.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.463 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.463 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.463 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.463 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.463 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.464 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.464 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.464 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.464 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.464 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.4.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.464 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.4.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.464 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.4.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.465 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.4.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.465 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.4.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.465 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.4.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.465 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.4.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.465 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.465 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.465 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.466 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.466 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.466 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.466 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.466 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.466 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.466 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.5.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.466 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.467 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.467 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.467 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.467 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.467 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.467 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.467 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.467 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.468 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.5.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.468 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.5.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.468 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.5.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.468 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.5.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.468 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.5.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.468 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.5.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.468 algo-2:65 INFO hook.py:591] name:module.model.decoder.layers.5.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.468 algo-2:65 INFO hook.py:591] name:module.model.decoder.layernorm_embedding.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.469 algo-2:65 INFO hook.py:591] name:module.model.decoder.layernorm_embedding.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.469 algo-2:65 INFO hook.py:593] Total Trainable Params: 305511424\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-12-01 09:23:05.469 algo-2:65 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:63:823 [0] NCCL INFO Launch mode Parallel\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:56:799 [0] NCCL INFO Launch mode Parallel\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:{'loss': 2.1871, 'learning_rate': 1.6933333333333333e-05, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:{'loss': 2.1496, 'learning_rate': 1.6933333333333333e-05, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Saving model checkpoint to /opt/ml/model/checkpoint-500\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Configuration saved in /opt/ml/model/checkpoint-500/config.json\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Model weights saved in /opt/ml/model/checkpoint-500/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:tokenizer config file saved in /opt/ml/model/checkpoint-500/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Special tokens file saved in /opt/ml/model/checkpoint-500/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Training completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:Training completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:{'train_runtime': 1127.352, 'train_samples_per_second': 0.665, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:{'train_runtime': 1127.3319, 'train_samples_per_second': 0.665, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:12/01/2021 09:41:53 - INFO - __main__ -   *** Evaluate ***\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  Num examples = 2000\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  Batch size = 4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:***** train metrics *****\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  epoch                      =        3.0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  init_mem_cpu_alloc_delta   =     -200MB\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  init_mem_cpu_peaked_delta  =      199MB\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  init_mem_gpu_alloc_delta   =     1165MB\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  init_mem_gpu_peaked_delta  =        0MB\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  train_mem_cpu_alloc_delta  =     1549MB\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  train_mem_cpu_peaked_delta =      516MB\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  train_mem_gpu_alloc_delta  =     4662MB\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  train_mem_gpu_peaked_delta =     5276MB\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  train_runtime              = 0:18:47.35\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  train_samples              =      16000\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  train_samples_per_second   =      0.665\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:12/01/2021 09:41:55 - INFO - __main__ -   *** Evaluate ***\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  Num examples = 2000\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  Batch size = 4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:***** eval metrics *****\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  epoch                     =        3.0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  eval_gen_len              =       20.0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  eval_loss                 =     2.2528\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  eval_mem_cpu_alloc_delta  =       35MB\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  eval_mem_cpu_peaked_delta =        1MB\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  eval_mem_gpu_alloc_delta  =        0MB\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  eval_mem_gpu_peaked_delta =      467MB\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  eval_rouge1               =    42.8812\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  eval_rouge2               =    22.7851\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  eval_rougeL               =    37.8441\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  eval_rougeLsum            =    37.8592\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  eval_runtime              = 0:00:44.06\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  eval_samples              =       2000\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  eval_samples_per_second   =     45.385\u001b[0m\n",
      "\u001b[34mWarning: Permanently added 'algo-2,10.2.219.113' (ECDSA) to the list of known hosts.#015\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:#0150 tables [00:00, ? tables/s][1,12]<stderr>:#0150 tables [00:00, ? tables/s][1,7]<stderr>:#0151 tables [00:00,  7.42 tables/s][1,7]<stderr>:#015                                #015[1,7]<stderr>:#0150 tables [00:00, ? tables/s][1,7]<stderr>:#015                            #015[1,12]<stderr>:#0151 tables [00:00,  7.34 tables/s][1,7]<stderr>:#015Downloading:   0%|          | 0.00/1.80k [00:00<?, ?B/s][1,7]<stderr>:#015Downloading: 100%|██████████| 1.80k/1.80k [00:00<00:00, 2.06MB/s]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:loading configuration file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/adac95cf641be69365b3dd7fe00d4114b3c7c77fb0572931db31a92d4995053b.a50597c2c8b540e8d07e03ca4d58bf615a365f134fb10ca988f4f67881789178\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:Model config BartConfig {\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"_num_labels\": 3,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"activation_dropout\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"activation_function\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"add_bias_logits\": false,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"add_final_layer_norm\": false,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    \"BartForConditionalGeneration\"\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  ],\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"attention_dropout\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"classif_dropout\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"classifier_dropout\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"d_model\": 1024,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"decoder_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"decoder_ffn_dim\": 4096,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"decoder_layerdrop\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"decoder_layers\": 6,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"dropout\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"encoder_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"encoder_ffn_dim\": 4096,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"encoder_layerdrop\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"encoder_layers\": 12,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"extra_pos_embeddings\": 2,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"force_bos_token_to_be_generated\": true,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"id2label\": {\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    \"0\": \"LABEL_0\",\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    \"1\": \"LABEL_1\",\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    \"2\": \"LABEL_2\"\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  },\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"init_std\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"is_encoder_decoder\": true,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"label2id\": {\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    \"LABEL_0\": 0,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    \"LABEL_1\": 1,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    \"LABEL_2\": 2\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  },\u001b[0m\n",
      "\u001b[34m2021-12-01 09:42:46,275 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"max_position_embeddings\": 1024,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"model_type\": \"bart\",\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"normalize_before\": false,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"normalize_embedding\": true,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"num_hidden_layers\": 12,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"output_past\": true,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"prefix\": \" \",\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"replacing_rate\": 0,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"scale_embedding\": false,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"static_position_embeddings\": false,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"student_decoder_layers\": null,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"student_encoder_layers\": null,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"task_specific_params\": {\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    \"summarization\": {\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:      \"early_stopping\": true,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:      \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:      \"max_length\": 142,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:      \"min_length\": 56,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:      \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:      \"num_beams\": 4\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    }\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  },\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"vocab_size\": 50264\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:#015                                #015[1,12]<stderr>:#0150 tables [00:00, ? tables/s][1,0]<stderr>:loading configuration file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/adac95cf641be69365b3dd7fe00d4114b3c7c77fb0572931db31a92d4995053b.a50597c2c8b540e8d07e03ca4d58bf615a365f134fb10ca988f4f67881789178\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:Model config BartConfig {\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"_num_labels\": 3,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"activation_dropout\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"activation_function\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"add_bias_logits\": false,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"add_final_layer_norm\": false,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    \"BartForConditionalGeneration\"\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  ],\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"attention_dropout\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"classif_dropout\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"classifier_dropout\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"d_model\": 1024,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"decoder_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"decoder_ffn_dim\": 4096,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"decoder_layerdrop\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"decoder_layers\": 6,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"dropout\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"encoder_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"encoder_ffn_dim\": 4096,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"encoder_layerdrop\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"encoder_layers\": 12,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"extra_pos_embeddings\": 2,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"force_bos_token_to_be_generated\": true,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"id2label\": {\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    \"0\": \"LABEL_0\",\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    \"1\": \"LABEL_1\",\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    \"2\": \"LABEL_2\"\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  },\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"init_std\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"is_encoder_decoder\": true,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"label2id\": {\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    \"LABEL_0\": 0,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    \"LABEL_1\": 1,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    \"LABEL_2\": 2\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  },\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"max_position_embeddings\": 1024,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"model_type\": \"bart\",\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"normalize_before\": false,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"normalize_embedding\": true,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"num_hidden_layers\": 12,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"output_past\": true,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"prefix\": \" \",\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"replacing_rate\": 0,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"scale_embedding\": false,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"static_position_embeddings\": false,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"student_decoder_layers\": null,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"student_encoder_layers\": null,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"task_specific_params\": {\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    \"summarization\": {\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:      \"early_stopping\": true,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:      \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:      \"max_length\": 142,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:      \"min_length\": 56,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:      \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:      \"num_beams\": 4\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    }\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  },\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  \"vocab_size\": 50264\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:#015                            #015[1,7]<stderr>:#015Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s][1,7]<stderr>:#015Downloading: 100%|██████████| 899k/899k [00:00<00:00, 45.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:#015Downloading:   0%|          | 0.00/1.80k [00:00<?, ?B/s][1,14]<stderr>:#015Downloading: 100%|██████████| 1.80k/1.80k [00:00<00:00, 1.17MB/s]\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:#015Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s][1,8]<stderr>:loading configuration file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/adac95cf641be69365b3dd7fe00d4114b3c7c77fb0572931db31a92d4995053b.a50597c2c8b540e8d07e03ca4d58bf615a365f134fb10ca988f4f67881789178\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:Model config BartConfig {\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"_num_labels\": 3,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"activation_dropout\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"activation_function\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"add_bias_logits\": false,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"add_final_layer_norm\": false,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:    \"BartForConditionalGeneration\"\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  ],\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"attention_dropout\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"classif_dropout\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"classifier_dropout\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"d_model\": 1024,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"decoder_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"decoder_ffn_dim\": 4096,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"decoder_layerdrop\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"decoder_layers\": 6,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"dropout\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"encoder_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"encoder_ffn_dim\": 4096,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"encoder_layerdrop\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"encoder_layers\": 12,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"extra_pos_embeddings\": 2,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"force_bos_token_to_be_generated\": true,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"id2label\": {\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:    \"0\": \"LABEL_0\",\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:    \"1\": \"LABEL_1\",\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:    \"2\": \"LABEL_2\"\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  },\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"init_std\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"is_encoder_decoder\": true,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"label2id\": {\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:    \"LABEL_0\": 0,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:    \"LABEL_1\": 1,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:    \"LABEL_2\": 2\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  },\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"max_position_embeddings\": 1024,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"model_type\": \"bart\",\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"normalize_before\": false,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"normalize_embedding\": true,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"num_hidden_layers\": 12,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"output_past\": true,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"prefix\": \" \",\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"replacing_rate\": 0,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"scale_embedding\": false,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"static_position_embeddings\": false,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"student_decoder_layers\": null,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"student_encoder_layers\": null,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"task_specific_params\": {\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:    \"summarization\": {\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:      \"early_stopping\": true,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:      \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:      \"max_length\": 142,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:      \"min_length\": 56,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:      \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:      \"num_beams\": 4\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:    }\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  },\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"vocab_size\": 50264\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:#015Downloading: 100%|██████████| 456k/456k [00:00<00:00, 45.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:loading configuration file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/adac95cf641be69365b3dd7fe00d4114b3c7c77fb0572931db31a92d4995053b.a50597c2c8b540e8d07e03ca4d58bf615a365f134fb10ca988f4f67881789178\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:Model config BartConfig {\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"_num_labels\": 3,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"activation_dropout\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"activation_function\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"add_bias_logits\": false,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"add_final_layer_norm\": false,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:    \"BartForConditionalGeneration\"\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  ],\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"attention_dropout\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"classif_dropout\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"classifier_dropout\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"d_model\": 1024,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"decoder_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"decoder_ffn_dim\": 4096,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"decoder_layerdrop\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"decoder_layers\": 6,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"dropout\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"encoder_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"encoder_ffn_dim\": 4096,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"encoder_layerdrop\": 0.0,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"encoder_layers\": 12,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"extra_pos_embeddings\": 2,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"force_bos_token_to_be_generated\": true,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"id2label\": {\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:    \"0\": \"LABEL_0\",\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:    \"1\": \"LABEL_1\",\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:    \"2\": \"LABEL_2\"\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  },\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"init_std\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"is_encoder_decoder\": true,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"label2id\": {\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:    \"LABEL_0\": 0,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:    \"LABEL_1\": 1,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:    \"LABEL_2\": 2\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  },\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"max_position_embeddings\": 1024,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"model_type\": \"bart\",\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"normalize_before\": false,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"normalize_embedding\": true,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"num_hidden_layers\": 12,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"output_past\": true,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"prefix\": \" \",\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"replacing_rate\": 0,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"scale_embedding\": false,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"static_position_embeddings\": false,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"student_decoder_layers\": null,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"student_encoder_layers\": null,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"task_specific_params\": {\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:    \"summarization\": {\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:      \"early_stopping\": true,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:      \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:      \"max_length\": 142,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:      \"min_length\": 56,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:      \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:      \"num_beams\": 4\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:    }\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  },\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"transformers_version\": \"4.6.1\",\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  \"vocab_size\": 50264\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:#015Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s][1,14]<stderr>:#015Downloading: 100%|██████████| 899k/899k [00:00<00:00, 46.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:#015Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s][1,7]<stderr>:#015Downloading: 100%|██████████| 26.0/26.0 [00:00<00:00, 31.0kB/s]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:#015Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s][1,14]<stderr>:#015Downloading: 100%|██████████| 456k/456k [00:00<00:00, 38.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/9951e68693b9a7c583ae677e9cb53c02715d9bd0311a78706401372653cdea0a.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/7588c8d398d659b230a038240cc023f67b6848117d2999f06ab625af7bfc7ec1.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/tokenizer.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/f5316f64f9716436994a7ad76a354dc20ecb2dd74eb61d278f103a9c8b80291f.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:#015Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s][1,10]<stderr>:#015Downloading: 100%|██████████| 26.0/26.0 [00:00<00:00, 31.1kB/s]\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/9951e68693b9a7c583ae677e9cb53c02715d9bd0311a78706401372653cdea0a.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/7588c8d398d659b230a038240cc023f67b6848117d2999f06ab625af7bfc7ec1.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/tokenizer.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/f5316f64f9716436994a7ad76a354dc20ecb2dd74eb61d278f103a9c8b80291f.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:#015Downloading:   0%|          | 0.00/1.22G [00:00<?, ?B/s][1,7]<stderr>:#015Downloading:   0%|          | 4.75M/1.22G [00:00<00:25, 47.5MB/s][1,10]<stderr>:#015Downloading:   0%|          | 0.00/1.22G [00:00<?, ?B/s][1,7]<stderr>:#015Downloading:   1%|          | 9.58M/1.22G [00:00<00:25, 47.7MB/s][1,10]<stderr>:#015Downloading:   0%|          | 4.60M/1.22G [00:00<00:26, 46.0MB/s][1,7]<stderr>:#015Downloading:   1%|          | 14.4M/1.22G [00:00<00:25, 48.0MB/s][1,10]<stderr>:#015Downloading:   1%|          | 9.29M/1.22G [00:00<00:26, 46.3MB/s][1,7]<stderr>:#015Downloading:   2%|▏         | 19.4M/1.22G [00:00<00:24, 48.4MB/s][1,10]<stderr>:#015Downloading:   1%|          | 14.2M/1.22G [00:00<00:25, 47.0MB/s][1,7]<stderr>:#015Downloading:   2%|▏         | 24.4M/1.22G [00:00<00:24, 48.9MB/s][1,10]<stderr>:#015Downloading:   2%|▏         | 19.2M/1.22G [00:00<00:25, 47.9MB/s][1,7]<stderr>:#015Downloading:   2%|▏         | 29.4M/1.22G [00:00<00:24, 49.4MB/s][1,10]<stderr>:#015Downloading:   2%|▏         | 24.3M/1.22G [00:00<00:24, 48.7MB/s][1,7]<stderr>:#015Downloading:   3%|▎         | 34.5M/1.22G [00:00<00:23, 49.7MB/s][1,10]<stderr>:#015Downloading:   2%|▏         | 29.4M/1.22G [00:00<00:24, 49.4MB/s][1,7]<stderr>:#015Downloading:   3%|▎         | 39.6M/1.22G [00:00<00:23, 50.1MB/s][1,10]<stderr>:#015Downloading:   3%|▎         | 34.5M/1.22G [00:00<00:23, 49.9MB/s][1,7]<stderr>:#015Downloading:   4%|▎         | 44.7M/1.22G [00:00<00:23, 50.3MB/s][1,10]<stderr>:#015Downloading:   3%|▎         | 39.6M/1.22G [00:00<00:23, 50.3MB/s][1,7]<stderr>:#015Downloading:   4%|▍         | 49.8M/1.22G [00:01<00:23, 50.6MB/s][1,10]<stderr>:#015Downloading:   4%|▎         | 44.8M/1.22G [00:00<00:23, 50.8MB/s][1,7]<stderr>:#015Downloading:   4%|▍         | 54.9M/1.22G [00:01<00:22, 50.8MB/s][1,10]<stderr>:#015Downloading:   4%|▍         | 50.0M/1.22G [00:01<00:22, 51.2MB/s][1,7]<stderr>:#015Downloading:   5%|▍         | 60.1M/1.22G [00:01<00:22, 51.0MB/s][1,10]<stderr>:#015Downloading:   5%|▍         | 55.2M/1.22G [00:01<00:22, 51.3MB/s][1,7]<stderr>:#015Downloading:   5%|▌         | 65.2M/1.22G [00:01<00:22, 51.1MB/s][1,10]<stderr>:#015Downloading:   5%|▍         | 60.4M/1.22G [00:01<00:22, 51.5MB/s][1,7]<stderr>:#015Downloading:   6%|▌         | 70.4M/1.22G [00:01<00:22, 51.3MB/s][1,10]<stderr>:#015Downloading:   5%|▌         | 65.6M/1.22G [00:01<00:22, 51.6MB/s][1,7]<stderr>:#015Downloading:   6%|▌         | 75.6M/1.22G [00:01<00:22, 51.4MB/s][1,10]<stderr>:#015Downloading:   6%|▌         | 70.8M/1.22G [00:01<00:22, 51.8MB/s][1,7]<stderr>:#015Downloading:   7%|▋         | 80.7M/1.22G [00:01<00:22, 51.4MB/s][1,10]<stderr>:#015Downloading:   6%|▌         | 76.0M/1.22G [00:01<00:22, 51.8MB/s][1,7]<stderr>:#015Downloading:   7%|▋         | 85.9M/1.22G [00:01<00:22, 51.5MB/s][1,10]<stderr>:#015Downloading:   7%|▋         | 81.1M/1.22G [00:01<00:22, 51.8MB/s][1,7]<stderr>:#015Downloading:   7%|▋         | 91.1M/1.22G [00:01<00:21, 51.6MB/s][1,10]<stderr>:#015Downloading:   7%|▋         | 86.3M/1.22G [00:01<00:21, 51.8MB/s][1,7]<stderr>:#015Downloading:   8%|▊         | 96.2M/1.22G [00:01<00:21, 51.6MB/s][1,10]<stderr>:#015Downloading:   7%|▋         | 91.5M/1.22G [00:01<00:21, 51.9MB/s][1,7]<stderr>:#015Downloading:   8%|▊         | 101M/1.22G [00:02<00:21, 51.7MB/s] [1,10]<stderr>:#015Downloading:   8%|▊         | 96.7M/1.22G [00:01<00:21, 51.9MB/s][1,7]<stderr>:#015Downloading:   9%|▊         | 107M/1.22G [00:02<00:21, 51.7MB/s][1,10]<stderr>:#015Downloading:   8%|▊         | 102M/1.22G [00:02<00:21, 51.9MB/s] [1,7]<stderr>:#015Downloading:   9%|▉         | 112M/1.22G [00:02<00:21, 51.7MB/s][1,10]<stderr>:#015Downloading:   9%|▉         | 107M/1.22G [00:02<00:21, 52.0MB/s][1,7]<stderr>:#015Downloading:  10%|▉         | 117M/1.22G [00:02<00:21, 51.7MB/s][1,10]<stderr>:#015Downloading:   9%|▉         | 112M/1.22G [00:02<00:21, 52.0MB/s][1,7]<stderr>:#015Downloading:  10%|▉         | 122M/1.22G [00:02<00:21, 51.7MB/s][1,10]<stderr>:#015Downloading:  10%|▉         | 118M/1.22G [00:02<00:21, 52.0MB/s][1,7]<stderr>:#015Downloading:  10%|█         | 127M/1.22G [00:02<00:21, 51.6MB/s][1,10]<stderr>:#015Downloading:  10%|█         | 123M/1.22G [00:02<00:21, 52.0MB/s][1,7]<stderr>:#015Downloading:  11%|█         | 132M/1.22G [00:02<00:21, 51.7MB/s][1,10]<stderr>:#015Downloading:  10%|█         | 128M/1.22G [00:02<00:21, 52.1MB/s][1,7]<stderr>:#015Downloading:  11%|█▏        | 138M/1.22G [00:02<00:21, 51.6MB/s][1,10]<stderr>:#015Downloading:  11%|█         | 133M/1.22G [00:02<00:20, 52.0MB/s][1,7]<stderr>:#015Downloading:  12%|█▏        | 143M/1.22G [00:02<00:20, 51.6MB/s][1,10]<stderr>:#015Downloading:  11%|█▏        | 138M/1.22G [00:02<00:20, 52.0MB/s][1,7]<stderr>:#015Downloading:  12%|█▏        | 148M/1.22G [00:02<00:20, 51.6MB/s][1,10]<stderr>:#015Downloading:  12%|█▏        | 144M/1.22G [00:02<00:20, 52.1MB/s][1,7]<stderr>:#015Downloading:  13%|█▎        | 153M/1.22G [00:03<00:20, 51.7MB/s][1,10]<stderr>:#015Downloading:  12%|█▏        | 149M/1.22G [00:02<00:20, 52.1MB/s][1,7]<stderr>:#015Downloading:  13%|█▎        | 158M/1.22G [00:03<00:20, 51.7MB/s][1,10]<stderr>:#015Downloading:  13%|█▎        | 154M/1.22G [00:03<00:20, 52.1MB/s][1,7]<stderr>:#015Downloading:  13%|█▎        | 164M/1.22G [00:03<00:20, 51.9MB/s][1,10]<stderr>:#015Downloading:  13%|█▎        | 159M/1.22G [00:03<00:20, 52.2MB/s][1,7]<stderr>:#015Downloading:  14%|█▍        | 169M/1.22G [00:03<00:20, 51.9MB/s][1,10]<stderr>:#015Downloading:  13%|█▎        | 164M/1.22G [00:03<00:20, 52.0MB/s][1,7]<stderr>:#015Downloading:  14%|█▍        | 174M/1.22G [00:03<00:20, 52.0MB/s][1,10]<stderr>:#015Downloading:  14%|█▍        | 170M/1.22G [00:03<00:20, 52.2MB/s][1,7]<stderr>:#015Downloading:  15%|█▍        | 179M/1.22G [00:03<00:20, 52.0MB/s][1,10]<stderr>:#015Downloading:  14%|█▍        | 175M/1.22G [00:03<00:20, 52.3MB/s][1,7]<stderr>:#015Downloading:  15%|█▌        | 184M/1.22G [00:03<00:19, 52.0MB/s][1,10]<stderr>:#015Downloading:  15%|█▍        | 180M/1.22G [00:03<00:19, 52.4MB/s][1,7]<stderr>:#015Downloading:  16%|█▌        | 190M/1.22G [00:03<00:19, 51.8MB/s][1,10]<stderr>:#015Downloading:  15%|█▌        | 185M/1.22G [00:03<00:19, 52.3MB/s][1,7]<stderr>:#015Downloading:  16%|█▌        | 195M/1.22G [00:03<00:19, 51.9MB/s][1,10]<stderr>:#015Downloading:  16%|█▌        | 191M/1.22G [00:03<00:19, 52.2MB/s][1,7]<stderr>:#015Downloading:  16%|█▋        | 200M/1.22G [00:03<00:19, 52.0MB/s][1,10]<stderr>:#015Downloading:  16%|█▌        | 196M/1.22G [00:03<00:20, 50.9MB/s][1,7]<stderr>:#015Downloading:  17%|█▋        | 205M/1.22G [00:04<00:19, 52.2MB/s][1,10]<stderr>:#015Downloading:  16%|█▋        | 201M/1.22G [00:03<00:19, 51.2MB/s][1,7]<stderr>:#015Downloading:  17%|█▋        | 210M/1.22G [00:04<00:19, 52.3MB/s][1,10]<stderr>:#015Downloading:  17%|█▋        | 206M/1.22G [00:04<00:19, 51.6MB/s][1,7]<stderr>:#015Downloading:  18%|█▊        | 216M/1.22G [00:04<00:19, 52.6MB/s][1,10]<stderr>:#015Downloading:  17%|█▋        | 212M/1.22G [00:04<00:19, 51.8MB/s][1,7]<stderr>:#015Downloading:  18%|█▊        | 221M/1.22G [00:04<00:19, 52.6MB/s][1,10]<stderr>:#015Downloading:  18%|█▊        | 217M/1.22G [00:04<00:19, 51.6MB/s][1,7]<stderr>:#015Downloading:  19%|█▊        | 226M/1.22G [00:04<00:18, 52.8MB/s][1,10]<stderr>:#015Downloading:  18%|█▊        | 222M/1.22G [00:04<00:19, 51.9MB/s][1,7]<stderr>:#015Downloading:  19%|█▉        | 232M/1.22G [00:04<00:18, 53.0MB/s][1,10]<stderr>:#015Downloading:  19%|█▊        | 227M/1.22G [00:04<00:19, 52.1MB/s][1,7]<stderr>:#015Downloading:  19%|█▉        | 237M/1.22G [00:04<00:18, 52.9MB/s][1,10]<stderr>:#015Downloading:  19%|█▉        | 233M/1.22G [00:04<00:18, 52.2MB/s][1,7]<stderr>:#015Downloading:  20%|█▉        | 242M/1.22G [00:04<00:18, 53.0MB/s][1,10]<stderr>:#015Downloading:  19%|█▉        | 238M/1.22G [00:04<00:18, 52.3MB/s][1,7]<stderr>:#015Downloading:  20%|██        | 248M/1.22G [00:04<00:18, 53.1MB/s][1,10]<stderr>:#015Downloading:  20%|█▉        | 243M/1.22G [00:04<00:18, 52.1MB/s][1,7]<stderr>:#015Downloading:  21%|██        | 253M/1.22G [00:04<00:18, 53.1MB/s][1,10]<stderr>:#015Downloading:  20%|██        | 248M/1.22G [00:04<00:18, 52.2MB/s][1,7]<stderr>:#015Downloading:  21%|██        | 258M/1.22G [00:05<00:18, 53.1MB/s][1,10]<stderr>:#015Downloading:  21%|██        | 253M/1.22G [00:04<00:18, 52.3MB/s][1,7]<stderr>:#015Downloading:  22%|██▏       | 264M/1.22G [00:05<00:18, 53.1MB/s][1,10]<stderr>:#015Downloading:  21%|██        | 259M/1.22G [00:05<00:18, 52.3MB/s][1,7]<stderr>:#015Downloading:  22%|██▏       | 269M/1.22G [00:05<00:17, 53.1MB/s][1,10]<stderr>:#015Downloading:  22%|██▏       | 264M/1.22G [00:05<00:18, 52.4MB/s][1,7]<stderr>:#015Downloading:  22%|██▏       | 274M/1.22G [00:05<00:17, 53.1MB/s][1,10]<stderr>:#015Downloading:  22%|██▏       | 269M/1.22G [00:05<00:18, 52.1MB/s][1,7]<stderr>:#015Downloading:  23%|██▎       | 280M/1.22G [00:05<00:17, 53.1MB/s][1,10]<stderr>:#015Downloading:  22%|██▏       | 274M/1.22G [00:05<00:18, 52.2MB/s][1,7]<stderr>:#015Downloading:  23%|██▎       | 285M/1.22G [00:05<00:17, 53.1MB/s][1,10]<stderr>:#015Downloading:  23%|██▎       | 280M/1.22G [00:05<00:18, 52.3MB/s][1,7]<stderr>:#015Downloading:  24%|██▎       | 290M/1.22G [00:05<00:17, 53.2MB/s][1,10]<stderr>:#015Downloading:  23%|██▎       | 285M/1.22G [00:05<00:18, 51.9MB/s][1,7]<stderr>:#015Downloading:  24%|██▍       | 296M/1.22G [00:05<00:17, 53.1MB/s][1,10]<stderr>:#015Downloading:  24%|██▎       | 290M/1.22G [00:05<00:17, 51.8MB/s][1,7]<stderr>:#015Downloading:  25%|██▍       | 301M/1.22G [00:05<00:17, 53.2MB/s][1,10]<stderr>:#015Downloading:  24%|██▍       | 295M/1.22G [00:05<00:17, 51.8MB/s][1,7]<stderr>:#015Downloading:  25%|██▌       | 306M/1.22G [00:05<00:17, 53.2MB/s][1,10]<stderr>:#015Downloading:  25%|██▍       | 301M/1.22G [00:05<00:17, 52.0MB/s][1,7]<stderr>:#015Downloading:  25%|██▌       | 312M/1.22G [00:06<00:17, 53.3MB/s][1,10]<stderr>:#015Downloading:  25%|██▌       | 306M/1.22G [00:05<00:17, 52.1MB/s][1,7]<stderr>:#015Downloading:  26%|██▌       | 317M/1.22G [00:06<00:16, 53.4MB/s][1,10]<stderr>:#015Downloading:  25%|██▌       | 311M/1.22G [00:06<00:17, 52.1MB/s][1,7]<stderr>:#015Downloading:  26%|██▋       | 322M/1.22G [00:06<00:16, 53.6MB/s][1,10]<stderr>:#015Downloading:  26%|██▌       | 316M/1.22G [00:06<00:17, 52.2MB/s][1,7]<stderr>:#015Downloading:  27%|██▋       | 328M/1.22G [00:06<00:16, 53.6MB/s][1,10]<stderr>:#015Downloading:  26%|██▋       | 321M/1.22G [00:06<00:17, 51.9MB/s][1,7]<stderr>:#015Downloading:  27%|██▋       | 333M/1.22G [00:06<00:16, 53.4MB/s][1,10]<stderr>:#015Downloading:  27%|██▋       | 327M/1.22G [00:06<00:17, 52.0MB/s][1,7]<stderr>:#015Downloading:  28%|██▊       | 338M/1.22G [00:06<00:16, 53.5MB/s][1,10]<stderr>:#015Downloading:  27%|██▋       | 332M/1.22G [00:06<00:17, 52.1MB/s][1,7]<stderr>:#015Downloading:  28%|██▊       | 344M/1.22G [00:06<00:16, 53.7MB/s][1,10]<stderr>:#015Downloading:  28%|██▊       | 337M/1.22G [00:06<00:17, 51.7MB/s][1,7]<stderr>:#015Downloading:  29%|██▊       | 349M/1.22G [00:06<00:16, 53.9MB/s][1,10]<stderr>:#015Downloading:  28%|██▊       | 342M/1.22G [00:06<00:16, 51.9MB/s][1,7]<stderr>:#015Downloading:  29%|██▉       | 355M/1.22G [00:06<00:16, 53.9MB/s][1,10]<stderr>:#015Downloading:  28%|██▊       | 348M/1.22G [00:06<00:16, 51.7MB/s][1,7]<stderr>:#015Downloading:  29%|██▉       | 360M/1.22G [00:06<00:15, 54.1MB/s][1,10]<stderr>:#015Downloading:  29%|██▉       | 353M/1.22G [00:06<00:16, 51.8MB/s][1,7]<stderr>:#015Downloading:  30%|██▉       | 366M/1.22G [00:07<00:15, 54.1MB/s][1,10]<stderr>:#015Downloading:  29%|██▉       | 358M/1.22G [00:06<00:16, 52.0MB/s][1,7]<stderr>:#015Downloading:  30%|███       | 371M/1.22G [00:07<00:15, 54.2MB/s][1,10]<stderr>:#015Downloading:  30%|██▉       | 363M/1.22G [00:07<00:16, 52.1MB/s][1,7]<stderr>:#015Downloading:  31%|███       | 376M/1.22G [00:07<00:15, 54.2MB/s][1,10]<stderr>:#015Downloading:  30%|███       | 368M/1.22G [00:07<00:16, 52.1MB/s][1,7]<stderr>:#015Downloading:  31%|███       | 382M/1.22G [00:07<00:15, 54.0MB/s][1,10]<stderr>:#015Downloading:  31%|███       | 374M/1.22G [00:07<00:16, 52.0MB/s][1,7]<stderr>:#015Downloading:  32%|███▏      | 387M/1.22G [00:07<00:15, 54.2MB/s][1,10]<stderr>:#015Downloading:  31%|███       | 379M/1.22G [00:07<00:16, 52.0MB/s][1,7]<stderr>:#015Downloading:  32%|███▏      | 393M/1.22G [00:07<00:15, 54.3MB/s][1,10]<stderr>:#015Downloading:  31%|███▏      | 384M/1.22G [00:07<00:16, 52.1MB/s][1,7]<stderr>:#015Downloading:  33%|███▎      | 398M/1.22G [00:07<00:15, 54.2MB/s][1,10]<stderr>:#015Downloading:  32%|███▏      | 389M/1.22G [00:07<00:16, 51.8MB/s][1,7]<stderr>:#015Downloading:  33%|███▎      | 404M/1.22G [00:07<00:15, 51.2MB/s][1,10]<stderr>:#015Downloading:  32%|███▏      | 395M/1.22G [00:07<00:15, 51.9MB/s][1,7]<stderr>:#015Downloading:  33%|███▎      | 409M/1.22G [00:07<00:16, 50.6MB/s][1,10]<stderr>:#015Downloading:  33%|███▎      | 400M/1.22G [00:07<00:15, 51.8MB/s][1,7]<stderr>:#015Downloading:  34%|███▍      | 414M/1.22G [00:07<00:16, 50.0MB/s][1,10]<stderr>:#015Downloading:  33%|███▎      | 405M/1.22G [00:07<00:15, 51.9MB/s][1,7]<stderr>:#015Downloading:  34%|███▍      | 419M/1.22G [00:08<00:16, 49.9MB/s][1,10]<stderr>:#015Downloading:  34%|███▎      | 410M/1.22G [00:07<00:15, 52.0MB/s][1,7]<stderr>:#015Downloading:  35%|███▍      | 424M/1.22G [00:08<00:16, 49.8MB/s][1,10]<stderr>:#015Downloading:  34%|███▍      | 416M/1.22G [00:08<00:15, 52.2MB/s][1,7]<stderr>:#015Downloading:  35%|███▌      | 429M/1.22G [00:08<00:16, 49.4MB/s][1,10]<stderr>:#015Downloading:  34%|███▍      | 421M/1.22G [00:08<00:15, 52.2MB/s][1,7]<stderr>:#015Downloading:  35%|███▌      | 434M/1.22G [00:08<00:15, 49.3MB/s][1,10]<stderr>:#015Downloading:  35%|███▍      | 426M/1.22G [00:08<00:15, 52.1MB/s][1,7]<stderr>:#015Downloading:  36%|███▌      | 439M/1.22G [00:08<00:15, 49.3MB/s][1,10]<stderr>:#015Downloading:  35%|███▌      | 431M/1.22G [00:08<00:15, 52.1MB/s][1,7]<stderr>:#015Downloading:  36%|███▋      | 444M/1.22G [00:08<00:15, 49.3MB/s][1,10]<stderr>:#015Downloading:  36%|███▌      | 436M/1.22G [00:08<00:15, 52.1MB/s][1,7]<stderr>:#015Downloading:  37%|███▋      | 449M/1.22G [00:08<00:15, 49.4MB/s][1,10]<stderr>:#015Downloading:  36%|███▌      | 442M/1.22G [00:08<00:15, 51.8MB/s][1,7]<stderr>:#015Downloading:  37%|███▋      | 454M/1.22G [00:08<00:15, 49.0MB/s][1,10]<stderr>:#015Downloading:  37%|███▋      | 447M/1.22G [00:08<00:14, 51.8MB/s][1,7]<stderr>:#015Downloading:  38%|███▊      | 459M/1.22G [00:08<00:15, 49.2MB/s][1,10]<stderr>:#015Downloading:  37%|███▋      | 452M/1.22G [00:08<00:14, 51.4MB/s][1,7]<stderr>:#015Downloading:  38%|███▊      | 464M/1.22G [00:08<00:15, 49.1MB/s][1,10]<stderr>:#015Downloading:  37%|███▋      | 457M/1.22G [00:08<00:14, 51.7MB/s][1,7]<stderr>:#015Downloading:  38%|███▊      | 468M/1.22G [00:09<00:15, 49.1MB/s][1,10]<stderr>:#015Downloading:  38%|███▊      | 462M/1.22G [00:08<00:14, 51.9MB/s][1,7]<stderr>:#015Downloading:  39%|███▊      | 473M/1.22G [00:09<00:15, 48.8MB/s][1,10]<stderr>:#015Downloading:  38%|███▊      | 468M/1.22G [00:09<00:14, 53.6MB/s][1,7]<stderr>:#015Downloading:  39%|███▉      | 478M/1.22G [00:09<00:15, 49.0MB/s][1,10]<stderr>:#015Downloading:  39%|███▉      | 474M/1.22G [00:09<00:13, 55.8MB/s][1,7]<stderr>:#015Downloading:  40%|███▉      | 483M/1.22G [00:09<00:15, 48.7MB/s][1,10]<stderr>:#015Downloading:  39%|███▉      | 481M/1.22G [00:09<00:12, 57.5MB/s][1,7]<stderr>:#015Downloading:  40%|███▉      | 488M/1.22G [00:09<00:15, 48.4MB/s][1,10]<stderr>:#015Downloading:  40%|███▉      | 487M/1.22G [00:09<00:12, 58.7MB/s][1,7]<stderr>:#015Downloading:  40%|████      | 493M/1.22G [00:09<00:14, 48.7MB/s][1,10]<stderr>:#015Downloading:  40%|████      | 493M/1.22G [00:09<00:12, 59.9MB/s][1,7]<stderr>:#015Downloading:  41%|████      | 498M/1.22G [00:09<00:14, 48.9MB/s][1,10]<stderr>:#015Downloading:  41%|████      | 499M/1.22G [00:09<00:11, 60.4MB/s][1,7]<stderr>:#015Downloading:  41%|████      | 503M/1.22G [00:09<00:14, 48.5MB/s][1,10]<std\u001b[0m\n",
      "\u001b[34merr>:#015Downloading:  41%|████▏     | 505M/1.22G [00:09<00:11, 60.8MB/s][1,7]<stderr>:#015Downloading:  42%|████▏     | 508M/1.22G [00:09<00:14, 49.5MB/s][1,10]<stderr>:#015Downloading:  42%|████▏     | 512M/1.22G [00:09<00:11, 61.5MB/s][1,7]<stderr>:#015Downloading:  42%|████▏     | 513M/1.22G [00:09<00:14, 48.9MB/s][1,10]<stderr>:#015Downloading:  42%|████▏     | 518M/1.22G [00:09<00:11, 61.8MB/s][1,7]<stderr>:#015Downloading:  42%|████▏     | 518M/1.22G [00:10<00:14, 48.7MB/s][1,10]<stderr>:#015Downloading:  43%|████▎     | 524M/1.22G [00:09<00:11, 62.1MB/s][1,7]<stderr>:#015Downloading:  43%|████▎     | 523M/1.22G [00:10<00:14, 48.8MB/s][1,10]<stderr>:#015Downloading:  43%|████▎     | 531M/1.22G [00:10<00:11, 62.3MB/s][1,7]<stderr>:#015Downloading:  43%|████▎     | 528M/1.22G [00:10<00:14, 48.9MB/s][1,10]<stderr>:#015Downloading:  44%|████▍     | 537M/1.22G [00:10<00:10, 62.4MB/s][1,7]<stderr>:#015Downloading:  44%|████▎     | 533M/1.22G [00:10<00:14, 49.0MB/s][1,10]<stderr>:#015Downloading:  44%|████▍     | 543M/1.22G [00:10<00:10, 62.4MB/s][1,7]<stderr>:#015Downloading:  44%|████▍     | 538M/1.22G [00:10<00:13, 49.3MB/s][1,10]<stderr>:#015Downloading:  45%|████▍     | 549M/1.22G [00:10<00:10, 62.1MB/s][1,7]<stderr>:#015Downloading:  44%|████▍     | 543M/1.22G [00:10<00:13, 49.9MB/s][1,10]<stderr>:#015Downloading:  45%|████▌     | 556M/1.22G [00:10<00:10, 61.8MB/s][1,7]<stderr>:#015Downloading:  45%|████▍     | 548M/1.22G [00:10<00:13, 49.5MB/s][1,10]<stderr>:#015Downloading:  46%|████▌     | 562M/1.22G [00:10<00:10, 61.8MB/s][1,7]<stderr>:#015Downloading:  45%|████▌     | 553M/1.22G [00:10<00:13, 49.3MB/s][1,10]<stderr>:#015Downloading:  46%|████▋     | 568M/1.22G [00:10<00:10, 61.2MB/s][1,7]<stderr>:#015Downloading:  46%|████▌     | 558M/1.22G [00:10<00:13, 49.5MB/s][1,10]<stderr>:#015Downloading:  47%|████▋     | 574M/1.22G [00:10<00:10, 61.5MB/s][1,7]<stderr>:#015Downloading:  46%|████▌     | 563M/1.22G [00:10<00:13, 49.6MB/s][1,10]<stderr>:#015Downloading:  47%|████▋     | 580M/1.22G [00:10<00:10, 61.9MB/s][1,7]<stderr>:#015Downloading:  46%|████▋     | 568M/1.22G [00:11<00:13, 49.5MB/s][1,10]<stderr>:#015Downloading:  48%|████▊     | 587M/1.22G [00:10<00:10, 62.1MB/s][1,7]<stderr>:#015Downloading:  47%|████▋     | 573M/1.22G [00:11<00:13, 49.0MB/s][1,10]<stderr>:#015Downloading:  49%|████▊     | 593M/1.22G [00:11<00:10, 62.2MB/s][1,7]<stderr>:#015Downloading:  47%|████▋     | 578M/1.22G [00:11<00:13, 49.2MB/s][1,10]<stderr>:#015Downloading:  49%|████▉     | 599M/1.22G [00:11<00:09, 62.4MB/s][1,7]<stderr>:#015Downloading:  48%|████▊     | 582M/1.22G [00:11<00:13, 49.0MB/s][1,10]<stderr>:#015Downloading:  50%|████▉     | 605M/1.22G [00:11<00:09, 62.5MB/s][1,7]<stderr>:#015Downloading:  48%|████▊     | 587M/1.22G [00:11<00:12, 49.2MB/s][1,10]<stderr>:#015Downloading:  50%|█████     | 612M/1.22G [00:11<00:09, 62.6MB/s][1,7]<stderr>:#015Downloading:  48%|████▊     | 592M/1.22G [00:11<00:12, 49.2MB/s][1,10]<stderr>:#015Downloading:  51%|█████     | 618M/1.22G [00:11<00:09, 62.7MB/s][1,7]<stderr>:#015Downloading:  49%|████▉     | 597M/1.22G [00:11<00:12, 49.3MB/s][1,10]<stderr>:#015Downloading:  51%|█████     | 624M/1.22G [00:11<00:09, 62.6MB/s][1,7]<stderr>:#015Downloading:  49%|████▉     | 602M/1.22G [00:11<00:12, 49.2MB/s][1,10]<stderr>:#015Downloading:  52%|█████▏    | 631M/1.22G [00:11<00:09, 62.7MB/s][1,7]<stderr>:#015Downloading:  50%|████▉     | 607M/1.22G [00:11<00:12, 49.0MB/s][1,10]<stderr>:#015Downloading:  52%|█████▏    | 637M/1.22G [00:11<00:09, 62.7MB/s][1,7]<stderr>:#015Downloading:  50%|█████     | 612M/1.22G [00:11<00:12, 49.1MB/s][1,10]<stderr>:#015Downloading:  53%|█████▎    | 643M/1.22G [00:11<00:09, 62.7MB/s][1,7]<stderr>:#015Downloading:  50%|█████     | 617M/1.22G [00:12<00:12, 49.2MB/s][1,10]<stderr>:#015Downloading:  53%|█████▎    | 649M/1.22G [00:11<00:09, 62.8MB/s][1,7]<stderr>:#015Downloading:  51%|█████     | 622M/1.22G [00:12<00:12, 49.1MB/s][1,10]<stderr>:#015Downloading:  54%|█████▎    | 656M/1.22G [00:12<00:09, 62.7MB/s][1,7]<stderr>:#015Downloading:  51%|█████▏    | 627M/1.22G [00:12<00:12, 49.0MB/s][1,10]<stderr>:#015Downloading:  54%|█████▍    | 662M/1.22G [00:12<00:08, 62.9MB/s][1,7]<stderr>:#015Downloading:  52%|█████▏    | 632M/1.22G [00:12<00:11, 49.4MB/s][1,10]<stderr>:#015Downloading:  55%|█████▍    | 668M/1.22G [00:12<00:08, 63.0MB/s][1,7]<stderr>:#015Downloading:  52%|█████▏    | 637M/1.22G [00:12<00:11, 49.5MB/s][1,10]<stderr>:#015Downloading:  55%|█████▌    | 675M/1.22G [00:12<00:08, 63.1MB/s][1,7]<stderr>:#015Downloading:  53%|█████▎    | 642M/1.22G [00:12<00:11, 48.9MB/s][1,10]<stderr>:#015Downloading:  56%|█████▌    | 681M/1.22G [00:12<00:08, 63.2MB/s][1,7]<stderr>:#015Downloading:  53%|█████▎    | 647M/1.22G [00:12<00:11, 49.0MB/s][1,10]<stderr>:#015Downloading:  56%|█████▌    | 687M/1.22G [00:12<00:08, 63.1MB/s][1,7]<stderr>:#015Downloading:  53%|█████▎    | 652M/1.22G [00:12<00:11, 49.3MB/s][1,10]<stderr>:#015Downloading:  57%|█████▋    | 694M/1.22G [00:12<00:08, 63.1MB/s][1,7]<stderr>:#015Downloading:  54%|█████▎    | 657M/1.22G [00:12<00:11, 49.3MB/s][1,10]<stderr>:#015Downloading:  57%|█████▋    | 700M/1.22G [00:12<00:08, 63.0MB/s][1,7]<stderr>:#015Downloading:  54%|█████▍    | 662M/1.22G [00:12<00:11, 48.9MB/s][1,10]<stderr>:#015Downloading:  58%|█████▊    | 706M/1.22G [00:12<00:08, 63.0MB/s][1,7]<stderr>:#015Downloading:  55%|█████▍    | 667M/1.22G [00:13<00:11, 48.7MB/s][1,10]<stderr>:#015Downloading:  58%|█████▊    | 713M/1.22G [00:12<00:08, 63.0MB/s][1,7]<stderr>:#015Downloading:  55%|█████▍    | 672M/1.22G [00:13<00:11, 48.7MB/s][1,10]<stderr>:#015Downloading:  59%|█████▉    | 719M/1.22G [00:13<00:07, 63.1MB/s][1,7]<stderr>:#015Downloading:  55%|█████▌    | 677M/1.22G [00:13<00:11, 49.4MB/s][1,10]<stderr>:#015Downloading:  59%|█████▉    | 725M/1.22G [00:13<00:07, 63.0MB/s][1,7]<stderr>:#015Downloading:  56%|█████▌    | 682M/1.22G [00:13<00:10, 49.5MB/s][1,10]<stderr>:#015Downloading:  60%|█████▉    | 731M/1.22G [00:13<00:07, 63.1MB/s][1,7]<stderr>:#015Downloading:  56%|█████▌    | 687M/1.22G [00:13<00:10, 49.2MB/s][1,10]<stderr>:#015Downloading:  60%|██████    | 738M/1.22G [00:13<00:07, 62.9MB/s][1,7]<stderr>:#015Downloading:  57%|█████▋    | 692M/1.22G [00:13<00:10, 49.5MB/s][1,10]<stderr>:#015Downloading:  61%|██████    | 744M/1.22G [00:13<00:07, 62.9MB/s][1,7]<stderr>:#015Downloading:  57%|█████▋    | 697M/1.22G [00:13<00:10, 49.2MB/s][1,10]<stderr>:#015Downloading:  61%|██████▏   | 750M/1.22G [00:13<00:07, 62.8MB/s][1,7]<stderr>:#015Downloading:  57%|█████▋    | 701M/1.22G [00:13<00:10, 49.4MB/s][1,10]<stderr>:#015Downloading:  62%|██████▏   | 757M/1.22G [00:13<00:07, 62.8MB/s][1,7]<stderr>:#015Downloading:  58%|█████▊    | 706M/1.22G [00:13<00:10, 49.3MB/s][1,10]<stderr>:#015Downloading:  62%|██████▏   | 763M/1.22G [00:13<00:07, 61.1MB/s][1,7]<stderr>:#015Downloading:  58%|█████▊    | 711M/1.22G [00:13<00:10, 48.8MB/s][1,10]<stderr>:#015Downloading:  63%|██████▎   | 769M/1.22G [00:13<00:07, 60.0MB/s][1,7]<stderr>:#015Downloading:  59%|█████▊    | 716M/1.22G [00:14<00:10, 48.8MB/s][1,10]<stderr>:#015Downloading:  63%|██████▎   | 775M/1.22G [00:13<00:07, 60.7MB/s][1,7]<stderr>:#015Downloading:  59%|█████▉    | 721M/1.22G [00:14<00:10, 48.7MB/s][1,10]<stderr>:#015Downloading:  64%|██████▍   | 782M/1.22G [00:14<00:07, 61.4MB/s][1,7]<stderr>:#015Downloading:  59%|█████▉    | 726M/1.22G [00:14<00:10, 49.2MB/s][1,10]<stderr>:#015Downloading:  64%|██████▍   | 788M/1.22G [00:14<00:07, 61.8MB/s][1,7]<stderr>:#015Downloading:  60%|█████▉    | 731M/1.22G [00:14<00:10, 48.9MB/s][1,10]<stderr>:#015Downloading:  65%|██████▍   | 794M/1.22G [00:14<00:06, 62.2MB/s][1,7]<stderr>:#015Downloading:  60%|██████    | 736M/1.22G [00:14<00:09, 49.0MB/s][1,10]<stderr>:#015Downloading:  65%|██████▌   | 800M/1.22G [00:14<00:06, 62.4MB/s][1,7]<stderr>:#015Downloading:  61%|██████    | 741M/1.22G [00:14<00:09, 48.6MB/s][1,10]<stderr>:#015Downloading:  66%|██████▌   | 807M/1.22G [00:14<00:06, 62.5MB/s][1,7]<stderr>:#015Downloading:  61%|██████    | 746M/1.22G [00:14<00:09, 49.2MB/s][1,10]<stderr>:#015Downloading:  67%|██████▋   | 813M/1.22G [00:14<00:06, 62.7MB/s][1,7]<stderr>:#015Downloading:  61%|██████▏   | 751M/1.22G [00:14<00:09, 49.3MB/s][1,10]<stderr>:#015Downloading:  67%|██████▋   | 819M/1.22G [00:14<00:06, 62.2MB/s][1,7]<stderr>:#015Downloading:  62%|██████▏   | 757M/1.22G [00:14<00:09, 51.3MB/s][1,10]<stderr>:#015Downloading:  68%|██████▊   | 826M/1.22G [00:14<00:06, 62.0MB/s][1,7]<stderr>:#015Downloading:  62%|██████▏   | 762M/1.22G [00:15<00:09, 49.5MB/s][1,10]<stderr>:#015Downloading:  68%|██████▊   | 832M/1.22G [00:14<00:06, 62.3MB/s][1,7]<stderr>:#015Downloading:  63%|██████▎   | 767M/1.22G [00:15<00:09, 49.7MB/s][1,10]<stderr>:#015Downloading:  69%|██████▊   | 838M/1.22G [00:14<00:06, 62.4MB/s][1,7]<stderr>:#015Downloading:  63%|██████▎   | 772M/1.22G [00:15<00:09, 49.6MB/s][1,10]<stderr>:#015Downloading:  69%|██████▉   | 844M/1.22G [00:15<00:06, 62.5MB/s][1,7]<stderr>:#015Downloading:  64%|██████▎   | 777M/1.22G [00:15<00:08, 49.6MB/s][1,10]<stderr>:#015Downloading:  70%|██████▉   | 851M/1.22G [00:15<00:05, 62.6MB/s][1,7]<stderr>:#015Downloading:  64%|██████▍   | 782M/1.22G [00:15<00:08, 49.2MB/s][1,10]<stderr>:#015Downloading:  70%|███████   | 857M/1.22G [00:15<00:05, 62.7MB/s][1,7]<stderr>:#015Downloading:  64%|██████▍   | 787M/1.22G [00:15<00:08, 48.9MB/s][1,10]<stderr>:#015Downloading:  71%|███████   | 863M/1.22G [00:15<00:05, 62.8MB/s][1,7]<stderr>:#015Downloading:  65%|██████▍   | 792M/1.22G [00:15<00:08, 48.8MB/s][1,10]<stderr>:#015Downloading:  71%|███████   | 870M/1.22G [00:15<00:05, 63.0MB/s][1,7]<stderr>:#015Downloading:  65%|██████▌   | 796M/1.22G [00:15<00:08, 48.8MB/s][1,10]<stderr>:#015Downloading:  72%|███████▏  | 876M/1.22G [00:15<00:05, 63.0MB/s][1,7]<stderr>:#015Downloading:  66%|██████▌   | 801M/1.22G [00:15<00:08, 48.6MB/s][1,10]<stderr>:#015Downloading:  72%|███████▏  | 882M/1.22G [00:15<00:05, 62.9MB/s][1,7]<stderr>:#015Downloading:  66%|██████▌   | 806M/1.22G [00:15<00:08, 48.6MB/s][1,10]<stderr>:#015Downloading:  73%|███████▎  | 889M/1.22G [00:15<00:05, 62.9MB/s][1,7]<stderr>:#015Downloading:  66%|██████▋   | 812M/1.22G [00:16<00:08, 50.7MB/s][1,10]<stderr>:#015Downloading:  73%|███████▎  | 895M/1.22G [00:15<00:05, 60.6MB/s][1,7]<stderr>:#015Downloading:  67%|██████▋   | 817M/1.22G [00:16<00:08, 49.2MB/s][1,10]<stderr>:#015Downloading:  74%|███████▎  | 901M/1.22G [00:15<00:05, 59.6MB/s][1,7]<stderr>:#015Downloading:  67%|██████▋   | 822M/1.22G [00:16<00:08, 49.3MB/s][1,10]<stderr>:#015Downloading:  74%|███████▍  | 907M/1.22G [00:16<00:05, 60.5MB/s][1,7]<stderr>:#015Downloading:  68%|██████▊   | 827M/1.22G [00:16<00:08, 49.1MB/s][1,10]<stderr>:#015Downloading:  75%|███████▍  | 913M/1.22G [00:16<00:05, 61.2MB/s][1,7]<stderr>:#015Downloading:  68%|██████▊   | 832M/1.22G [00:16<00:07, 49.1MB/s][1,10]<stderr>:#015Downloading:  75%|███████▌  | 920M/1.22G [00:16<00:04, 61.6MB/s][1,7]<stderr>:#015Downloading:  68%|██████▊   | 837M/1.22G [00:16<00:07, 50.0MB/s][1,10]<stderr>:#015Downloading:  76%|███████▌  | 926M/1.22G [00:16<00:04, 62.0MB/s][1,7]<stderr>:#015Downloading:  69%|██████▉   | 842M/1.22G [00:16<00:07, 49.6MB/s][1,10]<stderr>:#015Downloading:  76%|███████▋  | 932M/1.22G [00:16<00:04, 62.3MB/s][1,7]<stderr>:#015Downloading:  69%|██████▉   | 847M/1.22G [00:16<00:07, 49.3MB/s][1,10]<stderr>:#015Downloading:  77%|███████▋  | 939M/1.22G [00:16<00:04, 62.4MB/s][1,7]<stderr>:#015Downloading:  70%|██████▉   | 852M/1.22G [00:16<00:07, 49.5MB/s][1,10]<stderr>:#015Downloading:  77%|███████▋  | 945M/1.22G [00:16<00:04, 62.4MB/s][1,7]<stderr>:#015Downloading:  70%|███████   | 857M/1.22G [00:16<00:07, 49.4MB/s][1,10]<stderr>:#015Downloading:  78%|███████▊  | 951M/1.22G [00:16<00:04, 62.6MB/s][1,7]<stderr>:#015Downloading:  71%|███████   | 862M/1.22G [00:17<00:07, 50.6MB/s][1,10]<stderr>:#015Downloading:  78%|███████▊  | 957M/1.22G [00:16<00:04, 62.7MB/s][1,7]<stderr>:#015Downloading:  71%|███████   | 867M/1.22G [00:17<00:06, 50.8MB/s][1,10]<stderr>:#015Downloading:  79%|███████▉  | 964M/1.22G [00:16<00:04, 62.8MB/s][1,7]<stderr>:#015Downloading:  71%|███████▏  | 873M/1.22G [00:17<00:06, 52.7MB/s][1,10]<stderr>:#015Downloading:  79%|███████▉  | 970M/1.22G [00:17<00:04, 62.9MB/s][1,10]<stderr>:#015Downloading:  80%|███████▉  | 976M/1.22G [00:17<00:03, 62.9MB/s][1,7]<stderr>:#015Downloading:  72%|███████▏  | 878M/1.22G [00:17<00:06, 50.6MB/s][1,10]<stderr>:#015Downloading:  80%|████████  | 983M/1.22G [00:17<00:03, 63.0MB/s][1,7]<stderr>:#015Downloading:  72%|███████▏  | 884M/1.22G [00:17<00:06, 49.9MB/s][1,10]<stderr>:#015Downloading:  81%|████████  | 989M/1.22G [00:17<00:03, 63.0MB/s][1,7]<stderr>:#015Downloading:  73%|███████▎  | 889M/1.22G [00:17<00:06, 49.5MB/s][1,10]<stderr>:#015Downloading:  81%|████████▏ | 995M/1.22G [00:17<00:03, 63.0MB/s][1,7]<stderr>:#015Downloading:  73%|███████▎  | 894M/1.22G [00:17<00:06, 49.4MB/s][1,10]<stderr>:#015Downloading:  82%|████████▏ | 1.00G/1.22G [00:17<00:03, 63.1MB/s][1,7]<stderr>:#015Downloading:  74%|███████▎  | 899M/1.22G [00:17<00:06, 49.1MB/s][1,10]<stderr>:#015Downloading:  82%|████████▏ | 1.01G/1.22G [00:17<00:03, 63.0MB/s][1,7]<stderr>:#015Downloading:  74%|███████▍  | 903M/1.22G [00:17<00:06, 49.3MB/s][1,10]<stderr>:#015Downloading:  83%|████████▎ | 1.01G/1.22G [00:17<00:03, 63.0MB/s][1,7]<stderr>:#015Downloading:  74%|███████▍  | 909M/1.22G [00:17<00:06, 49.6MB/s][1,10]<stderr>:#015Downloading:  83%|████████▎ | 1.02G/1.22G [00:17<00:03, 63.0MB/s][1,7]<stderr>:#015Downloading:  75%|███████▍  | 914M/1.22G [00:18<00:06, 49.7MB/s][1,10]<stderr>:#015Downloading:  84%|████████▍ | 1.03G/1.22G [00:17<00:03, 63.1MB/s][1,7]<stderr>:#015Downloading:  75%|███████▌  | 919M/1.22G [00:18<00:06, 49.5MB/s][1,10]<stderr>:#015Downloading:  85%|████████▍ | 1.03G/1.22G [00:18<00:03, 63.0MB/s][1,7]<stderr>:#015Downloading:  76%|███████▌  | 923M/1.22G [00:18<00:06, 49.1MB/s][1,10]<stderr>:#015Downloading:  85%|████████▌ | 1.04G/1.22G [00:18<00:02, 63.1MB/s][1,7]<stderr>:#015Downloading:  76%|███████▌  | 928M/1.22G [00:18<00:05, 49.2MB/s][1,10]<stderr>:#015Downloading:  86%|████████▌ | 1.05G/1.22G [00:18<00:02, 63.2MB/s][1,7]<stderr>:#015Downloading:  76%|███████▋  | 933M/1.22G [00:18<00:05, 48.9MB/s][1,10]<stderr>:#015Downloading:  86%|████████▌ | 1.05G/1.22G [00:18<00:02, 63.0MB/s][1,7]<stderr>:#015Downloading:  77%|███████▋  | 938M/1.22G [00:18<00:05, 49.0MB/s][1,10]<stderr>:#015Downloading:  87%|████████▋ | 1.06G/1.22G [00:18<00:02, 62.9MB/s][1,7]<stderr>:#015Downloading:  77%|███████▋  | 943M/1.22G [00:18<00\u001b[0m\n",
      "\u001b[34m:05, 48.8MB/s][1,10]<stderr>:#015Downloading:  87%|████████▋ | 1.06G/1.22G [00:18<00:02, 62.9MB/s][1,7]<stderr>:#015Downloading:  78%|███████▊  | 948M/1.22G [00:18<00:05, 49.3MB/s][1,10]<stderr>:#015Downloading:  88%|████████▊ | 1.07G/1.22G [00:18<00:02, 62.7MB/s][1,7]<stderr>:#015Downloading:  78%|███████▊  | 953M/1.22G [00:18<00:05, 48.9MB/s][1,10]<stderr>:#015Downloading:  88%|████████▊ | 1.08G/1.22G [00:18<00:02, 61.8MB/s][1,7]<stderr>:#015Downloading:  78%|███████▊  | 958M/1.22G [00:18<00:05, 48.6MB/s][1,10]<stderr>:#015Downloading:  89%|████████▊ | 1.08G/1.22G [00:18<00:02, 62.0MB/s][1,7]<stderr>:#015Downloading:  79%|███████▉  | 963M/1.22G [00:19<00:05, 48.8MB/s][1,10]<stderr>:#015Downloading:  89%|████████▉ | 1.09G/1.22G [00:18<00:02, 62.2MB/s][1,7]<stderr>:#015Downloading:  79%|███████▉  | 968M/1.22G [00:19<00:05, 49.4MB/s][1,10]<stderr>:#015Downloading:  90%|████████▉ | 1.10G/1.22G [00:19<00:02, 62.4MB/s][1,7]<stderr>:#015Downloading:  80%|███████▉  | 973M/1.22G [00:19<00:04, 50.1MB/s][1,10]<stderr>:#015Downloading:  90%|█████████ | 1.10G/1.22G [00:19<00:01, 62.6MB/s][1,7]<stderr>:#015Downloading:  80%|████████  | 978M/1.22G [00:19<00:04, 49.3MB/s][1,10]<stderr>:#015Downloading:  91%|█████████ | 1.11G/1.22G [00:19<00:01, 62.8MB/s][1,7]<stderr>:#015Downloading:  80%|████████  | 984M/1.22G [00:19<00:04, 50.2MB/s][1,10]<stderr>:#015Downloading:  91%|█████████ | 1.11G/1.22G [00:19<00:01, 62.8MB/s][1,7]<stderr>:#015Downloading:  81%|████████  | 989M/1.22G [00:19<00:04, 51.3MB/s][1,10]<stderr>:#015Downloading:  92%|█████████▏| 1.12G/1.22G [00:19<00:01, 62.9MB/s][1,7]<stderr>:#015Downloading:  81%|████████▏ | 995M/1.22G [00:19<00:04, 52.6MB/s][1,10]<stderr>:#015Downloading:  92%|█████████▏| 1.13G/1.22G [00:19<00:01, 62.9MB/s][1,7]<stderr>:#015Downloading:  82%|████████▏ | 1.00G/1.22G [00:19<00:04, 54.4MB/s][1,7]<stderr>:#015Downloading:  82%|████████▏ | 1.01G/1.22G [00:19<00:03, 55.4MB/s][1,7]<stderr>:#015Downloading:  83%|████████▎ | 1.01G/1.22G [00:19<00:03, 56.1MB/s][1,10]<stderr>:#015Downloading:  93%|█████████▎| 1.13G/1.22G [00:19<00:02, 40.5MB/s][1,7]<stderr>:#015Downloading:  83%|████████▎ | 1.02G/1.22G [00:20<00:03, 56.9MB/s][1,10]<stderr>:#015Downloading:  93%|█████████▎| 1.14G/1.22G [00:19<00:01, 43.4MB/s][1,7]<stderr>:#015Downloading:  84%|████████▍ | 1.02G/1.22G [00:20<00:03, 57.3MB/s][1,10]<stderr>:#015Downloading:  94%|█████████▎| 1.14G/1.22G [00:20<00:01, 45.6MB/s][1,7]<stderr>:#015Downloading:  84%|████████▍ | 1.03G/1.22G [00:20<00:03, 57.8MB/s][1,10]<stderr>:#015Downloading:  94%|█████████▍| 1.15G/1.22G [00:20<00:01, 47.4MB/s][1,7]<stderr>:#015Downloading:  85%|████████▍ | 1.04G/1.22G [00:20<00:03, 59.2MB/s][1,10]<stderr>:#015Downloading:  94%|█████████▍| 1.15G/1.22G [00:20<00:01, 48.7MB/s][1,7]<stderr>:#015Downloading:  85%|████████▌ | 1.04G/1.22G [00:20<00:03, 59.5MB/s][1,10]<stderr>:#015Downloading:  95%|█████████▍| 1.16G/1.22G [00:20<00:01, 49.5MB/s][1,7]<stderr>:#015Downloading:  86%|████████▌ | 1.05G/1.22G [00:20<00:02, 59.1MB/s][1,10]<stderr>:#015Downloading:  95%|█████████▌| 1.17G/1.22G [00:20<00:01, 50.3MB/s][1,7]<stderr>:#015Downloading:  86%|████████▌ | 1.05G/1.22G [00:20<00:02, 58.8MB/s][1,10]<stderr>:#015Downloading:  96%|█████████▌| 1.17G/1.22G [00:20<00:01, 50.5MB/s][1,7]<stderr>:#015Downloading:  87%|████████▋ | 1.06G/1.22G [00:20<00:02, 59.9MB/s][1,10]<stderr>:#015Downloading:  96%|█████████▌| 1.18G/1.22G [00:20<00:00, 50.5MB/s][1,7]<stderr>:#015Downloading:  87%|████████▋ | 1.07G/1.22G [00:20<00:02, 59.2MB/s][1,10]<stderr>:#015Downloading:  97%|█████████▋| 1.18G/1.22G [00:20<00:00, 50.6MB/s][1,7]<stderr>:#015Downloading:  88%|████████▊ | 1.07G/1.22G [00:20<00:02, 58.9MB/s][1,10]<stderr>:#015Downloading:  97%|█████████▋| 1.19G/1.22G [00:20<00:00, 51.1MB/s][1,7]<stderr>:#015Downloading:  88%|████████▊ | 1.08G/1.22G [00:21<00:02, 58.6MB/s][1,10]<stderr>:#015Downloading:  97%|█████████▋| 1.19G/1.22G [00:20<00:00, 51.4MB/s][1,7]<stderr>:#015Downloading:  89%|████████▊ | 1.08G/1.22G [00:21<00:02, 59.1MB/s][1,10]<stderr>:#015Downloading:  98%|█████████▊| 1.20G/1.22G [00:21<00:00, 51.7MB/s][1,7]<stderr>:#015Downloading:  89%|████████▉ | 1.09G/1.22G [00:21<00:02, 60.0MB/s][1,10]<stderr>:#015Downloading:  98%|█████████▊| 1.20G/1.22G [00:21<00:00, 51.7MB/s][1,7]<stderr>:#015Downloading:  90%|████████▉ | 1.10G/1.22G [00:21<00:02, 60.5MB/s][1,10]<stderr>:#015Downloading:  99%|█████████▊| 1.21G/1.22G [00:21<00:00, 51.7MB/s][1,10]<stderr>:#015Downloading:  99%|█████████▉| 1.21G/1.22G [00:21<00:00, 51.6MB/s][1,7]<stderr>:#015Downloading:  90%|█████████ | 1.10G/1.22G [00:21<00:02, 42.7MB/s][1,10]<stderr>:#015Downloading: 100%|█████████▉| 1.22G/1.22G [00:21<00:00, 51.7MB/s][1,7]<stderr>:#015Downloading:  91%|█████████ | 1.11G/1.22G [00:21<00:02, 44.1MB/s][1,10]<stderr>:#015Downloading: 100%|█████████▉| 1.22G/1.22G [00:21<00:00, 51.9MB/s][1,10]<stderr>:#015Downloading: 100%|██████████| 1.22G/1.22G [00:21<00:00, 56.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:loading weights file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/b336fa0b874ea92e3e22f07a7e6f8fa9da01221759c33abeb2679d6d98fe7755.585965cf7e82e4536033cd21d76c486af3d6b1c2a34b3a847840d4e7fe9d8844\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:#015Downloading:  91%|█████████ | 1.11G/1.22G [00:21<00:02, 45.8MB/s][1,7]<stderr>:#015Downloading:  91%|█████████▏| 1.12G/1.22G [00:21<00:02, 46.2MB/s][1,7]<stderr>:#015Downloading:  92%|█████████▏| 1.12G/1.22G [00:22<00:02, 46.9MB/s][1,7]<stderr>:#015Downloading:  92%|█████████▏| 1.13G/1.22G [00:22<00:02, 47.5MB/s][1,7]<stderr>:#015Downloading:  93%|█████████▎| 1.13G/1.22G [00:22<00:01, 48.1MB/s][1,7]<stderr>:#015Downloading:  93%|█████████▎| 1.14G/1.22G [00:22<00:01, 48.3MB/s][1,7]<stderr>:#015Downloading:  93%|█████████▎| 1.14G/1.22G [00:22<00:01, 48.2MB/s][1,7]<stderr>:#015Downloading:  94%|█████████▍| 1.15G/1.22G [00:22<00:01, 48.5MB/s][1,7]<stderr>:#015Downloading:  94%|█████████▍| 1.15G/1.22G [00:22<00:01, 48.6MB/s][1,7]<stderr>:#015Downloading:  95%|█████████▍| 1.16G/1.22G [00:22<00:01, 48.6MB/s][1,7]<stderr>:#015Downloading:  95%|█████████▌| 1.16G/1.22G [00:22<00:01, 48.6MB/s][1,7]<stderr>:#015Downloading:  95%|█████████▌| 1.17G/1.22G [00:22<00:01, 50.0MB/s][1,7]<stderr>:#015Downloading:  96%|█████████▌| 1.17G/1.22G [00:23<00:01, 49.5MB/s][1,7]<stderr>:#015Downloading:  96%|█████████▋| 1.18G/1.22G [00:23<00:00, 51.2MB/s][1,7]<stderr>:#015Downloading:  97%|█████████▋| 1.18G/1.22G [00:23<00:00, 49.7MB/s][1,7]<stderr>:#015Downloading:  97%|█████████▋| 1.19G/1.22G [00:23<00:00, 49.3MB/s][1,7]<stderr>:#015Downloading:  98%|█████████▊| 1.19G/1.22G [00:23<00:00, 49.2MB/s][1,7]<stderr>:#015Downloading:  98%|█████████▊| 1.20G/1.22G [00:23<00:00, 48.6MB/s][1,7]<stderr>:#015Downloading:  98%|█████████▊| 1.20G/1.22G [00:23<00:00, 48.8MB/s][1,7]<stderr>:#015Downloading:  99%|█████████▉| 1.21G/1.22G [00:23<00:00, 49.1MB/s][1,7]<stderr>:#015Downloading:  99%|█████████▉| 1.21G/1.22G [00:23<00:00, 48.9MB/s][1,7]<stderr>:#015Downloading: 100%|█████████▉| 1.22G/1.22G [00:23<00:00, 48.9MB/s][1,7]<stderr>:#015Downloading: 100%|██████████| 1.22G/1.22G [00:24<00:00, 50.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:loading weights file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/b336fa0b874ea92e3e22f07a7e6f8fa9da01221759c33abeb2679d6d98fe7755.585965cf7e82e4536033cd21d76c486af3d6b1c2a34b3a847840d4e7fe9d8844\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:All model checkpoint weights were used when initializing BartForConditionalGeneration.\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:All the weights of BartForConditionalGeneration were initialized from the model checkpoint at sshleifer/distilbart-cnn-12-6.\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:#015  0%|          | 0/16 [00:00<?, ?ba/s][1,8]<stderr>:#015  0%|          | 0/16 [00:00<?, ?ba/s][1,15]<stderr>:#015  0%|          | 0/16 [00:00<?, ?ba/s][1,14]<stderr>:#015  0%|          | 0/16 [00:00<?, ?ba/s][1,10]<stderr>:#015  0%|          | 0/16 [00:00<?, ?ba/s][1,11]<stderr>:#015  0%|          | 0/16 [00:00<?, ?ba/s][1,12]<stderr>:#015  0%|          | 0/16 [00:00<?, ?ba/s][1,13]<stderr>:#015  0%|          | 0/16 [00:00<?, ?ba/s][1,9]<stderr>:#015  6%|▋         | 1/16 [00:00<00:04,  3.57ba/s][1,9]<stderr>:#015 12%|█▎        | 2/16 [00:00<00:04,  3.30ba/s][1,15]<stderr>:#015  6%|▋         | 1/16 [00:00<00:08,  1.77ba/s][1,14]<stderr>:#015  6%|▋         | 1/16 [00:00<00:08,  1.76ba/s][1,8]<stderr>:#015  6%|▋         | 1/16 [00:00<00:10,  1.44ba/s][1,13]<stderr>:#015  6%|▋         | 1/16 [00:00<00:09,  1.59ba/s][1,11]<stderr>:#015  6%|▋         | 1/16 [00:00<00:09,  1.50ba/s][1,12]<stderr>:#015  6%|▋         | 1/16 [00:00<00:10,  1.43ba/s][1,10]<stderr>:#015  6%|▋         | 1/16 [00:00<00:10,  1.38ba/s][1,0]<stderr>:All model checkpoint weights were used when initializing BartForConditionalGeneration.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:All the weights of BartForConditionalGeneration were initialized from the model checkpoint at sshleifer/distilbart-cnn-12-6.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:#015 19%|█▉        | 3/16 [00:01<00:04,  3.10ba/s][1,15]<stderr>:#015 12%|█▎        | 2/16 [00:01<00:07,  1.80ba/s][1,9]<stderr>:#015 25%|██▌       | 4/16 [00:01<00:03,  3.39ba/s][1,14]<stderr>:#015 12%|█▎        | 2/16 [00:01<00:07,  1.76ba/s][1,12]<stderr>:#015 12%|█▎        | 2/16 [00:01<00:08,  1.62ba/s][1,8]<stderr>:#015 12%|█▎        | 2/16 [00:01<00:08,  1.57ba/s][1,11]<stderr>:#015 12%|█▎        | 2/16 [00:01<00:08,  1.63ba/s][1,10]<stderr>:#015 12%|█▎        | 2/16 [00:01<00:09,  1.48ba/s][1,13]<stderr>:#015 12%|█▎        | 2/16 [00:01<00:08,  1.58ba/s][1,9]<stderr>:#015 31%|███▏      | 5/16 [00:01<00:03,  3.19ba/s][1,12]<stderr>:#015 19%|█▉        | 3/16 [00:01<00:06,  1.91ba/s][1,15]<stderr>:#015 19%|█▉        | 3/16 [00:01<00:06,  1.97ba/s][1,8]<stderr>:#015 19%|█▉        | 3/16 [00:01<00:06,  1.87ba/s][1,13]<stderr>:#015 19%|█▉        | 3/16 [00:01<00:06,  1.93ba/s][1,11]<stderr>:#015 19%|█▉        | 3/16 [00:01<00:07,  1.83ba/s][1,14]<stderr>:#015 19%|█▉        | 3/16 [00:01<00:06,  1.86ba/s][1,10]<stderr>:#015 19%|█▉        | 3/16 [00:01<00:07,  1.76ba/s][1,12]<stderr>:#015 25%|██▌       | 4/16 [00:01<00:05,  2.14ba/s][1,8]<stderr>:#015 25%|██▌       | 4/16 [00:01<00:05,  2.08ba/s][1,9]<stderr>:#015 38%|███▊      | 6/16 [00:01<00:03,  3.03ba/s][1,11]<stderr>:#015 25%|██▌       | 4/16 [00:01<00:05,  2.13ba/s][1,15]<stderr>:#015 25%|██▌       | 4/16 [00:01<00:05,  2.09ba/s][1,13]<stderr>:#015 25%|██▌       | 4/16 [00:01<00:05,  2.14ba/s][1,14]<stderr>:#015 25%|██▌       | 4/16 [00:01<00:05,  2.09ba/s][1,10]<stderr>:#015 25%|██▌       | 4/16 [00:01<00:06,  1.97ba/s][1,9]<stderr>:#015 44%|████▍     | 7/16 [00:02<00:02,  3.05ba/s][1,13]<stderr>:#015 31%|███▏      | 5/16 [00:02<00:04,  2.47ba/s][1,12]<stderr>:#015 31%|███▏      | 5/16 [00:02<00:04,  2.28ba/s][1,8]<stderr>:#015 31%|███▏      | 5/16 [00:02<00:04,  2.27ba/s][1,11]<stderr>:#015 31%|███▏      | 5/16 [00:02<00:04,  2.36ba/s][1,6]<stderr>:#015  0%|          | 0/16 [00:00<?, ?ba/s][1,2]<stderr>:#015  0%|          | 0/16 [00:00<?, ?ba/s][1,4]<stderr>:#015  0%|          | 0/16 [00:00<?, ?ba/s][1,7]<stderr>:#015  0%|          | 0/16 [00:00<?, ?ba/s][1,5]<stderr>:#015  0%|          | 0/16 [00:00<?, ?ba/s][1,3]<stderr>:#015  0%|          | 0/16 [00:00<?, ?ba/s][1,0]<stderr>:#015  0%|          | 0/16 [00:00<?, ?ba/s][1,14]<stderr>:#015 31%|███▏      | 5/16 [00:02<00:04,  2.27ba/s][1,1]<stderr>:#015  0%|          | 0/16 [00:00<?, ?ba/s][1,15]<stderr>:#015 31%|███▏      | 5/16 [00:02<00:05,  2.16ba/s][1,10]<stderr>:#015 31%|███▏      | 5/16 [00:02<00:05,  2.19ba/s][1,8]<stderr>:#015 38%|███▊      | 6/16 [00:02<00:03,  2.52ba/s][1,12]<stderr>:#015 38%|███▊      | 6/16 [00:02<00:04,  2.49ba/s][1,13]<stderr>:#015 38%|███▊      | 6/16 [00:02<00:03,  2.58ba/s][1,11]<stderr>:#015 38%|███▊      | 6/16 [00:02<00:03,  2.51ba/s][1,14]<stderr>:#015 38%|███▊      | 6/16 [00:02<00:03,  2.55ba/s][1,15]<stderr>:#015 38%|███▊      | 6/16 [00:02<00:04,  2.48ba/s][1,9]<stderr>:#015 50%|█████     | 8/16 [00:02<00:02,  2.90ba/s][1,2]<stderr>:#015  6%|▋         | 1/16 [00:00<00:06,  2.23ba/s][1,10]<stderr>:#015 38%|███▊      | 6/16 [00:02<00:04,  2.36ba/s][1,6]<stderr>:#015  6%|▋         | 1/16 [00:00<00:07,  1.98ba/s][1,8]<stderr>:#015 44%|████▍     | 7/16 [00:02<00:03,  2.68ba/s][1,12]<stderr>:#015 44%|████▍     | 7/16 [00:02<00:03,  2.58ba/s][1,13]<stderr>:#015 44%|████▍     | 7/16 [00:02<00:03,  2.68ba/s][1,15]<stderr>:#015 44%|████▍     | 7/16 [00:02<00:03,  2.70ba/s][1,11]<stderr>:#015 44%|████▍     | 7/16 [00:02<00:03,  2.59ba/s][1,9]<stderr>:#015 56%|█████▋    | 9/16 [00:03<00:02,  2.90ba/s][1,4]<stderr>:#015  6%|▋         | 1/16 [00:00<00:09,  1.57ba/s][1,5]<stderr>:#015  6%|▋         | 1/16 [00:00<00:09,  1.54ba/s][1,10]<stderr>:#015 44%|████▍     | 7/16 [00:02<00:03,  2.70ba/s][1,3]<stderr>:#015  6%|▋         | 1/16 [00:00<00:09,  1.51ba/s][1,14]<stderr>:#015 44%|████▍     | 7/16 [00:02<00:03,  2.64ba/s][1,7]<stderr>:#015  6%|▋         | 1/16 [00:00<00:11,  1.35ba/s][1,0]<stderr>:#015  6%|▋         | 1/16 [00:00<00:10,  1.42ba/s][1,6]<stderr>:#015 12%|█▎        | 2/16 [00:00<00:06,  2.12ba/s][1,2]<stderr>:#015 12%|█▎        | 2/16 [00:00<00:06,  2.23ba/s][1,8]<stderr>:#015 50%|█████     | 8/16 [00:03<00:02,  2.72ba/s][1,11]<stderr>:#015 50%|█████     | 8/16 [00:03<00:02,  2.82ba/s][1,10]<stderr>:#015 50%|█████     | 8/16 [00:03<00:02,  3.01ba/s][1,15]<stderr>:#015 50%|█████     | 8/16 [00:03<00:02,  2.87ba/s][1,12]<stderr>:#015 50%|█████     | 8/16 [00:03<00:02,  2.71ba/s][1,13]<stderr>:#015 50%|█████     | 8/16 [00:03<00:02,  2.80ba/s][1,9]<stderr>:#015 62%|██████▎   | 10/16 [00:03<00:02,  2.93ba/s][1,1]<stderr>:#015  6%|▋         | 1/16 [00:00<00:13,  1.09ba/s][1,14]<stderr>:#015 50%|█████     | 8/16 [00:03<00:02,  2.74ba/s][1,2]<stderr>:#015 19%|█▉        | 3/16 [00:01<00:05,  2.59ba/s][1,6]<stderr>:#015 19%|█▉        | 3/16 [00:01<00:05,  2.43ba/s][1,4]<stderr>:#015 12%|█▎        | 2/16 [00:01<00:08,  1.65ba/s][1,3]<stderr>:#015 12%|█▎        | 2/16 [00:01<00:08,  1.62ba/s][1,10]<stderr>:#015 56%|█████▋    | 9/16 [00:03<00:02,  3.09ba/s][1,13]<stderr>:#015 56%|█████▋    | 9/16 [00:03<00:02,  2.95ba/s][1,15]<stderr>:#015 56%|█████▋    | 9/16 [00:03<00:02,  2.97ba/s][1,11]<stderr>:#015 56%|█████▋    | 9/16 [00:03<00:02,  2.92ba/s][1,12]<stderr>:#015 56%|█████▋    | 9/16 [00:03<00:02,  2.86ba/s][1,8]<stderr>:#015 56%|█████▋    | 9/16 [00:03<00:02,  2.80ba/s][1,5]<stderr>:#015 12%|█▎        | 2/16 [00:01<00:08,  1.59ba/s][1,7]<stderr>:#015 12%|█▎        | 2/16 [00:01<00:09,  1.46ba/s][1,14]<stderr>:#015 56%|█████▋    | 9/16 [00:03<00:02,  2.72ba/s][1,0]<stderr>:#015 12%|█▎        | 2/16 [00:01<00:09,  1.45ba/s][1,2]<stderr>:#015 25%|██▌       | 4/16 [00:01<00:04,  2.70ba/s][1,6]<stderr>:#015 25%|██▌       | 4/16 [00:01<00:04,  2.59ba/s][1,1]<stderr>:#015 12%|█▎        | 2/16 [00:01<00:11,  1.25ba/s][1,10]<stderr>:#015 62%|██████▎   | 10/16 [00:03<00:01,  3.19ba/s][1,3]<stderr>:#015 19%|█▉        | 3/16 [00:01<00:06,  1.88ba/s][1,11]<stderr>:#015 62%|██████▎   | 10/16 [00:03<00:01,  3.05ba/s][1,9]<stderr>:#015 69%|██████▉   | 11/16 [00:03<00:02,  2.47ba/s][1,4]<stderr>:#015 19%|█▉        | 3/16 [00:01<00:06,  1.87ba/s][1,15]<stderr>:#015 62%|██████▎   | 10/16 [00:03<00:01,  3.01ba/s][1,5]<stderr>:#015 19%|█▉        | 3/16 [00:01<00:06,  1.86ba/s][1,14]<stderr>:#015 62%|██████▎   | 10/16 [00:03<00:01,  3.17ba/s][1,13]<stderr>:#015 62%|██████▎   | 10/16 [00:03<00:02,  2.72ba/s][1,8]<stderr>:#015 62%|██████▎   | 10/16 [00:03<00:02,  2.63ba/s][1,12]<stderr>:#015 62%|██████▎   | 10/16 [00:03<00:02,  2.58ba/s][1,7]<stderr>:#015 19%|█▉        | 3/16 [00:01<00:07,  1.64ba/s][1,2]<stderr>:#015 31%|███▏      | 5/16 [00:01<00:03,  2.81ba/s][1,3]<stderr>:#015 25%|██▌       | 4/16 [00:01<00:05,  2.22ba/s][1,5]<stderr>:#015 25%|██▌       | 4/16 [00:01<00:05,  2.27ba/s][1,4]<stderr>:#015 25%|██▌       | 4/16 [00:01<00:05,  2.24ba/s][1,9]<stderr>:#015 75%|███████▌  | 12/16 [00:04<00:01,  2.68ba/s][1,6]<stderr>:#015 31%|███▏      | 5/16 [00:01<00:04,  2.51ba/s][1,0]<stderr>:#015 19%|█▉        | 3/16 [00:01<00:08,  1.58ba/s][1,1]<stderr>:#015 19%|█▉        | 3/16 [00:01<00:08,  1.45ba/s][1,10]<stderr>:#015 69%|██████▉   | 11/16 [00:04<00:01,  2.85ba/s][1,12]<stderr>:#015 69%|██████▉   | 11/16 [00:04<00:01,  2.90ba/s][1,9]<stderr>:#015 81%|████████▏ | 13/16 [00:04<00:00,  3.18ba/s][1,15]<stderr>:#015 69%|██████▉   | 11/16 [00:04<00:01,  2.70ba/s][1,11]<stderr>:#015 69%|██████▉   | 11/16 [00:04<00:01,  2.63ba/s][1,2]<stderr>:#015 38%|███▊      | 6/16 [00:02<00:03,  2.86ba/s][1,14]<stderr>:#015 69%|██████▉   | 11/16 [00:04<00:01,  2.70ba/s][1,7]<stderr>:#015 25%|██▌       | 4/16 [00:02<00:06,  1.87ba/s][1,4]<stderr>:#015 31%|███▏      | 5/16 [00:02<00:04,  2.42ba/s][1,5]<stderr>:#015 31%|███▏      | 5/16 [00:02<00:04,  2.42ba/s][1,8]<stderr>:#015 69%|██████▉   | 11/16 [00:04<00:02,  2.45ba/s][1,1]<stderr>:#015 25%|██▌       | 4/16 [00:02<00:06,  1.83ba/s][1,6]<stderr>:#015 38%|███▊      | 6/16 [00:02<00:03,  2.76ba/s][1,3]<stderr>:#015 31%|███▏      | 5/16 [00:02<00:04,  2.28ba/s][1,13]<stderr>:#015 69%|██████▉   | 11/16 [00:04<00:02,  2.29ba/s][1,12]<stderr>:#015 75%|███████▌  | 12/16 [00:04<00:01,  2.93ba/s][1,9]<stderr>:#015 88%|████████▊ | 14/16 [00:04<00:00,  3.26ba/s][1,10]<stderr>:#015 75%|███████▌  | 12/16 [00:04<00:01,  2.84ba/s][1,15]<stderr>:#015 75%|███████▌  | 12/16 [00:04<00:01,  2.88ba/s][1,0]<stderr>:#015 25%|██▌       | 4/16 [00:02<00:06,  1.77ba/s][1,11]<stderr>:#015 75%|███████▌  | 12/16 [00:04<00:01,  2.75ba/s][1,6]<stderr>:#015 44%|████▍     | 7/16 [00:02<00:03,  2.97ba/s][1,8]<stderr>:#015 75%|███████▌  | 12/16 [00:04<00:01,  2.67ba/s][1,14]<stderr>:#015 75%|███████▌  | 12/16 [00:04<00:01,  2.74ba/s][1,5]<stderr>:#015 38%|███▊      | 6/16 [00:02<00:03,  2.63ba/s][1,4]<stderr>:#015 38%|███▊      | 6/16 [00:02<00:03,  2.60ba/s][1,2]<stderr>:#015 44%|████▍     | 7/16 [00:02<00:03,  2.81ba/s][1,7]<stderr>:#015 31%|███▏      | 5/16 [00:02<00:05,  2.08ba/s][1,3]<stderr>:#015 38%|███▊      | 6/16 [00:02<00:04,  2.44ba/s][1,1]<stderr>:#015 31%|███▏      | 5/16 [00:02<00:05,  1.94ba/s][1,13]<stderr>:#015 75%|███████▌  | 12/16 [00:04<00:01,  2.45ba/s][1,10]<stderr>:#015 81%|████████▏ | 13/16 [00:04<00:01,  2.97ba/s][1,9]<stderr>:#015 94%|█████████▍| 15/16 [00:04<00:00,  3.20ba/s][1,15]<stderr>:#015 81%|████████▏ | 13/16 [00:04<00:01,  2.93ba/s][1,14]<stderr>:#015 81%|████████▏ | 13/16 [00:04<00:00,  3.13ba/s][1,0]<stderr>:#015 31%|███▏      | 5/16 [00:02<00:05,  1.96ba/s][1,11]<stderr>:#015 81%|████████▏ | 13/16 [00:04<00:01,  2.78ba/s][1,12]<stderr>:#015 81%|████████▏ | 13/16 [00:04<00:01,  2.71ba/s][1,5]<stderr>:#015 44%|████▍     | 7/16 [00:02<00:03,  2.81ba/s][1,8]<stderr>:#015 81%|████████▏ | 13/16 [00:05<00:01,  2.80ba/s][1,2]<stderr>:#015 50%|█████     | 8/16 [00:02<00:02,  2.89ba/s][1,7]<stderr>:#015 38%|███▊      | 6/16 [00:02<00:04,  2.30ba/s][1,4]<stderr>:#015 44%|████▍     | 7/16 [00:02<00:03,  2.63ba/s][1,6]<stderr>:#015 50%|█████     | 8/16 [00:02<00:02,  2.75ba/s][1,3]<stderr>:#015 44%|████▍     | 7/16 [00:02<00:03,  2.51ba/s][1,1]<stderr>:#015 38%|███▊      | 6/16 [00:02<00:04,  2.18ba/s][1,13]<stderr>:#015 81%|████████▏ | 13/16 [00:05<00:01,  2.61ba/s][1,10]<stderr>:#015 88%|████████▊ | 14/16 [00:05<00:00,  2.96ba/s][1,14]<stderr>:#015 88%|████████▊ | 14/16 [00:05<00:00,  3.12ba/s][1,15]<stderr>:#015 88%|████████▊ | 14/16 [00:05<00:00,  2.92ba/s][1,8]<stderr>:#015 88%|████████▊ | 14/16 [00:05<00:00,  3.12ba/s][1,11]<stderr>:#015 88%|████████▊ | 14/16 [00:05<00:00,  2.94ba/s][1,0]<stderr>:#015 38%|███▊      | 6/16 [00:03<00:04,  2.11ba/s][1,9]<stderr>:#015100%|██████████| 16/16 [00:05<00:00,  2.76ba/s][1,5]<stderr>:#015 50%|█████     | 8/16 [00:03<00:02,  2.80ba/s][1,9]<stderr>:#015100%|██████████| 16/16 [00:05<00:00,  2.92ba/s][1,9]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:#015 56%|█████▋    | 9/16 [00:03<00:02,  2.94ba/s][1,6]<stderr>:#015 56%|█████▋    | 9/16 [00:03<00:02,  3.01ba/s][1,4]<stderr>:#015 50%|█████     | 8/16 [00:03<00:02,  2.81ba/s][1,12]<stderr>:#015 88%|████████▊ | 14/16 [00:05<00:00,  2.64ba/s][1,1]<stderr>:#015 44%|████▍     | 7/16 [00:03<00:03,  2.62ba/s][1,7]<stderr>:#015 44%|████▍     | 7/16 [00:03<00:03,  2.44ba/s][1,3]<stderr>:#015 50%|█████     | 8/16 [00:03<00:02,  2.81ba/s][1,9]<stderr>:#015  0%|          | 0/2 [00:00<?, ?ba/s][1,15]<stderr>:#015 94%|█████████▍| 15/16 [00:05<00:00,  3.12ba/s][1,14]<stderr>:#015 94%|█████████▍| 15/16 [00:05<00:00,  3.23ba/s][1,10]<stderr>:#015 94%|█████████▍| 15/16 [00:05<00:00,  3.00ba/s][1,8]<stderr>:#015 94%|█████████▍| 15/16 [00:05<00:00,  3.09ba/s][1,11]<stderr>:#015 94%|█████████▍| 15/16 [00:05<00:00,  2.86ba/s][1,13]<stderr>:#015 88%|████████▊ | 14/16 [00:05<00:00,  2.48ba/s][1,4]<stderr>:#015 56%|█████▋    | 9/16 [00:03<00:02,  3.01ba/s][1,0]<stderr>:#015 44%|████▍     | 7/16 [00:03<00:03,  2.35ba/s][1,2]<stderr>:#015 62%|██████▎   | 10/16 [00:03<00:01,  3.04ba/s][1,3]<stderr>:#015 56%|█████▋    | 9/16 [00:03<00:02,  3.05ba/s][1,5]<stderr>:#015 56%|█████▋    | 9/16 [00:03<00:02,  2.86ba/s][1,7]<stderr>:#015 50%|█████     | 8/16 [00:03<00:03,  2.63ba/s][1,12]<stderr>:#015 94%|█████████▍| 15/16 [00:05<00:00,  2.74ba/s][1,1]<stderr>:#015 50%|█████     | 8/16 [00:03<00:02,  2.68ba/s][1,6]<stderr>:#015 62%|██████▎   | 10/16 [00:03<00:02,  2.85ba/s][1,10]<stderr>:#015100%|██████████| 16/16 [00:05<00:00,  3.04ba/s][1,10]<stderr>:#015100%|██████████| 16/16 [00:05<00:00,  2.75ba/s][1,10]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:#015 50%|█████     | 1/2 [00:00<00:00,  2.85ba/s][1,15]<stderr>:#015100%|██████████| 16/16 [00:05<00:00,  3.03ba/s][1,15]<stderr>:#015100%|██████████| 16/16 [00:05<00:00,  2.73ba/s]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:#015100%|██████████| 16/16 [00:05<00:00,  3.07ba/s][1,14]<stderr>:#015100%|██████████| 16/16 [00:05<00:00,  2.73ba/s][1,14]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:#015100%|██████████| 16/16 [00:05<00:00,  3.07ba/s][1,8]<stderr>:#015100%|██████████| 16/16 [00:05<00:00,  3.08ba/s][1,8]<stderr>:#015100%|██████████| 16/16 [00:05<00:00,  2.71ba/s][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:#015100%|██████████| 16/16 [00:05<00:00,  2.73ba/s][1,11]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:#015 94%|█████████▍| 15/16 [00:05<00:00,  2.70ba/s][1,10]<stderr>:#015  0%|          | 0/2 [00:00<?, ?ba/s][1,12]<stderr>:#015100%|██████████| 16/16 [00:05<00:00,  3.10ba/s][1,15]<stderr>:#015  0%|          | 0/2 [00:00<?, ?ba/s][1,12]<stderr>:#015100%|██████████| 16/16 [00:05<00:00,  2.71ba/s][1,12]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 50%|█████     | 8/16 [00:03<00:03,  2.54ba/s][1,3]<stderr>:#015 62%|██████▎   | 10/16 [00:03<00:01,  3.15ba/s][1,7]<stderr>:#015 56%|█████▋    | 9/16 [00:03<00:02,  2.84ba/s][1,4]<stderr>:#015 62%|██████▎   | 10/16 [00:03<00:02,  2.98ba/s][1,14]<stderr>:#015  0%|          | 0/2 [00:00<?, ?ba/s][1,1]<stderr>:#015 56%|█████▋    | 9/16 [00:03<00:02,  2.86ba/s][1,11]<stderr>:#015  0%|          | 0/2 [00:00<?, ?ba/s][1,5]<stderr>:#015 62%|██████▎   | 10/16 [00:03<00:02,  2.70ba/s][1,12]<stderr>:#015  0%|          | 0/2 [00:00<?, ?ba/s][1,8]<stderr>:#015  0%|          | 0/2 [00:00<?, ?ba/s][1,9]<stderr>:#015100%|██████████| 2/2 [00:00<00:00,  2.89ba/s][1,9]<stderr>:#015100%|██████████| 2/2 [00:00<00:00,  2.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:#015100%|██████████| 16/16 [00:06<00:00,  2.78ba/s][1,13]<stderr>:#015100%|██████████| 16/16 [00:06<00:00,  2.58ba/s][1,13]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:#015 69%|██████▉   | 11/16 [00:04<00:02,  2.42ba/s][1,0]<stderr>:#015 56%|█████▋    | 9/16 [00:03<00:02,  2.72ba/s][1,10]<stderr>:#015 50%|█████     | 1/2 [00:00<00:00,  2.82ba/s][1,6]<stderr>:#015 69%|██████▉   | 11/16 [00:04<00:02,  2.47ba/s][1,14]<stderr>:#015 50%|█████     | 1/2 [00:00<00:00,  3.29ba/s][1,11]<stderr>:#015 50%|█████     | 1/2 [00:00<00:00,  3.56ba/s][1,1]<stderr>:#015 62%|██████▎   | 10/16 [00:04<00:01,  3.00ba/s][1,15]<stderr>:#015 50%|█████     | 1/2 [00:00<00:00,  2.27ba/s][1,9]<stderr>:#015Downloading:   0%|          | 0.00/2.17k [00:00<?, ?B/s][1,9]<stderr>:#015Downloading: 5.61kB [00:00, 1.46MB/s]                   [1,9]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:#015 69%|██████▉   | 11/16 [00:04<00:01,  2.77ba/s][1,12]<stderr>:#015 50%|█████     | 1/2 [00:00<00:00,  3.31ba/s][1,13]<stderr>:#015  0%|          | 0/2 [00:00<?, ?ba/s][1,8]<stderr>:#015 50%|█████     | 1/2 [00:00<00:00,  3.03ba/s][1,0]<stderr>:#015 62%|██████▎   | 10/16 [00:04<00:01,  3.00ba/s][1,3]<stderr>:#015 69%|██████▉   | 11/16 [00:04<00:01,  2.56ba/s][1,6]<stderr>:#015 75%|███████▌  | 12/16 [00:04<00:01,  2.78ba/s][1,2]<stderr>:#015 75%|███████▌  | 12/16 [00:04<00:01,  2.62ba/s][1,5]<stderr>:#015 69%|██████▉   | 11/16 [00:04<00:02,  2.48ba/s][1,7]<stderr>:#015 62%|██████▎   | 10/16 [00:04<00:02,  2.32ba/s][1,10]<stderr>:#015100%|██████████| 2/2 [00:00<00:00,  2.82ba/s][1,10]<stderr>:#015100%|██████████| 2/2 [00:00<00:00,  2.81ba/s]\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:#015100%|██████████| 2/2 [00:00<00:00,  3.16ba/s][1,14]<stderr>:#015100%|██████████| 2/2 [00:00<00:00,  3.08ba/s]\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:#015100%|██████████| 2/2 [00:00<00:00,  3.31ba/s][1,11]<stderr>:#015100%|██████████| 2/2 [00:00<00:00,  3.16ba/s]\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:#015100%|██████████| 2/2 [00:00<00:00,  2.52ba/s][1,15]<stderr>:#015100%|██████████| 2/2 [00:00<00:00,  2.71ba/s]\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015100%|██████████| 2/2 [00:00<00:00,  3.50ba/s][1,8]<stderr>:#015100%|██████████| 2/2 [00:00<00:00,  3.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:#015 50%|█████     | 1/2 [00:00<00:00,  3.93ba/s][1,4]<stderr>:#015 75%|███████▌  | 12/16 [00:04<00:01,  2.89ba/s][1,1]<stderr>:#015 69%|██████▉   | 11/16 [00:04<00:01,  2.69ba/s][1,12]<stderr>:#015100%|██████████| 2/2 [00:00<00:00,  3.11ba/s][1,12]<stderr>:#015100%|██████████| 2/2 [00:00<00:00,  3.00ba/s]\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:#015100%|██████████| 2/2 [00:00<00:00,  4.49ba/s][1,13]<stderr>:#015100%|██████████| 2/2 [00:00<00:00,  4.96ba/s]\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:#015 69%|██████▉   | 11/16 [00:04<00:01,  2.63ba/s][1,2]<stderr>:#015 81%|████████▏ | 13/16 [00:04<00:01,  2.80ba/s][1,3]<stderr>:#015 75%|███████▌  | 12/16 [00:04<00:01,  2.65ba/s][1,6]<stderr>:#015 81%|████████▏ | 13/16 [00:04<00:01,  2.84ba/s][1,5]<stderr>:#015 75%|███████▌  | 12/16 [00:04<00:01,  2.67ba/s][1,4]<stderr>:#015 81%|████████▏ | 13/16 [00:04<00:00,  3.01ba/s][1,1]<stderr>:#015 75%|███████▌  | 12/16 [00:04<00:01,  2.71ba/s][1,0]<stderr>:#015 69%|██████▉   | 11/16 [00:04<00:02,  2.38ba/s][1,7]<stderr>:#015 75%|███████▌  | 12/16 [00:04<00:01,  2.80ba/s][1,3]<stderr>:#015 81%|████████▏ | 13/16 [00:04<00:01,  2.79ba/s][1,6]<stderr>:#015 88%|████████▊ | 14/16 [00:04<00:00,  2.94ba/s][1,2]<stderr>:#015 88%|████████▊ | 14/16 [00:05<00:00,  2.86ba/s][1,5]<stderr>:#015 81%|████████▏ | 13/16 [00:05<00:01,  2.46ba/s][1,4]<stderr>:#015 88%|████████▊ | 14/16 [00:05<00:00,  2.88ba/s][1,7]<stderr>:#015 81%|████████▏ | 13/16 [00:05<00:01,  2.90ba/s][1,2]<stderr>:#015 94%|█████████▍| 15/16 [00:05<00:00,  3.05ba/s][1,0]<stderr>:#015 75%|███████▌  | 12/16 [00:05<00:01,  2.53ba/s][1,1]<stderr>:#015 81%|████████▏ | 13/16 [00:05<00:01,  2.74ba/s][1,6]<stderr>:#015 94%|█████████▍| 15/16 [00:05<00:00,  2.93ba/s][1,3]<stderr>:#015 88%|████████▊ | 14/16 [00:05<00:00,  2.78ba/s][1,5]<stderr>:#015 88%|████████▊ | 14/16 [00:05<00:00,  2.52ba/s][1,7]<stderr>:#015 88%|████████▊ | 14/16 [00:05<00:00,  2.99ba/s][1,1]<stderr>:#015 88%|████████▊ | 14/16 [00:05<00:00,  2.89ba/s][1,0]<stderr>:#015 81%|████████▏ | 13/16 [00:05<00:01,  2.69ba/s][1,4]<stderr>:#015 94%|█████████▍| 15/16 [00:05<00:00,  2.73ba/s][1,6]<stderr>:#015100%|██████████| 16/16 [00:05<00:00,  2.97ba/s][1,6]<stderr>:#015100%|██████████| 16/16 [00:05<00:00,  2.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:#015100%|██████████| 16/16 [00:05<00:00,  2.90ba/s][1,2]<stderr>:#015100%|██████████| 16/16 [00:05<00:00,  2.82ba/s][1,2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:#015 94%|█████████▍| 15/16 [00:05<00:00,  2.80ba/s][1,2]<stderr>:#015  0%|          | 0/2 [00:00<?, ?ba/s][1,6]<stderr>:#015  0%|          | 0/2 [00:00<?, ?ba/s][1,4]<stderr>:#015100%|██████████| 16/16 [00:05<00:00,  2.82ba/s][1,4]<stderr>:#015100%|██████████| 16/16 [00:05<00:00,  2.72ba/s][1,4]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 88%|████████▊ | 14/16 [00:05<00:00,  2.76ba/s][1,5]<stderr>:#015 94%|█████████▍| 15/16 [00:05<00:00,  2.47ba/s][1,1]<stderr>:#015 94%|█████████▍| 15/16 [00:05<00:00,  2.79ba/s][1,7]<stderr>:#015 94%|█████████▍| 15/16 [00:05<00:00,  2.81ba/s][1,3]<stderr>:#015100%|██████████| 16/16 [00:05<00:00,  2.88ba/s][1,3]<stderr>:#015100%|██████████| 16/16 [00:05<00:00,  2.68ba/s][1,3]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:#015 50%|█████     | 1/2 [00:00<00:00,  3.82ba/s][1,4]<stderr>:#015  0%|          | 0/2 [00:00<?, ?ba/s][1,3]<stderr>:#015  0%|          | 0/2 [00:00<?, ?ba/s][1,6]<stderr>:#015 50%|█████     | 1/2 [00:00<00:00,  2.58ba/s][1,0]<stderr>:#015 94%|█████████▍| 15/16 [00:06<00:00,  2.76ba/s][1,1]<stderr>:#015100%|██████████| 16/16 [00:06<00:00,  2.87ba/s][1,1]<stderr>:#015100%|██████████| 16/16 [00:06<00:00,  2.58ba/s][1,1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:#015100%|██████████| 2/2 [00:00<00:00,  3.85ba/s][1,2]<stderr>:#015100%|██████████| 2/2 [00:00<00:00,  3.86ba/s][1,2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:#015100%|██████████| 16/16 [00:06<00:00,  2.89ba/s][1,7]<stderr>:#015100%|██████████| 16/16 [00:06<00:00,  2.55ba/s][1,7]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:#015 50%|█████     | 1/2 [00:00<00:00,  3.20ba/s][1,5]<stderr>:#015100%|██████████| 16/16 [00:06<00:00,  2.42ba/s][1,5]<stderr>:#015100%|██████████| 16/16 [00:06<00:00,  2.52ba/s]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:#015  0%|          | 0/2 [00:00<?, ?ba/s][1,7]<stderr>:#015  0%|          | 0/2 [00:00<?, ?ba/s][1,3]<stderr>:#015 50%|█████     | 1/2 [00:00<00:00,  3.21ba/s][1,6]<stderr>:#015100%|██████████| 2/2 [00:00<00:00,  2.83ba/s][1,6]<stderr>:#015100%|██████████| 2/2 [00:00<00:00,  3.02ba/s]\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:#015  0%|          | 0/2 [00:00<?, ?ba/s][1,0]<stderr>:#015100%|██████████| 16/16 [00:06<00:00,  2.88ba/s][1,0]<stderr>:#015100%|██████████| 16/16 [00:06<00:00,  2.45ba/s][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:#015Downloading:   0%|          | 0.00/2.17k [00:00<?, ?B/s][1,2]<stderr>:#015Downloading: 5.61kB [00:00, 733kB/s]                    \u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  0%|          | 0/2 [00:00<?, ?ba/s][1,4]<stderr>:#015100%|██████████| 2/2 [00:00<00:00,  3.14ba/s][1,4]<stderr>:#015100%|██████████| 2/2 [00:00<00:00,  3.11ba/s]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:#015100%|██████████| 2/2 [00:00<00:00,  3.42ba/s][1,3]<stderr>:#015100%|██████████| 2/2 [00:00<00:00,  3.58ba/s]\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:#015 50%|█████     | 1/2 [00:00<00:00,  3.09ba/s][1,5]<stderr>:#015 50%|█████     | 1/2 [00:00<00:00,  4.12ba/s][1,1]<stderr>:#015 50%|█████     | 1/2 [00:00<00:00,  3.02ba/s][1,0]<stderr>:#015 50%|█████     | 1/2 [00:00<00:00,  3.48ba/s][1,1]<stderr>:#015100%|██████████| 2/2 [00:00<00:00,  3.43ba/s][1,1]<stderr>:#015100%|██████████| 2/2 [00:00<00:00,  3.74ba/s][1,1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:#015100%|██████████| 2/2 [00:00<00:00,  4.05ba/s][1,5]<stderr>:#015100%|██████████| 2/2 [00:00<00:00,  3.99ba/s][1,5]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:#015100%|██████████| 2/2 [00:00<00:00,  3.02ba/s][1,7]<stderr>:#015100%|██████████| 2/2 [00:00<00:00,  2.98ba/s]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015100%|██████████| 2/2 [00:00<00:00,  3.96ba/s][1,0]<stderr>:#015100%|██████████| 2/2 [00:00<00:00,  4.35ba/s]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:Using amp fp16 backend\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:Using amp fp16 backend\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:***** Running training *****\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  Num examples = 16000\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  Num Epochs = 3\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  Instantaneous batch size per device = 4\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  Total train batch size (w. parallel, distributed & accumulation) = 64\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  Total optimization steps = 750\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  0%|          | 0/750 [00:00<?, ?it/s][1,8]<stderr>:***** Running training *****\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  Num examples = 16000\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  Num Epochs = 3\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  Instantaneous batch size per device = 4\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  Total train batch size (w. parallel, distributed & accumulation) = 64\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  Total optimization steps = 750\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015  0%|          | 0/750 [00:00<?, ?it/s][1,0]<stderr>:#015  0%|          | 1/750 [00:03<48:00,  3.85s/it][1,8]<stderr>:#015  0%|          | 1/750 [00:03<47:54,  3.84s/it][1,0]<stderr>:#015  0%|          | 2/750 [00:05<39:58,  3.21s/it][1,8]<stderr>:#015  0%|          | 2/750 [00:05<40:26,  3.24s/it][1,8]<stderr>:#015  0%|          | 3/750 [00:06<31:38,  2.54s/it][1,0]<stderr>:#015  0%|          | 3/750 [00:06<32:14,  2.59s/it][1,8]<stderr>:#015  1%|          | 4/750 [00:07<27:04,  2.18s/it][1,0]<stderr>:#015  1%|          | 4/750 [00:07<27:20,  2.20s/it][1,8]<stderr>:#015  1%|          | 5/750 [00:09<24:12,  1.95s/it][1,0]<stderr>:#015  1%|          | 5/750 [00:09<24:25,  1.97s/it][1,8]<stderr>:#015  1%|          | 6/750 [00:10<22:11,  1.79s/it][1,0]<stderr>:#015  1%|          | 6/750 [00:11<23:07,  1.87s/it][1,8]<stderr>:#015  1%|          | 7/750 [00:12<22:28,  1.82s/it][1,0]<stderr>:#015  1%|          | 7/750 [00:12<22:19,  1.80s/it][1,8]<stderr>:#015  1%|          | 8/750 [00:13<20:25,  1.65s/it][1,0]<stderr>:#015  1%|          | 8/750 [00:14<20:35,  1.67s/it][1,8]<stderr>:#015  1%|          | 9/750 [00:15<20:09,  1.63s/it][1,0]<stderr>:#015  1%|          | 9/750 [00:15<20:03,  1.62s/it][1,8]<stderr>:#015  1%|▏         | 10/750 [00:17<20:15,  1.64s/it][1,0]<stderr>:#015  1%|▏         | 10/750 [00:17<20:06,  1.63s/it][1,8]<stderr>:#015  1%|▏         | 11/750 [00:18<19:48,  1.61s/it][1,0]<stderr>:#015  1%|▏         | 11/750 [00:18<20:01,  1.63s/it][1,0]<stderr>:#015  2%|▏         | 12/750 [00:20<18:48,  1.53s/it][1,8]<stderr>:#015  2%|▏         | 12/750 [00:20<19:09,  1.56s/it][1,8]<stderr>:#015  2%|▏         | 13/750 [00:21<17:52,  1.46s/it][1,0]<stderr>:#015  2%|▏         | 13/750 [00:21<17:42,  1.44s/it][1,0]<stderr>:#015  2%|▏         | 14/750 [00:22<17:49,  1.45s/it][1,8]<stderr>:#015  2%|▏         | 14/750 [00:22<18:12,  1.48s/it][1,0]<stderr>:#015  2%|▏         | 15/750 [00:24<18:58,  1.55s/it][1,8]<stderr>:#015  2%|▏         | 15/750 [00:24<19:00,  1.55s/it][1,8]<stderr>:#015  2%|▏         | 16/750 [00:26<18:38,  1.52s/it][1,0]<stderr>:#015  2%|▏         | 16/750 [00:26<19:18,  1.58s/it][1,8]<stderr>:#015  2%|▏         | 17/750 [00:27<19:34,  1.60s/it][1,0]<stderr>:#015  2%|▏         | 17/750 [00:27<19:26,  1.59s/it][1,8]<stderr>:#015  2%|▏         | 18/750 [00:29<19:08,  1.57s/it][1,0]<stderr>:#015  2%|▏         | 18/750 [00:29<19:13,  1.58s/it][1,0]<stderr>:#015  3%|▎         | 19/750 [00:30<18:56,  1.55s/it][1,8]<stderr>:#015  3%|▎         | 19/750 [00:30<19:21,  1.59s/it][1,8]<stderr>:#015  3%|▎         | 20/750 [00:32<18:15,  1.50s/it][1,0]<stderr>:#015  3%|▎         | 20/750 [00:32<18:14,  1.50s/it][1,0]<stderr>:#015  3%|▎         | 21/750 [00:33<18:35,  1.53s/it][1,8]<stderr>:#015  3%|▎         | 21/750 [00:33<18:59,  1.56s/it][1,0]<stderr>:#015  3%|▎         | 22/750 [00:35<18:15,  1.50s/it][1,8]<stderr>:#015  3%|▎         | 22/750 [00:35<18:36,  1.53s/it][1,8]<stderr>:#015  3%|▎         | 23/750 [00:36<18:00,  1.49s/it][1,0]<stderr>:#015  3%|▎         | 23/750 [00:36<18:21,  1.52s/it][1,8]<stderr>:#015  3%|▎         | 24/750 [00:38<17:57,  1.48s/it][1,0]<stderr>:#015  3%|▎         | 24/750 [00:38<18:10,  1.50s/it][1,8]<stderr>:#015  3%|▎         | 25/750 [00:39<17:19,  1.43s/it][1,0]<stderr>:#015  3%|▎         | 25/750 [00:39<17:22,  1.44s/it][1,0]<stderr>:#015  3%|▎         | 26/750 [00:41<17:12,  1.43s/it][1,8]<stderr>:#015  3%|▎         | 26/750 [00:41<17:37,  1.46s/it][1,0]<stderr>:#015  4%|▎         | 27/750 [00:42<18:03,  1.50s/it][1,8]<stderr>:#015  4%|▎         | 27/750 [00:42<18:28,  1.53s/it][1,8]<stderr>:#015  4%|▎         | 28/750 [00:44<17:56,  1.49s/it][1,0]<stderr>:#015  4%|▎         | 28/750 [00:44<18:28,  1.54s/it][1,8]<stderr>:#015  4%|▍         | 29/750 [00:45<18:33,  1.54s/it][1,0]<stderr>:#015  4%|▍         | 29/750 [00:45<18:49,  1.57s/it][1,8]<stderr>:#015  4%|▍         | 30/750 [00:47<19:17,  1.61s/it][1,0]<stderr>:#015  4%|▍         | 30/750 [00:47<19:27,  1.62s/it][1,0]<stderr>:#015  4%|▍         | 31/750 [00:49<18:47,  1.57s/it][1,8]<stderr>:#015  4%|▍         | 31/750 [00:49<18:59,  1.58s/it][1,0]<stderr>:#015  4%|▍         | 32/750 [00:50<18:13,  1.52s/it][1,8]<stderr>:#015  4%|▍         | 32/750 [00:50<18:17,  1.53s/it][1,0]<stderr>:#015  4%|▍         | 33/750 [00:52<17:45,  1.49s/it][1,8]<stderr>:#015  4%|▍         | 33/750 [00:52<17:58,  1.50s/it][1,8]<stderr>:#015  5%|▍         | 34/750 [00:53<17:09,  1.44s/it][1,0]<stderr>:#015  5%|▍         | 34/750 [00:53<17:17,  1.45s/it][1,0]<stderr>:#015  5%|▍         | 35/750 [00:54<17:28,  1.47s/it][1,8]<stderr>:#015  5%|▍         | 35/750 [00:54<17:37,  1.48s/it][1,0]<stderr>:#015  5%|▍         | 36/750 [00:56<17:03,  1.43s/it][1,8]<stderr>:#015  5%|▍         | 36/750 [00:56<17:13,  1.45s/it][1,8]<stderr>:#015  5%|▍         | 37/750 [00:57<17:23,  1.46s/it][1,0]<stderr>:#015  5%|▍         | 37/750 [00:57<18:03,  1.52s/it][1,8]<stderr>:#015  5%|▌         | 38/750 [00:59<19:05,  1.61s/it][1,0]<stderr>:#015  5%|▌         | 38/750 [00:59<19:07,  1.61s/it][1,8]<stderr>:#015  5%|▌         | 39/750 [01:00<17:36,  1.49s/it][1,0]<stderr>:#015  5%|▌         | 39/750 [01:00<17:34,  1.48s/it][1,0]<stderr>:#015  5%|▌         | 40/750 [01:02<16:05,  1.36s/it][1,8]<stderr>:#015  5%|▌         | 40/750 [01:02<16:14,  1.37s/it][1,0]<stderr>:#015  5%|▌         | 41/750 [01:03<17:32,  1.48s/it][1,8]<stderr>:#015  5%|▌         | 41/750 [01:03<18:03,  1.53s/it][1,0]<stderr>:#015  6%|▌         | 42/750 [01:05<17:49,  1.51s/it][1,8]<stderr>:#015  6%|▌         | 42/750 [01:05<17:50,  1.51s/it][1,8]<stderr>:#015  6%|▌         | 43/750 [01:06<18:09,  1.54s/it][1,0]<stderr>:#015  6%|▌         | 43/750 [01:07<19:02,  1.62s/it][1,8]<stderr>:#015  6%|▌         | 44/750 [01:08<18:01,  1.53s/it][1,0]<stderr>:#015  6%|▌         | 44/750 [01:08<18:09,  1.54s/it][1,8]<stderr>:#015  6%|▌         | 45/750 [01:09<17:44,  1.51s/it][1,0]<stderr>:#015  6%|▌         | 45/750 [01:10<18:10,  1.55s/it][1,8]<stderr>:#015  6%|▌         | 46/750 [01:11<18:02,  1.54s/it][1,0]<stderr>:#015  6%|▌         | 46/750 [01:11<17:50,  1.52s/it][1,8]<stderr>:#015  6%|▋         | 47/750 [01:13<18:10,  1.55s/it][1,0]<stderr>:#015  6%|▋         | 47/750 [01:13<18:59,  1.62s/it][1,0]<stderr>:#015  6%|▋         | 48/750 [01:14<17:26,  1.49s/it][1,8]<stderr>:#015  6%|▋         | 48/750 [01:14<18:00,  1.54s/it][1,0]<stderr>:#015  7%|▋         | 49/750 [01:16<17:13,  1.47s/it][1,8]<stderr>:#015  7%|▋         | 49/750 [01:16<17:49,  1.53s/it][1,0]<stderr>:#015  7%|▋         | 50/750 [01:17<16:54,  1.45s/it][1,8]<stderr>:#015  7%|▋         | 50/750 [01:17<17:36,  1.51s/it][1,0]<stderr>:#015  7%|▋         | 51/750 [01:19<17:36,  1.51s/it][1,8]<stderr>:#015  7%|▋         | 51/750 [01:19<18:01,  1.55s/it][1,0]<stderr>:#015  7%|▋         | 52/750 [01:20<17:32,  1.51s/it][1,8]<stderr>:#015  7%|▋         | 52/750 [01:20<17:52,  1.54s/it][1,8]<stderr>:#015  7%|▋         | 53/750 [01:22<17:22,  1.50s/it][1,0]<stderr>:#015  7%|▋         | 53/750 [01:22<17:44,  1.53s/it][1,8]<stderr>:#015  7%|▋         | 54/750 [01:23<17:16,  1.49s/it][1,0]<stderr>:#015  7%|▋         | 54/750 [01:23<17:38,  1.52s/it][1,0]<stderr>:#015  7%|▋         | 55/750 [01:25<17:22,  1.50s/it][1,8]<stderr>:#015  7%|▋         | 55/750 [01:25<17:33,  1.52s/it][1,8]<stderr>:#015  7%|▋         | 56/750 [01:26<17:11,  1.49s/it][1,0]<stderr>:#015  7%|▋         | 56/750 [01:26<17:37,  1.52s/it][1,8]<stderr>:#015  8%|▊         | 57/750 [01:28<16:54,  1.46s/it][1,0]<stderr>:#015  8%|▊         | 57/750 [01:28<17:11,  1.49s/it][1,0]<stderr>:#015  8%|▊         | 58/750 [01:29<17:23,  1.51s/it][1,8]<stderr>:#015  8%|▊         | 58/750 [01:29<17:29,  1.52s/it][1,0]<stderr>:#015  8%|▊         | 59/750 [01:31<16:55,  1.47s/it][1,8]<stderr>:#015  8%|▊         | 59/750 [01:31<17:23,  1.51s/it][1,0]<stderr>:#015  8%|▊         | 60/750 [01:32<17:07,  1.49s/it][1,8]<stderr>:#015  8%|▊         | 60/750 [01:32<17:33,  1.53s/it][1,0]<stderr>:#015  8%|▊         | 61/750 [01:33<16:37,  1.45s/it][1,8]<stderr>:#015  8%|▊         | 61/750 [01:33<16:26,  1.43s/it][1,8]<stderr>:#015  8%|▊         | 62/750 [01:35<17:59,  1.57s/it][1,0]<stderr>:#015  8%|▊         | 62/750 [01:35<18:12,  1.59s/it][1,8]<stderr>:#015  8%|▊         | 63/750 [01:37<18:41,  1.63s/it][1,0]<stderr>:#015  8%|▊         | 63/750 [01:37<19:29,  1.70s/it][1,8]<stderr>:#015  9%|▊         | 64/750 [01:39<18:13,  1.59s/it][1,0]<stderr>:#015  9%|▊         | 64/750 [01:39<18:27,  1.61s/it][1,8]<stderr>:#015  9%|▊         | 65/750 [01:40<17:00,  1.49s/it][1,0]<stderr>:#015  9%|▊         | 65/750 [01:40<16:51,  1.48s/it][1,0]<stderr>:#015  9%|▉         | 66/750 [01:41<16:17,  1.43s/it][1,8]<stderr>:#015  9%|▉         | 66/750 [01:41<16:41,  1.46s/it][1,8]<stderr>:#015  9%|▉         | 67/750 [01:43<15:49,  1.39s/it][1,0]<stderr>:#015  9%|▉         | 67/750 [01:43<16:08,  1.42s/it][1,0]<stderr>:#015  9%|▉         | 68/750 [01:44<15:30,  1.36s/it][1,8]<stderr>:#015  9%|▉         | 68/750 [01:44<16:14,  1.43s/it][1,0]<stderr>:#015  9%|▉         | 69/750 [01:45<15:01,  1.32s/it][1,8]<stderr>:#015  9%|▉         | 69/750 [01:45<14:59,  1.32s/it][1,0]<stderr>:#015  9%|▉         | 70/750 [01:47<15:35,  1.38s/it][1,8]<stderr>:#015  9%|▉         | 70/750 [01:47<15:50,  1.40s/it][1,0]<stderr>:#015  9%|▉         | 71/750 [01:48<14:45,  1.30s/it][1,8]<stderr>:#015  9%|▉         | 71/750 [01:48<14:47,  1.31s/it][1,8]<stderr>:#015 10%|▉         | 72/750 [01:50<17:01,  1.51s/it][1,0]<stderr>:#015 10%|▉         | 72/750 [01:50<17:18,  1.53s/it][1,8]<stderr>:#015 10%|▉         | 73/750 [01:52<18:19,  1.62s/it][1,0]<stderr>:#015 10%|▉         | 73/750 [01:52<18:35,  1.65s/it][1,8]<stderr>:#015 10%|▉         | 74/750 [01:54<19:15,  1.71s/it][1,0]<stderr>:#015 10%|▉         | 74/750 [01:54<19:28,  1.73s/it][1,0]<stderr>:#015 10%|█         | 75/750 [01:56<20:01,  1.78s/it][1,8]<stderr>:#015 10%|█         | 75/750 [01:56<20:17,  1.80s/it][1,8]<stderr>:#015 10%|█         | 76/750 [01:57<20:35,  1.83s/it][1,0]<stderr>:#015 10%|█         | 76/750 [01:58<20:37,  1.84s/it][1,0]<stderr>:#015 10%|█         | 77/750 [01:59<19:16,  1.72s/it][1,8]<stderr>:#015 10%|█         | 77/750 [01:59<19:17,  1.72s/it][1,8]<stderr>:#015 10%|█         | 78/750 [02:00<17:45,  1.59s/it][1,0]<stderr>:#015 10%|█         | 78/750 [02:00<17:54,  1.60s/it][1,0]<stderr>:#015 11%|█         | 79/750 [02:02<18:01,  1.61s/it][1,8]<stderr>:#015 11%|█         | 79/750 [02:02<18:34,  1.66s/it][1,8]<stderr>:#015 11%|█         | 80/750 [02:03<17:11,  1.54s/it][1,0]<stderr>:#015 11%|█         | 80/750 [02:03<17:24,  1.56s/it][1,8]<stderr>:#015 11%|█         | 81/750 [02:05<16:43,  1.50s/it][1,0]<stderr>:#015 11%|█         | 81/750 [02:05<16:52,  1.51s/it][1,8]<stderr>:#015 11%|█         | 82/750 [02:06<16:57,  1.52s/it][1,0]<stderr>:#015 11%|█         | 82/750 [02:06<17:00,  1.53s/it][1,8]<stderr>:#015 11%|█         | 83/750 [02:08<16:36,  1.49s/it][1,0]<stderr>:#015 11%|█         | 83/750 [02:08<16:51,  1.52s/it][1,8]<stderr>:#015 11%|█         | 84/750 [02:09<16:08,  1.45s/it][1,0]<stderr>:#015 11%|█         | 84/750 [02:09<16:17,  1.47s/it][1,0]<stderr>:#015 11%|█▏        | 85/750 [02:10<15:38,  1.41s/it][1,8]<stderr>:#015 11%|█▏        | 85/750 [02:11<16:26,  1.48s/it][1,0]<stderr>:#015 11%|█▏        | 86/750 [02:12<15:57,  1.44s/it][1,8]<stderr>:#015 11%|█▏        | 86/750 [02:12<16:13,  1.47s/it][1,0]<stderr>:#015 12%|█▏        | 87/750 [02:14<16:41,  1.51s/it][1,8]<stderr>:#015 12%|█▏        | 87/750 [02:14<16:49,  1.52s/it][1,8]<stderr>:#015 12%|█▏        | 88/750 [02:15<16:17,  1.48s/it][1,0]<stderr>:#015 12%|█▏        | 88/750 [02:15<16:34,  1.50s/it][1,8]<stderr>:#015 12%|█▏        | 89/750 [02:17<16:18,  1.48s/it][1,0]<stderr>:#015 12%|█▏        | 89/750 [02:17<16:42,  1.52s/it][1,8]<stderr>:#015 12%|█▏        | 90/750 [02:18<15:52,  1.44s/it][1,0]<stderr>:#015 12%|█▏        | 90/750 [02:18<16:20,  1.49s/it][1,8]<stderr>:#015 12%|█▏        | 91/750 [02:20<16:27,  1.50s/it][1,0]<stderr>:#015 12%|█▏        | 91/750 [02:20<16:28,  1.50s/it][1,0]<stderr>:#015 12%|█▏        | 92/750 [02:21<15:48,  1.44s/it][1,8]<stderr>:#015 12%|█▏        | 92/750 [02:21<15:55,  1.45s/it][1,0]<stderr>:#015 12%|█▏        | 93/750 [02:22<15:17,  1.40s/it][1,8]<stderr>:#015 12%|█▏        | 93/750 [02:22<15:22,  1.40s/it][1,8]<stderr>:#015 13%|█▎        | 94/750 [02:23<14:57,  1.37s/it][1,0]<stderr>:#015 13%|█▎        | 94/750 [02:24<15:09,  1.39s/it][1,8]<stderr>:#015 13%|█▎        | 95/750 [02:25<15:30,  1.42s/it][1,0]<stderr>:#015 13%|█▎        | 95/750 [02:25<15:53,  1.46s/it][1,0]<stderr>:#015 13%|█▎        | 96/750 [02:26<15:10,  1.39s/it][1,8]<stderr>:#015 13%|█▎        | 96/750 [02:26<15:24,  1.41s/it][1,0]<stderr>:#015 13%|█▎        | 97/750 [02:28<15:44,  1.45s/it][1,8]<stderr>:#015 13%|█▎        | 97/750 [02:28<16:01,  1.47s/it][1,8]<stderr>:#015 13%|█▎        | 98/750 [02:30<16:10,  1.49s/it][1,0]<stderr>:#015 13%|█▎        | 98/750 [02:30<16:24,  1.51s/it][1,8]<stderr>:#015 13%|█▎        | 99/750 [02:31<15:53,  1.46s/it][1,0]<stderr>:#015 13%|█▎        | 99/750 [02:31<16:03,  1.48s/it][1,8]<stderr>:#015 13%|█▎        | 100/750 [02:33<16:13,  1.50s/it][1,0]<stderr>:#015 13%|█▎        | 100/750 [02:33<16:45,  1.55s/it][1,0]<stderr>:#015 13%|█▎        | 101/750 [02:34<15:42,  1.45s/it][1,8]<stderr>:#015 13%|█▎        | 101/750 [02:34<16:02,  1.48s/it][1,8]<stderr>:#015 14%|█▎        | 102/750 [02:35<15:45,  1.46s/it][1,0]<stderr>:#015 14%|█▎        | 102/750 [02:35<15:44,  1.46s/it][1,0]<stderr>:#015 14%|█▎        | 103/750 [02:37<15:53,  1.47s/it][1,8]<stderr>:#015 14%|█▎        | 103/750 [02:37<16:26,  1.52s/it][1,0]<stderr>:#015 14%|█▍        | 104/750 [02:38<15:27,  1.44s/it][1,8]<stderr>:#015 14%|█▍        | 104/750 [02:38<15:50,  1.47s/it][1,0]<stderr>:#015 14%|█▍        | 105/750 [02:40<15:15,  1.42s/it][1,8]<stderr>:#015 14%|█▍        | 105/750 [02:40<15:17,  1.42s/it][1,0]<stderr>:#015 14%|█▍        | 106/750 [02:41<15:43,  1.47s/it][1,8]<stderr>:#015 14%|█▍        | 106/750 [02:41<15:43,  1.47s/it][1,8]<stderr>:#015 14%|█▍        | 107/750 [02:43<15:43,  1.47s/it][1,0]<stderr>:#015 14%|█▍        | 107/750 [02:43<15:59,  1.49s/it][1,0]<stderr>:#015 14%|█▍        | 108/750 [02:44<16:13,  1.52s/it][1,8]<stderr>:#015 14%|█▍        | 108/750 [02:44<16:16,  1.52s/it][1,8]<stderr>:#015 15%|█▍        | 109/750 [02:46<16:14,  1.52s/it][1,0]<stderr>:#015 15%|█▍        | 109/750 [02:46<16:44,  1.57s/it][1,8]<stderr>:#015 15%|█▍        | 110/750 [02:47<16:14,  1.52s/it][1,0]<stderr>:#015 15%|█▍        | 110/750 [02:48<16:27,  1.54s/it][1,8]<stderr>:#015 15%|█▍        | 111/750 [02:49<15:38,  1.47s/it][1,0]<stderr>:#015 15%|█▍        | 111/750 [02:49<15:34,  1.46s/it][1,8]<stderr>:#015 15%|█▍        | 112/750 [02:50<15:49,  1.49s/it][1,0]<stderr>:#015 15%|█▍        | 112/750 [02:50<15:53,  1.49s/it][1,0]<stderr>:#015 15%|█▌        | 113/750 [02:52<15:59,  1.51s/it][1,8]<stderr>:#015 15%|█▌        | 113/750 [02:52<16:14,  1.53s/it][1,8]<stderr>:#015 15%|█▌        | 114/750 [02:53<15:50,  1.49s/it][1,0]<stderr>:#015 15%|█▌        | 114/750 [02:53<15:50,  1.49s/it][1,8]<stderr>:#015 15%|█▌        | 115/750 [02:55<15:37,  1.48s/it][1,0]<stderr>:#015 15%|█▌        | 115/750 [02:55<15:54,  1.50s/it][1,0]<stderr>:#015 15%|█▌        | 116/750 [02:57<16:06,  1.52s/it][1,8]<stderr>:#015 15%|█▌        | 116/750 [02:57<16:28,  1.56s/it][1,8]<stderr>:#015 16%|█▌        | 117/750 [02:58<16:38,  1.58s/it][1,0]<stderr>:#015 16%|█▌        | 117/750 [02:58<16:40,  1.58s/it][1,0]<stderr>:#015 16%|█▌        | 118/750 [02:59<15:05,  1.43s/it][1,8]<stderr>:#015 16%|█▌        | 118/750 [02:59<15:20,  1.46s/it][1,0]<stderr>:#015 16%|█▌        | 119/750 [03:01<14:22,  1.37s/it][1,8]<stderr>:#015 16%|█▌        | 119/750 [03:01<14:54,  1.42s/it][1,8]<stderr>:#015 16%|█▌        | 120/750 [03:02<15:42,  1.50s/it][1,0]<stderr>:#015 16%|█▌        | 120/750 [03:02<16:06,  1.53s/it][1,0]<stderr>:#015 16%|█▌        | 121/750 [03:04<15:58,  1.52s/it][1,8]<stderr>:#015 16%|█▌        | 121/750 [03:04<16:11,  1.55s/it][1,8]<stderr>:#015 16%|█▋        | 122/750 [03:06<16:23,  1.57s/it][1,0]<stderr>:#015 16%|█▋        | 122/750 [03:06<16:35,  1.59s/it][1,0]<stderr>:#015 16%|█▋        | 123/750 [03:07<15:47,  1.51s/it][1,8]<stderr>:#015 16%|█▋        | 123/750 [03:07<16:13,  1.55s/it][1,0]<stderr>:#015 17%|█▋        | 124/750 [03:09<16:26,  1.58s/it][1,8]<stderr>:#015 17%|█▋        | 124/750 [03:09<16:26,  1.58s/it][1,8]<stderr>:#015 17%|█▋        | 125/750 [03:10<15:\u001b[0m\n",
      "\u001b[34m44,  1.51s/it][1,0]<stderr>:#015 17%|█▋        | 125/750 [03:10<16:06,  1.55s/it][1,8]<stderr>:#015 17%|█▋        | 126/750 [03:12<16:11,  1.56s/it][1,0]<stderr>:#015 17%|█▋        | 126/750 [03:12<16:16,  1.56s/it][1,8]<stderr>:#015 17%|█▋        | 127/750 [03:13<15:51,  1.53s/it][1,0]<stderr>:#015 17%|█▋        | 127/750 [03:13<15:50,  1.53s/it][1,8]<stderr>:#015 17%|█▋        | 128/750 [03:15<15:44,  1.52s/it][1,0]<stderr>:#015 17%|█▋        | 128/750 [03:15<15:55,  1.54s/it][1,8]<stderr>:#015 17%|█▋        | 129/750 [03:16<15:50,  1.53s/it][1,0]<stderr>:#015 17%|█▋        | 129/750 [03:16<16:08,  1.56s/it][1,0]<stderr>:#015 17%|█▋        | 130/750 [03:18<15:24,  1.49s/it][1,8]<stderr>:#015 17%|█▋        | 130/750 [03:18<15:39,  1.51s/it][1,8]<stderr>:#015 17%|█▋        | 131/750 [03:19<15:02,  1.46s/it][1,0]<stderr>:#015 17%|█▋        | 131/750 [03:19<15:06,  1.47s/it][1,0]<stderr>:#015 18%|█▊        | 132/750 [03:21<15:30,  1.51s/it][1,8]<stderr>:#015 18%|█▊        | 132/750 [03:21<15:53,  1.54s/it][1,0]<stderr>:#015 18%|█▊        | 133/750 [03:22<15:01,  1.46s/it][1,8]<stderr>:#015 18%|█▊        | 133/750 [03:22<15:21,  1.49s/it][1,0]<stderr>:#015 18%|█▊        | 134/750 [03:23<14:29,  1.41s/it][1,8]<stderr>:#015 18%|█▊        | 134/750 [03:23<14:35,  1.42s/it][1,0]<stderr>:#015 18%|█▊        | 135/750 [03:25<14:07,  1.38s/it][1,8]<stderr>:#015 18%|█▊        | 135/750 [03:25<14:38,  1.43s/it][1,8]<stderr>:#015 18%|█▊        | 136/750 [03:26<13:52,  1.36s/it][1,0]<stderr>:#015 18%|█▊        | 136/750 [03:26<14:43,  1.44s/it][1,0]<stderr>:#015 18%|█▊        | 137/750 [03:28<14:57,  1.46s/it][1,8]<stderr>:#015 18%|█▊        | 137/750 [03:28<15:01,  1.47s/it][1,8]<stderr>:#015 18%|█▊        | 138/750 [03:29<14:41,  1.44s/it][1,0]<stderr>:#015 18%|█▊        | 138/750 [03:29<14:46,  1.45s/it][1,0]<stderr>:#015 19%|█▊        | 139/750 [03:30<14:03,  1.38s/it][1,8]<stderr>:#015 19%|█▊        | 139/750 [03:31<14:13,  1.40s/it][1,0]<stderr>:#015 19%|█▊        | 140/750 [03:32<13:43,  1.35s/it][1,8]<stderr>:#015 19%|█▊        | 140/750 [03:32<13:41,  1.35s/it][1,0]<stderr>:#015 19%|█▉        | 141/750 [03:33<14:17,  1.41s/it][1,8]<stderr>:#015 19%|█▉        | 141/750 [03:33<14:18,  1.41s/it][1,8]<stderr>:#015 19%|█▉        | 142/750 [03:35<14:20,  1.41s/it][1,0]<stderr>:#015 19%|█▉        | 142/750 [03:35<14:45,  1.46s/it][1,8]<stderr>:#015 19%|█▉        | 143/750 [03:36<15:01,  1.49s/it][1,0]<stderr>:#015 19%|█▉        | 143/750 [03:37<15:23,  1.52s/it][1,8]<stderr>:#015 19%|█▉        | 144/750 [03:38<15:15,  1.51s/it][1,0]<stderr>:#015 19%|█▉        | 144/750 [03:38<15:10,  1.50s/it][1,8]<stderr>:#015 19%|█▉        | 145/750 [03:39<15:00,  1.49s/it][1,0]<stderr>:#015 19%|█▉        | 145/750 [03:39<15:06,  1.50s/it][1,8]<stderr>:#015 19%|█▉        | 146/750 [03:41<14:30,  1.44s/it][1,0]<stderr>:#015 19%|█▉        | 146/750 [03:41<14:22,  1.43s/it][1,8]<stderr>:#015 20%|█▉        | 147/750 [03:42<14:55,  1.48s/it][1,0]<stderr>:#015 20%|█▉        | 147/750 [03:42<14:49,  1.47s/it][1,8]<stderr>:#015 20%|█▉        | 148/750 [03:44<15:07,  1.51s/it][1,0]<stderr>:#015 20%|█▉        | 148/750 [03:44<15:27,  1.54s/it][1,8]<stderr>:#015 20%|█▉        | 149/750 [03:45<15:08,  1.51s/it][1,0]<stderr>:#015 20%|█▉        | 149/750 [03:45<15:07,  1.51s/it][1,8]<stderr>:#015 20%|██        | 150/750 [03:47<15:39,  1.57s/it][1,0]<stderr>:#015 20%|██        | 150/750 [03:47<15:31,  1.55s/it][1,8]<stderr>:#015 20%|██        | 151/750 [03:49<15:55,  1.60s/it][1,0]<stderr>:#015 20%|██        | 151/750 [03:49<15:53,  1.59s/it][1,0]<stderr>:#015 20%|██        | 152/750 [03:50<14:52,  1.49s/it][1,8]<stderr>:#015 20%|██        | 152/750 [03:50<15:09,  1.52s/it][1,0]<stderr>:#015 20%|██        | 153/750 [03:51<14:25,  1.45s/it][1,8]<stderr>:#015 20%|██        | 153/750 [03:51<14:34,  1.47s/it][1,8]<stderr>:#015 21%|██        | 154/750 [03:53<15:35,  1.57s/it][1,0]<stderr>:#015 21%|██        | 154/750 [03:53<15:41,  1.58s/it][1,8]<stderr>:#015 21%|██        | 155/750 [03:55<15:33,  1.57s/it][1,0]<stderr>:#015 21%|██        | 155/750 [03:55<15:33,  1.57s/it][1,8]<stderr>:#015 21%|██        | 156/750 [03:56<15:35,  1.57s/it][1,0]<stderr>:#015 21%|██        | 156/750 [03:56<15:37,  1.58s/it][1,8]<stderr>:#015 21%|██        | 157/750 [03:58<15:52,  1.61s/it][1,0]<stderr>:#015 21%|██        | 157/750 [03:58<16:08,  1.63s/it][1,0]<stderr>:#015 21%|██        | 158/750 [03:59<14:48,  1.50s/it][1,8]<stderr>:#015 21%|██        | 158/750 [03:59<14:59,  1.52s/it][1,8]<stderr>:#015 21%|██        | 159/750 [04:01<14:54,  1.51s/it][1,0]<stderr>:#015 21%|██        | 159/750 [04:01<15:22,  1.56s/it][1,8]<stderr>:#015 21%|██▏       | 160/750 [04:02<14:23,  1.46s/it][1,0]<stderr>:#015 21%|██▏       | 160/750 [04:02<14:14,  1.45s/it][1,0]<stderr>:#015 21%|██▏       | 161/750 [04:03<13:24,  1.37s/it][1,8]<stderr>:#015 21%|██▏       | 161/750 [04:03<13:36,  1.39s/it][1,8]<stderr>:#015 22%|██▏       | 162/750 [04:05<14:17,  1.46s/it][1,0]<stderr>:#015 22%|██▏       | 162/750 [04:05<14:18,  1.46s/it][1,8]<stderr>:#015 22%|██▏       | 163/750 [04:06<13:59,  1.43s/it][1,0]<stderr>:#015 22%|██▏       | 163/750 [04:07<14:15,  1.46s/it][1,8]<stderr>:#015 22%|██▏       | 164/750 [04:08<14:11,  1.45s/it][1,0]<stderr>:#015 22%|██▏       | 164/750 [04:08<14:05,  1.44s/it][1,8]<stderr>:#015 22%|██▏       | 165/750 [04:10<14:36,  1.50s/it][1,0]<stderr>:#015 22%|██▏       | 165/750 [04:10<14:41,  1.51s/it][1,8]<stderr>:#015 22%|██▏       | 166/750 [04:11<14:39,  1.51s/it][1,0]<stderr>:#015 22%|██▏       | 166/750 [04:11<14:44,  1.52s/it][1,8]<stderr>:#015 22%|██▏       | 167/750 [04:13<15:20,  1.58s/it][1,0]<stderr>:#015 22%|██▏       | 167/750 [04:13<15:21,  1.58s/it][1,8]<stderr>:#015 22%|██▏       | 168/750 [04:14<15:20,  1.58s/it][1,0]<stderr>:#015 22%|██▏       | 168/750 [04:15<15:25,  1.59s/it][1,8]<stderr>:#015 23%|██▎       | 169/750 [04:16<15:49,  1.63s/it][1,0]<stderr>:#015 23%|██▎       | 169/750 [04:16<15:46,  1.63s/it][1,8]<stderr>:#015 23%|██▎       | 170/750 [04:18<15:09,  1.57s/it][1,0]<stderr>:#015 23%|██▎       | 170/750 [04:18<15:07,  1.56s/it][1,8]<stderr>:#015 23%|██▎       | 171/750 [04:19<14:38,  1.52s/it][1,0]<stderr>:#015 23%|██▎       | 171/750 [04:19<14:27,  1.50s/it][1,0]<stderr>:#015 23%|██▎       | 172/750 [04:20<14:14,  1.48s/it][1,8]<stderr>:#015 23%|██▎       | 172/750 [04:20<14:26,  1.50s/it][1,8]<stderr>:#015 23%|██▎       | 173/750 [04:22<14:35,  1.52s/it][1,0]<stderr>:#015 23%|██▎       | 173/750 [04:22<14:39,  1.52s/it][1,0]<stderr>:#015 23%|██▎       | 174/750 [04:24<14:34,  1.52s/it][1,8]<stderr>:#015 23%|██▎       | 174/750 [04:24<15:03,  1.57s/it][1,8]<stderr>:#015 23%|██▎       | 175/750 [04:25<15:24,  1.61s/it][1,0]<stderr>:#015 23%|██▎       | 175/750 [04:26<15:46,  1.65s/it][1,0]<stderr>:#015 23%|██▎       | 176/750 [04:27<15:38,  1.63s/it][1,8]<stderr>:#015 23%|██▎       | 176/750 [04:27<15:48,  1.65s/it][1,8]<stderr>:#015 24%|██▎       | 177/750 [04:29<15:40,  1.64s/it][1,0]<stderr>:#015 24%|██▎       | 177/750 [04:29<15:52,  1.66s/it][1,8]<stderr>:#015 24%|██▎       | 178/750 [04:30<15:25,  1.62s/it][1,0]<stderr>:#015 24%|██▎       | 178/750 [04:30<15:28,  1.62s/it][1,8]<stderr>:#015 24%|██▍       | 179/750 [04:32<15:37,  1.64s/it][1,0]<stderr>:#015 24%|██▍       | 179/750 [04:32<15:47,  1.66s/it][1,8]<stderr>:#015 24%|██▍       | 180/750 [04:34<15:50,  1.67s/it][1,0]<stderr>:#015 24%|██▍       | 180/750 [04:34<16:03,  1.69s/it][1,0]<stderr>:#015 24%|██▍       | 181/750 [04:35<15:36,  1.65s/it][1,8]<stderr>:#015 24%|██▍       | 181/750 [04:35<15:53,  1.68s/it][1,8]<stderr>:#015 24%|██▍       | 182/750 [04:37<14:19,  1.51s/it][1,0]<stderr>:#015 24%|██▍       | 182/750 [04:37<14:31,  1.53s/it][1,8]<stderr>:#015 24%|██▍       | 183/750 [04:38<14:54,  1.58s/it][1,0]<stderr>:#015 24%|██▍       | 183/750 [04:38<14:43,  1.56s/it][1,0]<stderr>:#015 25%|██▍       | 184/750 [04:40<14:53,  1.58s/it][1,8]<stderr>:#015 25%|██▍       | 184/750 [04:40<15:03,  1.60s/it][1,8]<stderr>:#015 25%|██▍       | 185/750 [04:42<15:08,  1.61s/it][1,0]<stderr>:#015 25%|██▍       | 185/750 [04:42<15:06,  1.60s/it][1,0]<stderr>:#015 25%|██▍       | 186/750 [04:43<14:36,  1.55s/it][1,8]<stderr>:#015 25%|██▍       | 186/750 [04:43<14:46,  1.57s/it][1,0]<stderr>:#015 25%|██▍       | 187/750 [04:45<14:17,  1.52s/it][1,8]<stderr>:#015 25%|██▍       | 187/750 [04:45<14:35,  1.56s/it][1,0]<stderr>:#015 25%|██▌       | 188/750 [04:46<14:39,  1.56s/it][1,8]<stderr>:#015 25%|██▌       | 188/750 [04:46<14:45,  1.57s/it][1,8]<stderr>:#015 25%|██▌       | 189/750 [04:48<14:14,  1.52s/it][1,0]<stderr>:#015 25%|██▌       | 189/750 [04:48<14:33,  1.56s/it][1,8]<stderr>:#015 25%|██▌       | 190/750 [04:49<15:07,  1.62s/it][1,0]<stderr>:#015 25%|██▌       | 190/750 [04:50<15:20,  1.64s/it][1,8]<stderr>:#015 25%|██▌       | 191/750 [04:51<14:49,  1.59s/it][1,0]<stderr>:#015 25%|██▌       | 191/750 [04:51<15:03,  1.62s/it][1,8]<stderr>:#015 26%|██▌       | 192/750 [04:52<13:58,  1.50s/it][1,0]<stderr>:#015 26%|██▌       | 192/750 [04:52<14:04,  1.51s/it][1,8]<stderr>:#015 26%|██▌       | 193/750 [04:54<14:07,  1.52s/it][1,0]<stderr>:#015 26%|██▌       | 193/750 [04:54<14:02,  1.51s/it][1,8]<stderr>:#015 26%|██▌       | 194/750 [04:55<13:45,  1.48s/it][1,0]<stderr>:#015 26%|██▌       | 194/750 [04:55<13:49,  1.49s/it][1,0]<stderr>:#015 26%|██▌       | 195/750 [04:57<13:22,  1.45s/it][1,8]<stderr>:#015 26%|██▌       | 195/750 [04:57<13:47,  1.49s/it][1,8]<stderr>:#015 26%|██▌       | 196/750 [04:58<13:34,  1.47s/it][1,0]<stderr>:#015 26%|██▌       | 196/750 [04:58<13:56,  1.51s/it][1,0]<stderr>:#015 26%|██▋       | 197/750 [05:00<13:33,  1.47s/it][1,8]<stderr>:#015 26%|██▋       | 197/750 [05:00<13:44,  1.49s/it][1,0]<stderr>:#015 26%|██▋       | 198/750 [05:01<12:49,  1.39s/it][1,8]<stderr>:#015 26%|██▋       | 198/750 [05:01<12:56,  1.41s/it][1,0]<stderr>:#015 27%|██▋       | 199/750 [05:03<14:08,  1.54s/it][1,8]<stderr>:#015 27%|██▋       | 199/750 [05:03<14:12,  1.55s/it][1,8]<stderr>:#015 27%|██▋       | 200/750 [05:04<13:04,  1.43s/it][1,0]<stderr>:#015 27%|██▋       | 200/750 [05:04<13:14,  1.44s/it][1,8]<stderr>:#015 27%|██▋       | 201/750 [05:05<13:12,  1.44s/it][1,0]<stderr>:#015 27%|██▋       | 201/750 [05:06<13:30,  1.48s/it][1,0]<stderr>:#015 27%|██▋       | 202/750 [05:07<12:42,  1.39s/it][1,8]<stderr>:#015 27%|██▋       | 202/750 [05:07<13:17,  1.46s/it][1,0]<stderr>:#015 27%|██▋       | 203/750 [05:08<13:16,  1.46s/it][1,8]<stderr>:#015 27%|██▋       | 203/750 [05:08<13:30,  1.48s/it][1,8]<stderr>:#015 27%|██▋       | 204/750 [05:10<14:02,  1.54s/it][1,0]<stderr>:#015 27%|██▋       | 204/750 [05:10<14:29,  1.59s/it][1,0]<stderr>:#015 27%|██▋       | 205/750 [05:12<14:02,  1.55s/it][1,8]<stderr>:#015 27%|██▋       | 205/750 [05:12<14:09,  1.56s/it][1,8]<stderr>:#015 27%|██▋       | 206/750 [05:13<13:18,  1.47s/it][1,0]<stderr>:#015 27%|██▋       | 206/750 [05:13<13:25,  1.48s/it][1,8]<stderr>:#015 28%|██▊       | 207/750 [05:15<13:49,  1.53s/it][1,0]<stderr>:#015 28%|██▊       | 207/750 [05:15<13:52,  1.53s/it][1,0]<stderr>:#015 28%|██▊       | 208/750 [05:16<13:27,  1.49s/it][1,8]<stderr>:#015 28%|██▊       | 208/750 [05:16<13:34,  1.50s/it][1,0]<stderr>:#015 28%|██▊       | 209/750 [05:18<14:33,  1.62s/it][1,8]<stderr>:#015 28%|██▊       | 209/750 [05:18<14:41,  1.63s/it][1,8]<stderr>:#015 28%|██▊       | 210/750 [05:20<14:32,  1.62s/it][1,0]<stderr>:#015 28%|██▊       | 210/750 [05:20<14:42,  1.63s/it][1,8]<stderr>:#015 28%|██▊       | 211/750 [05:21<14:06,  1.57s/it][1,0]<stderr>:#015 28%|██▊       | 211/750 [05:21<14:25,  1.61s/it][1,0]<stderr>:#015 28%|██▊       | 212/750 [05:22<13:23,  1.49s/it][1,8]<stderr>:#015 28%|██▊       | 212/750 [05:22<13:35,  1.52s/it][1,0]<stderr>:#015 28%|██▊       | 213/750 [05:24<12:48,  1.43s/it][1,8]<stderr>:#015 28%|██▊       | 213/750 [05:24<13:25,  1.50s/it][1,0]<stderr>:#015 29%|██▊       | 214/750 [05:25<12:53,  1.44s/it][1,8]<stderr>:#015 29%|██▊       | 214/750 [05:25<13:08,  1.47s/it][1,0]<stderr>:#015 29%|██▊       | 215/750 [05:27<12:46,  1.43s/it][1,8]<stderr>:#015 29%|██▊       | 215/750 [05:27<12:53,  1.45s/it][1,0]<stderr>:#015 29%|██▉       | 216/750 [05:28<12:45,  1.43s/it][1,8]<stderr>:#015 29%|██▉       | 216/750 [05:28<12:33,  1.41s/it][1,8]<stderr>:#015 29%|██▉       | 217/750 [05:29<12:15,  1.38s/it][1,0]<stderr>:#015 29%|██▉       | 217/750 [05:29<12:28,  1.40s/it][1,0]<stderr>:#015 29%|██▉       | 218/750 [05:31<12:42,  1.43s/it][1,8]<stderr>:#015 29%|██▉       | 218/750 [05:31<12:53,  1.45s/it][1,0]<stderr>:#015 29%|██▉       | 219/750 [05:32<12:32,  1.42s/it][1,8]<stderr>:#015 29%|██▉       | 219/750 [05:32<12:26,  1.41s/it][1,0]<stderr>:#015 29%|██▉       | 220/750 [05:34<12:40,  1.44s/it][1,8]<stderr>:#015 29%|██▉       | 220/750 [05:34<12:41,  1.44s/it][1,8]<stderr>:#015 29%|██▉       | 221/750 [05:35<12:37,  1.43s/it][1,0]<stderr>:#015 29%|██▉       | 221/750 [05:35<12:51,  1.46s/it][1,0]<stderr>:#015 30%|██▉       | 222/750 [05:37<12:46,  1.45s/it][1,8]<stderr>:#015 30%|██▉       | 222/750 [05:37<12:55,  1.47s/it][1,0]<stderr>:#015 30%|██▉       | 223/750 [05:38<13:07,  1.49s/it][1,8]<stderr>:#015 30%|██▉       | 223/750 [05:38<13:02,  1.48s/it][1,8]<stderr>:#015 30%|██▉       | 224/750 [05:40<12:31,  1.43s/it][1,0]<stderr>:#015 30%|██▉       | 224/750 [05:40<12:35,  1.44s/it][1,8]<stderr>:#015 30%|███       | 225/750 [05:41<12:13,  1.40s/it][1,0]<stderr>:#015 30%|███       | 225/750 [05:41<12:16,  1.40s/it][1,8]<stderr>:#015 30%|███       | 226/750 [05:42<12:24,  1.42s/it][1,0]<stderr>:#015 30%|███       | 226/750 [05:42<12:29,  1.43s/it][1,8]<stderr>:#015 30%|███       | 227/750 [05:44<12:21,  1.42s/it][1,0]<stderr>:#015 30%|███       | 227/750 [05:44<12:33,  1.44s/it][1,8]<stderr>:#015 30%|███       | 228/750 [05:45<12:25,  1.43s/it][1,0]<stderr>:#015 30%|███       | 228/750 [05:45<12:22,  1.42s/it][1,0]<stderr>:#015 31%|███       | 229/750 [05:47<12:03,  1.39s/it][1,8]<stderr>:#015 31%|███       | 229/750 [05:47<12:22,  1.43s/it][1,0]<stderr>:#015 31%|███       | 230/750 [05:48<12:29,  1.44s/it][1,8]<stderr>:#015 31%|███       | 230/750 [05:48<12:39,  1.46s/it][1,0]<stderr>:#015 31%|███       | 231/750 [05:50<12:21,  1.43s/it][1,8]<stderr>:#015 31%|███       | 231/750 [05:50<12:23,  1.43s/it][1,0]<stderr>:#015 31%|███       | 232/750 [05:51<12:31,  1.45s/it][1,8]<stderr>:#015 31%|███       | 232/750 [05:51<12:36,  1.46s/it][1,0]<stderr>:#015 31%|███       | 233/750 [05:52<12:22,  1.44s/it][1,8]<stderr>:#015 31%|███       | 233/750 [05:53<13:01,  1.51s/it][1,8]<stderr>:#015 31%|███       | 234/750 [05:54<11:52,  1.38s/it][1,0]<stderr>:#015 31%|███       | 234/750 [05:54<12:25,  1.45s/it][1,8]<stderr>:#015 31%|███▏      | 235/750 [05:55<11:20,  1.32s/it][1,0]<stderr>:#015 31%|███▏      | 235/750 [05:55<11:45,  1.37s/it][1,0]<stderr>:#015 31%|███▏      | 236/750 [05:56<11:43,  1.37s/it][1,8]<stderr>:#015 31%|███▏      | 236/750 [05:56<11:48,  1.38s/it][1,0]<stderr>:#015 32%|███▏      | 237/750 [05:58<11:49,  1.38s/it][1,8]<stderr>:#015 32%|███▏      | 237/750 [05:58<11:46,  1.38s/it][1,0]<stderr>:#015 32%|███▏      | 238/750 [05:59<11:57,  1.40s/it][1,8]<stderr>:#015 32%|███▏      | 238/750 [05:59<12:06,  1.42s/it][1,8]<stderr>:#015 32%|███▏      | 239/750 [06:01<11:31,  1.35s/it][1,0]<stderr>:#015 32%|███▏      | 239/750 [06:01<12:07,  1.42s/it][1,8]<stderr>:#015 32%|███▏      | 240/750 [06:02<11:29,  1.35s/it][1,0]<stderr>:#015 32%|███▏      | 240/750 [06:02<11:30,  1.35s/it][1,8]<stderr>:#015 32%|███▏      | 241/750 [06:03<11:52,  1.40s/it][1,0]<stderr>:#015 32%|███▏      | 241/750 [06:03<11:49,  1.39s/it][1,8]<stderr>:#015 32%|███▏      | 242/750 [06:05<12:14,  1.45s/it][1,0]<stderr>:#015 32%|███▏      | 242/750 [06:05<12:21,  1.46s/it][1,8]<stderr>:#015 32%|██\u001b[0m\n",
      "\u001b[34m█▏      | 243/750 [06:07<12:52,  1.52s/it][1,0]<stderr>:#015 32%|███▏      | 243/750 [06:07<13:13,  1.57s/it][1,8]<stderr>:#015 33%|███▎      | 244/750 [06:08<12:24,  1.47s/it][1,0]<stderr>:#015 33%|███▎      | 244/750 [06:08<12:34,  1.49s/it][1,8]<stderr>:#015 33%|███▎      | 245/750 [06:10<12:26,  1.48s/it][1,0]<stderr>:#015 33%|███▎      | 245/750 [06:10<12:13,  1.45s/it][1,8]<stderr>:#015 33%|███▎      | 246/750 [06:11<12:19,  1.47s/it][1,0]<stderr>:#015 33%|███▎      | 246/750 [06:11<12:23,  1.48s/it][1,8]<stderr>:#015 33%|███▎      | 247/750 [06:13<12:33,  1.50s/it][1,0]<stderr>:#015 33%|███▎      | 247/750 [06:13<13:14,  1.58s/it][1,8]<stderr>:#015 33%|███▎      | 248/750 [06:14<12:59,  1.55s/it][1,0]<stderr>:#015 33%|███▎      | 248/750 [06:14<12:52,  1.54s/it][1,8]<stderr>:#015 33%|███▎      | 249/750 [06:15<12:13,  1.47s/it][1,0]<stderr>:#015 33%|███▎      | 249/750 [06:16<11:52,  1.42s/it][1,8]<stderr>:#015 33%|███▎      | 250/750 [06:17<12:31,  1.50s/it][1,0]<stderr>:#015 33%|███▎      | 250/750 [06:17<12:37,  1.51s/it][1,0]<stderr>:#015 33%|███▎      | 251/750 [06:18<11:30,  1.38s/it][1,8]<stderr>:#015 33%|███▎      | 251/750 [06:18<11:51,  1.43s/it][1,8]<stderr>:#015 34%|███▎      | 252/750 [06:20<11:53,  1.43s/it][1,0]<stderr>:#015 34%|███▎      | 252/750 [06:20<11:55,  1.44s/it][1,8]<stderr>:#015 34%|███▎      | 253/750 [06:21<11:27,  1.38s/it][1,0]<stderr>:#015 34%|███▎      | 253/750 [06:21<11:23,  1.37s/it][1,0]<stderr>:#015 34%|███▍      | 254/750 [06:22<11:12,  1.36s/it][1,8]<stderr>:#015 34%|███▍      | 254/750 [06:22<11:30,  1.39s/it][1,0]<stderr>:#015 34%|███▍      | 255/750 [06:24<10:52,  1.32s/it][1,8]<stderr>:#015 34%|███▍      | 255/750 [06:24<11:03,  1.34s/it][1,0]<stderr>:#015 34%|███▍      | 256/750 [06:25<10:55,  1.33s/it][1,8]<stderr>:#015 34%|███▍      | 256/750 [06:25<11:07,  1.35s/it][1,0]<stderr>:#015 34%|███▍      | 257/750 [06:27<11:35,  1.41s/it][1,8]<stderr>:#015 34%|███▍      | 257/750 [06:27<11:43,  1.43s/it][1,8]<stderr>:#015 34%|███▍      | 258/750 [06:28<11:50,  1.44s/it][1,0]<stderr>:#015 34%|███▍      | 258/750 [06:28<11:59,  1.46s/it][1,8]<stderr>:#015 35%|███▍      | 259/750 [06:29<11:29,  1.40s/it][1,0]<stderr>:#015 35%|███▍      | 259/750 [06:30<11:46,  1.44s/it][1,0]<stderr>:#015 35%|███▍      | 260/750 [06:31<11:47,  1.44s/it][1,8]<stderr>:#015 35%|███▍      | 260/750 [06:31<11:59,  1.47s/it][1,8]<stderr>:#015 35%|███▍      | 261/750 [06:33<12:48,  1.57s/it][1,0]<stderr>:#015 35%|███▍      | 261/750 [06:33<12:54,  1.58s/it][1,8]<stderr>:#015 35%|███▍      | 262/750 [06:34<12:24,  1.53s/it][1,0]<stderr>:#015 35%|███▍      | 262/750 [06:34<12:36,  1.55s/it][1,0]<stderr>:#015 35%|███▌      | 263/750 [06:36<11:46,  1.45s/it][1,8]<stderr>:#015 35%|███▌      | 263/750 [06:36<12:06,  1.49s/it][1,0]<stderr>:#015 35%|███▌      | 264/750 [06:37<12:29,  1.54s/it][1,8]<stderr>:#015 35%|███▌      | 264/750 [06:37<12:36,  1.56s/it][1,8]<stderr>:#015 35%|███▌      | 265/750 [06:39<11:46,  1.46s/it][1,0]<stderr>:#015 35%|███▌      | 265/750 [06:39<11:50,  1.47s/it][1,0]<stderr>:#015 35%|███▌      | 266/750 [06:40<11:30,  1.43s/it][1,8]<stderr>:#015 35%|███▌      | 266/750 [06:40<11:41,  1.45s/it][1,8]<stderr>:#015 36%|███▌      | 267/750 [06:41<11:18,  1.40s/it][1,0]<stderr>:#015 36%|███▌      | 267/750 [06:41<11:23,  1.41s/it][1,0]<stderr>:#015 36%|███▌      | 268/750 [06:43<11:45,  1.46s/it][1,8]<stderr>:#015 36%|███▌      | 268/750 [06:43<11:50,  1.47s/it][1,0]<stderr>:#015 36%|███▌      | 269/750 [06:44<10:55,  1.36s/it][1,8]<stderr>:#015 36%|███▌      | 269/750 [06:44<11:05,  1.38s/it][1,0]<stderr>:#015 36%|███▌      | 270/750 [06:46<11:03,  1.38s/it][1,8]<stderr>:#015 36%|███▌      | 270/750 [06:46<11:15,  1.41s/it][1,8]<stderr>:#015 36%|███▌      | 271/750 [06:47<11:10,  1.40s/it][1,0]<stderr>:#015 36%|███▌      | 271/750 [06:47<11:24,  1.43s/it][1,0]<stderr>:#015 36%|███▋      | 272/750 [06:48<11:04,  1.39s/it][1,8]<stderr>:#015 36%|███▋      | 272/750 [06:48<11:05,  1.39s/it][1,8]<stderr>:#015 36%|███▋      | 273/750 [06:50<11:16,  1.42s/it][1,0]<stderr>:#015 36%|███▋      | 273/750 [06:50<11:21,  1.43s/it][1,0]<stderr>:#015 37%|███▋      | 274/750 [06:51<10:57,  1.38s/it][1,8]<stderr>:#015 37%|███▋      | 274/750 [06:51<11:10,  1.41s/it][1,0]<stderr>:#015 37%|███▋      | 275/750 [06:53<11:08,  1.41s/it][1,8]<stderr>:#015 37%|███▋      | 275/750 [06:53<11:24,  1.44s/it][1,8]<stderr>:#015 37%|███▋      | 276/750 [06:54<11:09,  1.41s/it][1,0]<stderr>:#015 37%|███▋      | 276/750 [06:54<11:28,  1.45s/it][1,0]<stderr>:#015 37%|███▋      | 277/750 [06:55<10:32,  1.34s/it][1,8]<stderr>:#015 37%|███▋      | 277/750 [06:55<10:44,  1.36s/it][1,8]<stderr>:#015 37%|███▋      | 278/750 [06:57<10:37,  1.35s/it][1,0]<stderr>:#015 37%|███▋      | 278/750 [06:57<10:55,  1.39s/it][1,0]<stderr>:#015 37%|███▋      | 279/750 [06:58<10:54,  1.39s/it][1,8]<stderr>:#015 37%|███▋      | 279/750 [06:58<10:52,  1.39s/it][1,8]<stderr>:#015 37%|███▋      | 280/750 [06:59<10:38,  1.36s/it][1,0]<stderr>:#015 37%|███▋      | 280/750 [07:00<10:50,  1.38s/it][1,8]<stderr>:#015 37%|███▋      | 281/750 [07:01<11:21,  1.45s/it][1,0]<stderr>:#015 37%|███▋      | 281/750 [07:01<11:24,  1.46s/it][1,8]<stderr>:#015 38%|███▊      | 282/750 [07:02<11:02,  1.41s/it][1,0]<stderr>:#015 38%|███▊      | 282/750 [07:03<11:03,  1.42s/it][1,0]<stderr>:#015 38%|███▊      | 283/750 [07:04<10:16,  1.32s/it][1,8]<stderr>:#015 38%|███▊      | 283/750 [07:04<10:27,  1.34s/it][1,8]<stderr>:#015 38%|███▊      | 284/750 [07:05<10:37,  1.37s/it][1,0]<stderr>:#015 38%|███▊      | 284/750 [07:05<10:44,  1.38s/it][1,8]<stderr>:#015 38%|███▊      | 285/750 [07:07<11:02,  1.42s/it][1,0]<stderr>:#015 38%|███▊      | 285/750 [07:07<11:07,  1.43s/it][1,8]<stderr>:#015 38%|███▊      | 286/750 [07:08<10:56,  1.42s/it][1,0]<stderr>:#015 38%|███▊      | 286/750 [07:08<11:00,  1.42s/it][1,0]<stderr>:#015 38%|███▊      | 287/750 [07:09<10:56,  1.42s/it][1,8]<stderr>:#015 38%|███▊      | 287/750 [07:10<11:06,  1.44s/it][1,8]<stderr>:#015 38%|███▊      | 288/750 [07:11<11:28,  1.49s/it][1,0]<stderr>:#015 38%|███▊      | 288/750 [07:11<11:32,  1.50s/it][1,8]<stderr>:#015 39%|███▊      | 289/750 [07:12<11:03,  1.44s/it][1,0]<stderr>:#015 39%|███▊      | 289/750 [07:12<11:04,  1.44s/it][1,8]<stderr>:#015 39%|███▊      | 290/750 [07:14<10:22,  1.35s/it][1,0]<stderr>:#015 39%|███▊      | 290/750 [07:14<10:40,  1.39s/it][1,8]<stderr>:#015 39%|███▉      | 291/750 [07:15<10:45,  1.41s/it][1,0]<stderr>:#015 39%|███▉      | 291/750 [07:15<10:59,  1.44s/it][1,8]<stderr>:#015 39%|███▉      | 292/750 [07:17<10:51,  1.42s/it][1,0]<stderr>:#015 39%|███▉      | 292/750 [07:17<10:50,  1.42s/it][1,8]<stderr>:#015 39%|███▉      | 293/750 [07:18<10:27,  1.37s/it][1,0]<stderr>:#015 39%|███▉      | 293/750 [07:18<10:21,  1.36s/it][1,8]<stderr>:#015 39%|███▉      | 294/750 [07:19<10:48,  1.42s/it][1,0]<stderr>:#015 39%|███▉      | 294/750 [07:20<10:56,  1.44s/it][1,8]<stderr>:#015 39%|███▉      | 295/750 [07:21<10:41,  1.41s/it][1,0]<stderr>:#015 39%|███▉      | 295/750 [07:21<10:35,  1.40s/it][1,0]<stderr>:#015 39%|███▉      | 296/750 [07:22<10:28,  1.38s/it][1,8]<stderr>:#015 39%|███▉      | 296/750 [07:22<10:38,  1.41s/it][1,0]<stderr>:#015 40%|███▉      | 297/750 [07:24<10:24,  1.38s/it][1,8]<stderr>:#015 40%|███▉      | 297/750 [07:24<10:32,  1.40s/it][1,8]<stderr>:#015 40%|███▉      | 298/750 [07:25<11:27,  1.52s/it][1,0]<stderr>:#015 40%|███▉      | 298/750 [07:25<11:28,  1.52s/it][1,8]<stderr>:#015 40%|███▉      | 299/750 [07:27<11:35,  1.54s/it][1,0]<stderr>:#015 40%|███▉      | 299/750 [07:27<11:35,  1.54s/it][1,8]<stderr>:#015 40%|████      | 300/750 [07:28<11:18,  1.51s/it][1,0]<stderr>:#015 40%|████      | 300/750 [07:28<11:15,  1.50s/it][1,8]<stderr>:#015 40%|████      | 301/750 [07:30<11:42,  1.57s/it][1,0]<stderr>:#015 40%|████      | 301/750 [07:30<11:56,  1.60s/it][1,8]<stderr>:#015 40%|████      | 302/750 [07:32<11:28,  1.54s/it][1,0]<stderr>:#015 40%|████      | 302/750 [07:32<11:30,  1.54s/it][1,0]<stderr>:#015 40%|████      | 303/750 [07:33<11:37,  1.56s/it][1,8]<stderr>:#015 40%|████      | 303/750 [07:33<11:46,  1.58s/it][1,0]<stderr>:#015 41%|████      | 304/750 [07:35<11:29,  1.55s/it][1,8]<stderr>:#015 41%|████      | 304/750 [07:35<11:40,  1.57s/it][1,0]<stderr>:#015 41%|████      | 305/750 [07:36<11:46,  1.59s/it][1,8]<stderr>:#015 41%|████      | 305/750 [07:36<11:56,  1.61s/it][1,8]<stderr>:#015 41%|████      | 306/750 [07:38<11:05,  1.50s/it][1,0]<stderr>:#015 41%|████      | 306/750 [07:38<11:13,  1.52s/it][1,8]<stderr>:#015 41%|████      | 307/750 [07:39<10:46,  1.46s/it][1,0]<stderr>:#015 41%|████      | 307/750 [07:39<11:08,  1.51s/it][1,0]<stderr>:#015 41%|████      | 308/750 [07:41<11:06,  1.51s/it][1,8]<stderr>:#015 41%|████      | 308/750 [07:41<11:22,  1.54s/it][1,8]<stderr>:#015 41%|████      | 309/750 [07:42<11:06,  1.51s/it][1,0]<stderr>:#015 41%|████      | 309/750 [07:42<11:14,  1.53s/it][1,8]<stderr>:#015 41%|████▏     | 310/750 [07:44<10:41,  1.46s/it][1,0]<stderr>:#015 41%|████▏     | 310/750 [07:44<10:39,  1.45s/it][1,8]<stderr>:#015 41%|████▏     | 311/750 [07:45<10:51,  1.48s/it][1,0]<stderr>:#015 41%|████▏     | 311/750 [07:45<10:49,  1.48s/it][1,8]<stderr>:#015 42%|████▏     | 312/750 [07:47<10:45,  1.47s/it][1,0]<stderr>:#015 42%|████▏     | 312/750 [07:47<10:53,  1.49s/it][1,8]<stderr>:#015 42%|████▏     | 313/750 [07:48<10:57,  1.50s/it][1,0]<stderr>:#015 42%|████▏     | 313/750 [07:48<10:54,  1.50s/it][1,8]<stderr>:#015 42%|████▏     | 314/750 [07:49<10:22,  1.43s/it][1,0]<stderr>:#015 42%|████▏     | 314/750 [07:49<10:26,  1.44s/it][1,8]<stderr>:#015 42%|████▏     | 315/750 [07:51<09:43,  1.34s/it][1,0]<stderr>:#015 42%|████▏     | 315/750 [07:51<10:39,  1.47s/it][1,8]<stderr>:#015 42%|████▏     | 316/750 [07:52<10:06,  1.40s/it][1,0]<stderr>:#015 42%|████▏     | 316/750 [07:52<09:51,  1.36s/it][1,8]<stderr>:#015 42%|████▏     | 317/750 [07:53<09:59,  1.38s/it][1,0]<stderr>:#015 42%|████▏     | 317/750 [07:54<09:55,  1.38s/it][1,8]<stderr>:#015 42%|████▏     | 318/750 [07:55<09:56,  1.38s/it][1,0]<stderr>:#015 42%|████▏     | 318/750 [07:55<10:15,  1.42s/it][1,8]<stderr>:#015 43%|████▎     | 319/750 [07:56<10:15,  1.43s/it][1,0]<stderr>:#015 43%|████▎     | 319/750 [07:56<09:55,  1.38s/it][1,8]<stderr>:#015 43%|████▎     | 320/750 [07:57<09:30,  1.33s/it][1,0]<stderr>:#015 43%|████▎     | 320/750 [07:58<09:49,  1.37s/it][1,8]<stderr>:#015 43%|████▎     | 321/750 [07:59<10:20,  1.45s/it][1,0]<stderr>:#015 43%|████▎     | 321/750 [07:59<10:36,  1.48s/it][1,8]<stderr>:#015 43%|████▎     | 322/750 [08:01<11:04,  1.55s/it][1,0]<stderr>:#015 43%|████▎     | 322/750 [08:01<10:44,  1.51s/it][1,8]<stderr>:#015 43%|████▎     | 323/750 [08:02<10:12,  1.43s/it][1,0]<stderr>:#015 43%|████▎     | 323/750 [08:02<10:12,  1.43s/it][1,8]<stderr>:#015 43%|████▎     | 324/750 [08:03<09:43,  1.37s/it][1,0]<stderr>:#015 43%|████▎     | 324/750 [08:04<09:43,  1.37s/it][1,8]<stderr>:#015 43%|████▎     | 325/750 [08:05<09:58,  1.41s/it][1,0]<stderr>:#015 43%|████▎     | 325/750 [08:05<09:52,  1.39s/it][1,8]<stderr>:#015 43%|████▎     | 326/750 [08:06<10:03,  1.42s/it][1,0]<stderr>:#015 43%|████▎     | 326/750 [08:07<10:21,  1.47s/it][1,8]<stderr>:#015 44%|████▎     | 327/750 [08:08<10:19,  1.46s/it][1,0]<stderr>:#015 44%|████▎     | 327/750 [08:08<10:02,  1.42s/it][1,8]<stderr>:#015 44%|████▎     | 328/750 [08:09<10:36,  1.51s/it][1,0]<stderr>:#015 44%|████▎     | 328/750 [08:10<11:03,  1.57s/it][1,0]<stderr>:#015 44%|████▍     | 329/750 [08:11<10:09,  1.45s/it][1,8]<stderr>:#015 44%|████▍     | 329/750 [08:11<10:37,  1.51s/it][1,8]<stderr>:#015 44%|████▍     | 330/750 [08:13<10:49,  1.55s/it][1,0]<stderr>:#015 44%|████▍     | 330/750 [08:13<10:42,  1.53s/it][1,8]<stderr>:#015 44%|████▍     | 331/750 [08:14<10:31,  1.51s/it][1,0]<stderr>:#015 44%|████▍     | 331/750 [08:14<10:16,  1.47s/it][1,8]<stderr>:#015 44%|████▍     | 332/750 [08:15<10:12,  1.46s/it][1,0]<stderr>:#015 44%|████▍     | 332/750 [08:16<10:11,  1.46s/it][1,8]<stderr>:#015 44%|████▍     | 333/750 [08:17<11:21,  1.63s/it][1,0]<stderr>:#015 44%|████▍     | 333/750 [08:18<11:28,  1.65s/it][1,8]<stderr>:#015 45%|████▍     | 334/750 [08:19<11:24,  1.64s/it][1,0]<stderr>:#015 45%|████▍     | 334/750 [08:19<11:10,  1.61s/it][1,0]<stderr>:#015 45%|████▍     | 335/750 [08:20<10:35,  1.53s/it][1,8]<stderr>:#015 45%|████▍     | 335/750 [08:20<10:53,  1.58s/it][1,0]<stderr>:#015 45%|████▍     | 336/750 [08:22<10:30,  1.52s/it][1,8]<stderr>:#015 45%|████▍     | 336/750 [08:22<10:35,  1.54s/it][1,0]<stderr>:#015 45%|████▍     | 337/750 [08:24<10:34,  1.54s/it][1,8]<stderr>:#015 45%|████▍     | 337/750 [08:24<10:58,  1.60s/it][1,0]<stderr>:#015 45%|████▌     | 338/750 [08:25<11:01,  1.61s/it][1,8]<stderr>:#015 45%|████▌     | 338/750 [08:25<11:11,  1.63s/it][1,8]<stderr>:#015 45%|████▌     | 339/750 [08:27<10:33,  1.54s/it][1,0]<stderr>:#015 45%|████▌     | 339/750 [08:27<10:49,  1.58s/it][1,8]<stderr>:#015 45%|████▌     | 340/750 [08:28<10:25,  1.52s/it][1,0]<stderr>:#015 45%|████▌     | 340/750 [08:28<10:39,  1.56s/it][1,8]<stderr>:#015 45%|████▌     | 341/750 [08:30<10:08,  1.49s/it][1,0]<stderr>:#015 45%|████▌     | 341/750 [08:30<10:31,  1.54s/it][1,8]<stderr>:#015 46%|████▌     | 342/750 [08:31<10:23,  1.53s/it][1,0]<stderr>:#015 46%|████▌     | 342/750 [08:31<10:16,  1.51s/it][1,8]<stderr>:#015 46%|████▌     | 343/750 [08:33<10:51,  1.60s/it][1,0]<stderr>:#015 46%|████▌     | 343/750 [08:33<10:57,  1.61s/it][1,8]<stderr>:#015 46%|████▌     | 344/750 [08:34<10:01,  1.48s/it][1,0]<stderr>:#015 46%|████▌     | 344/750 [08:34<09:54,  1.47s/it][1,8]<stderr>:#015 46%|████▌     | 345/750 [08:36<09:57,  1.48s/it][1,0]<stderr>:#015 46%|████▌     | 345/750 [08:36<09:56,  1.47s/it][1,8]<stderr>:#015 46%|████▌     | 346/750 [08:37<09:31,  1.42s/it][1,0]<stderr>:#015 46%|████▌     | 346/750 [08:37<09:39,  1.44s/it][1,8]<stderr>:#015 46%|████▋     | 347/750 [08:38<09:35,  1.43s/it][1,0]<stderr>:#015 46%|████▋     | 347/750 [08:38<09:33,  1.42s/it][1,8]<stderr>:#015 46%|████▋     | 348/750 [08:40<09:07,  1.36s/it][1,0]<stderr>:#015 46%|████▋     | 348/750 [08:40<09:09,  1.37s/it][1,8]<stderr>:#015 47%|████▋     | 349/750 [08:41<09:12,  1.38s/it][1,0]<stderr>:#015 47%|████▋     | 349/750 [08:41<09:16,  1.39s/it][1,0]<stderr>:#015 47%|████▋     | 350/750 [08:42<08:54,  1.34s/it][1,8]<stderr>:#015 47%|████▋     | 350/750 [08:42<09:08,  1.37s/it][1,8]<stderr>:#015 47%|████▋     | 351/750 [08:44<09:17,  1.40s/it][1,0]<stderr>:#015 47%|████▋     | 351/750 [08:44<09:17,  1.40s/it][1,8]<stderr>:#015 47%|████▋     | 352/750 [08:45<09:17,  1.40s/it][1,0]<stderr>:#015 47%|████▋     | 352/750 [08:45<09:15,  1.40s/it][1,8]<stderr>:#015 47%|████▋     | 353/750 [08:47<09:39,  1.46s/it][1,0]<stderr>:#015 47%|████▋     | 353/750 [08:47<09:37,  1.45s/it][1,8]<stderr>:#015 47%|████▋     | 354/750 [08:48<09:51,  1.49s/it][1,0]<stderr>:#015 47%|████▋     | 354/750 [08:49<10:08,  1.54s/it][1,8]<stderr>:#015 47%|████▋     | 355/750 [08:50<10:09,  1.54s/it][1,0]<stderr>:#015 47%|██�\u001b[0m\n",
      "\u001b[34m�█▋     | 355/750 [08:50<10:04,  1.53s/it][1,8]<stderr>:#015 47%|████▋     | 356/750 [08:52<10:36,  1.62s/it][1,0]<stderr>:#015 47%|████▋     | 356/750 [08:52<10:37,  1.62s/it][1,0]<stderr>:#015 48%|████▊     | 357/750 [08:53<09:56,  1.52s/it][1,8]<stderr>:#015 48%|████▊     | 357/750 [08:53<10:04,  1.54s/it][1,8]<stderr>:#015 48%|████▊     | 358/750 [08:55<09:44,  1.49s/it][1,0]<stderr>:#015 48%|████▊     | 358/750 [08:55<09:59,  1.53s/it][1,8]<stderr>:#015 48%|████▊     | 359/750 [08:56<09:33,  1.47s/it][1,0]<stderr>:#015 48%|████▊     | 359/750 [08:56<09:30,  1.46s/it][1,8]<stderr>:#015 48%|████▊     | 360/750 [08:58<10:12,  1.57s/it][1,0]<stderr>:#015 48%|████▊     | 360/750 [08:58<10:06,  1.56s/it][1,8]<stderr>:#015 48%|████▊     | 361/750 [09:00<10:34,  1.63s/it][1,0]<stderr>:#015 48%|████▊     | 361/750 [09:00<10:33,  1.63s/it][1,0]<stderr>:#015 48%|████▊     | 362/750 [09:01<09:52,  1.53s/it][1,8]<stderr>:#015 48%|████▊     | 362/750 [09:01<10:06,  1.56s/it][1,8]<stderr>:#015 48%|████▊     | 363/750 [09:02<09:11,  1.42s/it][1,0]<stderr>:#015 48%|████▊     | 363/750 [09:02<09:08,  1.42s/it][1,0]<stderr>:#015 49%|████▊     | 364/750 [09:03<08:41,  1.35s/it][1,8]<stderr>:#015 49%|████▊     | 364/750 [09:03<08:46,  1.36s/it][1,8]<stderr>:#015 49%|████▊     | 365/750 [09:05<08:30,  1.33s/it][1,0]<stderr>:#015 49%|████▊     | 365/750 [09:05<08:32,  1.33s/it][1,0]<stderr>:#015 49%|████▉     | 366/750 [09:06<08:33,  1.34s/it][1,8]<stderr>:#015 49%|████▉     | 366/750 [09:06<08:45,  1.37s/it][1,8]<stderr>:#015 49%|████▉     | 367/750 [09:07<08:16,  1.30s/it][1,0]<stderr>:#015 49%|████▉     | 367/750 [09:07<08:19,  1.31s/it][1,8]<stderr>:#015 49%|████▉     | 368/750 [09:09<08:32,  1.34s/it][1,0]<stderr>:#015 49%|████▉     | 368/750 [09:09<08:50,  1.39s/it][1,8]<stderr>:#015 49%|████▉     | 369/750 [09:10<08:41,  1.37s/it][1,0]<stderr>:#015 49%|████▉     | 369/750 [09:10<08:42,  1.37s/it][1,8]<stderr>:#015 49%|████▉     | 370/750 [09:11<08:40,  1.37s/it][1,0]<stderr>:#015 49%|████▉     | 370/750 [09:11<08:42,  1.38s/it][1,0]<stderr>:#015 49%|████▉     | 371/750 [09:13<08:29,  1.34s/it][1,8]<stderr>:#015 49%|████▉     | 371/750 [09:13<08:46,  1.39s/it][1,8]<stderr>:#015 50%|████▉     | 372/750 [09:14<08:33,  1.36s/it][1,0]<stderr>:#015 50%|████▉     | 372/750 [09:14<08:43,  1.39s/it][1,0]<stderr>:#015 50%|████▉     | 373/750 [09:15<08:09,  1.30s/it][1,8]<stderr>:#015 50%|████▉     | 373/750 [09:15<08:13,  1.31s/it][1,8]<stderr>:#015 50%|████▉     | 374/750 [09:17<08:38,  1.38s/it][1,0]<stderr>:#015 50%|████▉     | 374/750 [09:17<08:53,  1.42s/it][1,0]<stderr>:#015 50%|█████     | 375/750 [09:18<08:42,  1.39s/it][1,8]<stderr>:#015 50%|█████     | 375/750 [09:18<08:57,  1.43s/it][1,8]<stderr>:#015 50%|█████     | 376/750 [09:20<08:39,  1.39s/it][1,0]<stderr>:#015 50%|█████     | 376/750 [09:20<08:44,  1.40s/it][1,8]<stderr>:#015 50%|█████     | 377/750 [09:21<09:19,  1.50s/it][1,0]<stderr>:#015 50%|█████     | 377/750 [09:22<09:27,  1.52s/it][1,8]<stderr>:#015 50%|█████     | 378/750 [09:24<10:22,  1.67s/it][1,0]<stderr>:#015 50%|█████     | 378/750 [09:24<10:17,  1.66s/it][1,8]<stderr>:#015 51%|█████     | 379/750 [09:25<09:34,  1.55s/it][1,0]<stderr>:#015 51%|█████     | 379/750 [09:25<09:49,  1.59s/it][1,0]<stderr>:#015 51%|█████     | 380/750 [09:26<09:00,  1.46s/it][1,8]<stderr>:#015 51%|█████     | 380/750 [09:26<09:27,  1.53s/it][1,8]<stderr>:#015 51%|█████     | 381/750 [09:28<09:16,  1.51s/it][1,0]<stderr>:#015 51%|█████     | 381/750 [09:28<09:20,  1.52s/it][1,0]<stderr>:#015 51%|█████     | 382/750 [09:29<09:06,  1.49s/it][1,8]<stderr>:#015 51%|█████     | 382/750 [09:29<09:10,  1.50s/it][1,0]<stderr>:#015 51%|█████     | 383/750 [09:31<09:14,  1.51s/it][1,8]<stderr>:#015 51%|█████     | 383/750 [09:31<09:17,  1.52s/it][1,0]<stderr>:#015 51%|█████     | 384/750 [09:32<09:28,  1.55s/it][1,8]<stderr>:#015 51%|█████     | 384/750 [09:33<09:40,  1.59s/it][1,0]<stderr>:#015 51%|█████▏    | 385/750 [09:34<09:50,  1.62s/it][1,8]<stderr>:#015 51%|█████▏    | 385/750 [09:34<09:48,  1.61s/it][1,0]<stderr>:#015 51%|█████▏    | 386/750 [09:36<09:30,  1.57s/it][1,8]<stderr>:#015 51%|█████▏    | 386/750 [09:36<09:29,  1.57s/it][1,0]<stderr>:#015 52%|█████▏    | 387/750 [09:37<09:12,  1.52s/it][1,8]<stderr>:#015 52%|█████▏    | 387/750 [09:37<09:11,  1.52s/it][1,0]<stderr>:#015 52%|█████▏    | 388/750 [09:38<08:48,  1.46s/it][1,8]<stderr>:#015 52%|█████▏    | 388/750 [09:38<08:53,  1.47s/it][1,8]<stderr>:#015 52%|█████▏    | 389/750 [09:40<09:07,  1.52s/it][1,0]<stderr>:#015 52%|█████▏    | 389/750 [09:40<09:23,  1.56s/it][1,0]<stderr>:#015 52%|█████▏    | 390/750 [09:41<08:33,  1.43s/it][1,8]<stderr>:#015 52%|█████▏    | 390/750 [09:41<08:51,  1.48s/it][1,0]<stderr>:#015 52%|█████▏    | 391/750 [09:43<08:15,  1.38s/it][1,8]<stderr>:#015 52%|█████▏    | 391/750 [09:43<08:16,  1.38s/it][1,0]<stderr>:#015 52%|█████▏    | 392/750 [09:44<08:12,  1.38s/it][1,8]<stderr>:#015 52%|█████▏    | 392/750 [09:44<08:12,  1.38s/it][1,8]<stderr>:#015 52%|█████▏    | 393/750 [09:45<07:57,  1.34s/it][1,0]<stderr>:#015 52%|█████▏    | 393/750 [09:45<08:08,  1.37s/it][1,0]<stderr>:#015 53%|█████▎    | 394/750 [09:47<08:05,  1.36s/it][1,8]<stderr>:#015 53%|█████▎    | 394/750 [09:47<08:05,  1.36s/it][1,8]<stderr>:#015 53%|█████▎    | 395/750 [09:48<08:23,  1.42s/it][1,0]<stderr>:#015 53%|█████▎    | 395/750 [09:48<08:34,  1.45s/it][1,8]<stderr>:#015 53%|█████▎    | 396/750 [09:50<08:29,  1.44s/it][1,0]<stderr>:#015 53%|█████▎    | 396/750 [09:50<08:33,  1.45s/it][1,0]<stderr>:#015 53%|█████▎    | 397/750 [09:51<08:15,  1.40s/it][1,8]<stderr>:#015 53%|█████▎    | 397/750 [09:51<08:25,  1.43s/it][1,8]<stderr>:#015 53%|█████▎    | 398/750 [09:52<08:22,  1.43s/it][1,0]<stderr>:#015 53%|█████▎    | 398/750 [09:53<08:25,  1.44s/it][1,0]<stderr>:#015 53%|█████▎    | 399/750 [09:54<08:28,  1.45s/it][1,8]<stderr>:#015 53%|█████▎    | 399/750 [09:54<08:33,  1.46s/it][1,0]<stderr>:#015 53%|█████▎    | 400/750 [09:56<08:28,  1.45s/it][1,8]<stderr>:#015 53%|█████▎    | 400/750 [09:55<08:33,  1.47s/it][1,0]<stderr>:#015 53%|█████▎    | 401/750 [09:57<08:36,  1.48s/it][1,8]<stderr>:#015 53%|█████▎    | 401/750 [09:57<08:52,  1.53s/it][1,0]<stderr>:#015 54%|█████▎    | 402/750 [09:59<09:09,  1.58s/it][1,8]<stderr>:#015 54%|█████▎    | 402/750 [09:59<09:06,  1.57s/it][1,8]<stderr>:#015 54%|█████▎    | 403/750 [10:00<08:47,  1.52s/it][1,0]<stderr>:#015 54%|█████▎    | 403/750 [10:00<08:51,  1.53s/it][1,0]<stderr>:#015 54%|█████▍    | 404/750 [10:02<08:22,  1.45s/it][1,8]<stderr>:#015 54%|█████▍    | 404/750 [10:02<08:29,  1.47s/it][1,8]<stderr>:#015 54%|█████▍    | 405/750 [10:03<07:43,  1.34s/it][1,0]<stderr>:#015 54%|█████▍    | 405/750 [10:03<07:48,  1.36s/it][1,8]<stderr>:#015 54%|█████▍    | 406/750 [10:04<07:43,  1.35s/it][1,0]<stderr>:#015 54%|█████▍    | 406/750 [10:04<07:56,  1.38s/it][1,0]<stderr>:#015 54%|█████▍    | 407/750 [10:05<07:44,  1.35s/it][1,8]<stderr>:#015 54%|█████▍    | 407/750 [10:05<07:51,  1.38s/it][1,0]<stderr>:#015 54%|█████▍    | 408/750 [10:07<07:54,  1.39s/it][1,8]<stderr>:#015 54%|█████▍    | 408/750 [10:07<07:55,  1.39s/it][1,8]<stderr>:#015 55%|█████▍    | 409/750 [10:08<08:07,  1.43s/it][1,0]<stderr>:#015 55%|█████▍    | 409/750 [10:08<08:12,  1.44s/it][1,8]<stderr>:#015 55%|█████▍    | 410/750 [10:10<07:58,  1.41s/it][1,0]<stderr>:#015 55%|█████▍    | 410/750 [10:10<08:07,  1.43s/it][1,8]<stderr>:#015 55%|█████▍    | 411/750 [10:11<08:28,  1.50s/it][1,0]<stderr>:#015 55%|█████▍    | 411/750 [10:12<08:48,  1.56s/it][1,8]<stderr>:#015 55%|█████▍    | 412/750 [10:13<08:09,  1.45s/it][1,0]<stderr>:#015 55%|█████▍    | 412/750 [10:13<08:01,  1.43s/it][1,8]<stderr>:#015 55%|█████▌    | 413/750 [10:14<08:23,  1.49s/it][1,0]<stderr>:#015 55%|█████▌    | 413/750 [10:15<08:28,  1.51s/it][1,0]<stderr>:#015 55%|█████▌    | 414/750 [10:16<08:09,  1.46s/it][1,8]<stderr>:#015 55%|█████▌    | 414/750 [10:16<08:29,  1.52s/it][1,8]<stderr>:#015 55%|█████▌    | 415/750 [10:17<08:25,  1.51s/it][1,0]<stderr>:#015 55%|█████▌    | 415/750 [10:18<08:28,  1.52s/it][1,0]<stderr>:#015 55%|█████▌    | 416/750 [10:19<08:26,  1.52s/it][1,8]<stderr>:#015 55%|█████▌    | 416/750 [10:19<08:34,  1.54s/it][1,0]<stderr>:#015 56%|█████▌    | 417/750 [10:21<08:25,  1.52s/it][1,8]<stderr>:#015 56%|█████▌    | 417/750 [10:21<08:30,  1.53s/it][1,8]<stderr>:#015 56%|█████▌    | 418/750 [10:22<08:15,  1.49s/it][1,0]<stderr>:#015 56%|█████▌    | 418/750 [10:22<08:19,  1.50s/it][1,0]<stderr>:#015 56%|█████▌    | 419/750 [10:24<08:30,  1.54s/it][1,8]<stderr>:#015 56%|█████▌    | 419/750 [10:24<08:33,  1.55s/it][1,8]<stderr>:#015 56%|█████▌    | 420/750 [10:25<08:02,  1.46s/it][1,0]<stderr>:#015 56%|█████▌    | 420/750 [10:25<08:08,  1.48s/it][1,8]<stderr>:#015 56%|█████▌    | 421/750 [10:26<08:08,  1.48s/it][1,0]<stderr>:#015 56%|█████▌    | 421/750 [10:27<08:12,  1.50s/it][1,8]<stderr>:#015 56%|█████▋    | 422/750 [10:28<08:10,  1.49s/it][1,0]<stderr>:#015 56%|█████▋    | 422/750 [10:28<08:18,  1.52s/it][1,8]<stderr>:#015 56%|█████▋    | 423/750 [10:30<08:30,  1.56s/it][1,0]<stderr>:#015 56%|█████▋    | 423/750 [10:30<08:29,  1.56s/it][1,8]<stderr>:#015 57%|█████▋    | 424/750 [10:31<08:33,  1.58s/it][1,0]<stderr>:#015 57%|█████▋    | 424/750 [10:32<08:54,  1.64s/it][1,8]<stderr>:#015 57%|█████▋    | 425/750 [10:33<08:17,  1.53s/it][1,0]<stderr>:#015 57%|█████▋    | 425/750 [10:33<08:17,  1.53s/it][1,0]<stderr>:#015 57%|█████▋    | 426/750 [10:34<08:15,  1.53s/it][1,8]<stderr>:#015 57%|█████▋    | 426/750 [10:34<08:26,  1.56s/it][1,8]<stderr>:#015 57%|█████▋    | 427/750 [10:36<08:19,  1.55s/it][1,0]<stderr>:#015 57%|█████▋    | 427/750 [10:36<08:22,  1.55s/it][1,8]<stderr>:#015 57%|█████▋    | 428/750 [10:37<08:05,  1.51s/it][1,0]<stderr>:#015 57%|█████▋    | 428/750 [10:37<08:03,  1.50s/it][1,0]<stderr>:#015 57%|█████▋    | 429/750 [10:39<08:02,  1.50s/it][1,8]<stderr>:#015 57%|█████▋    | 429/750 [10:39<08:10,  1.53s/it][1,8]<stderr>:#015 57%|█████▋    | 430/750 [10:40<07:45,  1.45s/it][1,0]<stderr>:#015 57%|█████▋    | 430/750 [10:40<07:52,  1.48s/it][1,0]<stderr>:#015 57%|█████▋    | 431/750 [10:41<07:23,  1.39s/it][1,8]<stderr>:#015 57%|█████▋    | 431/750 [10:42<07:36,  1.43s/it][1,0]<stderr>:#015 58%|█████▊    | 432/750 [10:43<07:25,  1.40s/it][1,8]<stderr>:#015 58%|█████▊    | 432/750 [10:43<07:29,  1.41s/it][1,8]<stderr>:#015 58%|█████▊    | 433/750 [10:44<07:35,  1.44s/it][1,0]<stderr>:#015 58%|█████▊    | 433/750 [10:44<07:39,  1.45s/it][1,8]<stderr>:#015 58%|█████▊    | 434/750 [10:46<08:08,  1.55s/it][1,0]<stderr>:#015 58%|█████▊    | 434/750 [10:46<08:26,  1.60s/it][1,0]<stderr>:#015 58%|█████▊    | 435/750 [10:47<07:32,  1.44s/it][1,8]<stderr>:#015 58%|█████▊    | 435/750 [10:47<07:42,  1.47s/it][1,8]<stderr>:#015 58%|█████▊    | 436/750 [10:49<08:00,  1.53s/it][1,0]<stderr>:#015 58%|█████▊    | 436/750 [10:49<07:58,  1.52s/it][1,8]<stderr>:#015 58%|█████▊    | 437/750 [10:51<07:49,  1.50s/it][1,0]<stderr>:#015 58%|█████▊    | 437/750 [10:51<07:52,  1.51s/it][1,8]<stderr>:#015 58%|█████▊    | 438/750 [10:52<07:50,  1.51s/it][1,0]<stderr>:#015 58%|█████▊    | 438/750 [10:52<07:51,  1.51s/it][1,8]<stderr>:#015 59%|█████▊    | 439/750 [10:54<07:41,  1.48s/it][1,0]<stderr>:#015 59%|█████▊    | 439/750 [10:54<07:36,  1.47s/it][1,0]<stderr>:#015 59%|█████▊    | 440/750 [10:55<07:23,  1.43s/it][1,8]<stderr>:#015 59%|█████▊    | 440/750 [10:55<07:27,  1.44s/it][1,0]<stderr>:#015 59%|█████▉    | 441/750 [10:56<07:19,  1.42s/it][1,8]<stderr>:#015 59%|█████▉    | 441/750 [10:56<07:27,  1.45s/it][1,8]<stderr>:#015 59%|█████▉    | 442/750 [10:58<07:23,  1.44s/it][1,0]<stderr>:#015 59%|█████▉    | 442/750 [10:58<07:26,  1.45s/it][1,0]<stderr>:#015 59%|█████▉    | 443/750 [10:59<07:05,  1.39s/it][1,8]<stderr>:#015 59%|█████▉    | 443/750 [10:59<07:11,  1.41s/it][1,0]<stderr>:#015 59%|█████▉    | 444/750 [11:00<06:56,  1.36s/it][1,8]<stderr>:#015 59%|█████▉    | 444/750 [11:00<07:08,  1.40s/it][1,0]<stderr>:#015 59%|█████▉    | 445/750 [11:02<06:49,  1.34s/it][1,8]<stderr>:#015 59%|█████▉    | 445/750 [11:02<07:07,  1.40s/it][1,8]<stderr>:#015 59%|█████▉    | 446/750 [11:03<06:41,  1.32s/it][1,0]<stderr>:#015 59%|█████▉    | 446/750 [11:03<06:51,  1.35s/it][1,0]<stderr>:#015 60%|█████▉    | 447/750 [11:04<06:28,  1.28s/it][1,8]<stderr>:#015 60%|█████▉    | 447/750 [11:04<06:26,  1.28s/it][1,8]<stderr>:#015 60%|█████▉    | 448/750 [11:05<06:20,  1.26s/it][1,0]<stderr>:#015 60%|█████▉    | 448/750 [11:05<06:27,  1.28s/it][1,0]<stderr>:#015 60%|█████▉    | 449/750 [11:07<06:47,  1.36s/it][1,8]<stderr>:#015 60%|█████▉    | 449/750 [11:07<06:46,  1.35s/it][1,0]<stderr>:#015 60%|██████    | 450/750 [11:08<06:59,  1.40s/it][1,8]<stderr>:#015 60%|██████    | 450/750 [11:09<07:03,  1.41s/it][1,0]<stderr>:#015 60%|██████    | 451/750 [11:10<07:19,  1.47s/it][1,8]<stderr>:#015 60%|██████    | 451/750 [11:10<07:23,  1.48s/it][1,8]<stderr>:#015 60%|██████    | 452/750 [11:11<06:58,  1.40s/it][1,0]<stderr>:#015 60%|██████    | 452/750 [11:11<07:09,  1.44s/it][1,8]<stderr>:#015 60%|██████    | 453/750 [11:13<06:55,  1.40s/it][1,0]<stderr>:#015 60%|██████    | 453/750 [11:13<07:03,  1.43s/it][1,8]<stderr>:#015 61%|██████    | 454/750 [11:14<06:42,  1.36s/it][1,0]<stderr>:#015 61%|██████    | 454/750 [11:14<06:53,  1.40s/it][1,0]<stderr>:#015 61%|██████    | 455/750 [11:16<06:45,  1.38s/it][1,8]<stderr>:#015 61%|██████    | 455/750 [11:16<07:02,  1.43s/it][1,0]<stderr>:#015 61%|██████    | 456/750 [11:17<07:05,  1.45s/it][1,8]<stderr>:#015 61%|██████    | 456/750 [11:17<07:05,  1.45s/it][1,0]<stderr>:#015 61%|██████    | 457/750 [11:18<06:38,  1.36s/it][1,8]<stderr>:#015 61%|██████    | 457/750 [11:18<06:44,  1.38s/it][1,0]<stderr>:#015 61%|██████    | 458/750 [11:20<06:40,  1.37s/it][1,8]<stderr>:#015 61%|██████    | 458/750 [11:20<06:54,  1.42s/it][1,0]<stderr>:#015 61%|██████    | 459/750 [11:21<06:43,  1.39s/it][1,8]<stderr>:#015 61%|██████    | 459/750 [11:21<06:39,  1.37s/it][1,0]<stderr>:#015 61%|██████▏   | 460/750 [11:23<06:41,  1.39s/it][1,8]<stderr>:#015 61%|██████▏   | 460/750 [11:23<06:48,  1.41s/it][1,0]<stderr>:#015 61%|██████▏   | 461/750 [11:24<07:09,  1.49s/it][1,8]<stderr>:#015 61%|██████▏   | 461/750 [11:24<07:07,  1.48s/it][1,0]<stderr>:#015 62%|██████▏   | 462/750 [11:26<07:18,  1.52s/it][1,8]<stderr>:#015 62%|██████▏   | 462/750 [11:26<07:11,  1.50s/it][1,0]<stderr>:#015 62%|██████▏   | 463/750 [11:27<07:00,  1.46s/it][1,8]<stderr>:#015 62%|██████▏   | 463/750 [11:27<06:59,  1.46s/it][1,8]<stderr>:#015 62%|�\u001b[0m\n",
      "\u001b[34m�█████▏   | 464/750 [11:28<06:45,  1.42s/it][1,0]<stderr>:#015 62%|██████▏   | 464/750 [11:29<06:54,  1.45s/it][1,0]<stderr>:#015 62%|██████▏   | 465/750 [11:30<07:02,  1.48s/it][1,8]<stderr>:#015 62%|██████▏   | 465/750 [11:30<07:05,  1.49s/it][1,8]<stderr>:#015 62%|██████▏   | 466/750 [11:32<07:10,  1.52s/it][1,0]<stderr>:#015 62%|██████▏   | 466/750 [11:32<07:20,  1.55s/it][1,0]<stderr>:#015 62%|██████▏   | 467/750 [11:33<06:47,  1.44s/it][1,8]<stderr>:#015 62%|██████▏   | 467/750 [11:33<07:00,  1.48s/it][1,0]<stderr>:#015 62%|██████▏   | 468/750 [11:35<07:16,  1.55s/it][1,8]<stderr>:#015 62%|██████▏   | 468/750 [11:35<07:25,  1.58s/it][1,8]<stderr>:#015 63%|██████▎   | 469/750 [11:36<06:58,  1.49s/it][1,0]<stderr>:#015 63%|██████▎   | 469/750 [11:36<07:10,  1.53s/it][1,8]<stderr>:#015 63%|██████▎   | 470/750 [11:38<06:44,  1.44s/it][1,0]<stderr>:#015 63%|██████▎   | 470/750 [11:38<06:52,  1.47s/it][1,8]<stderr>:#015 63%|██████▎   | 471/750 [11:39<07:04,  1.52s/it][1,0]<stderr>:#015 63%|██████▎   | 471/750 [11:40<07:24,  1.59s/it][1,0]<stderr>:#015 63%|██████▎   | 472/750 [11:41<07:04,  1.53s/it][1,8]<stderr>:#015 63%|██████▎   | 472/750 [11:41<07:10,  1.55s/it][1,8]<stderr>:#015 63%|██████▎   | 473/750 [11:42<07:13,  1.57s/it][1,0]<stderr>:#015 63%|██████▎   | 473/750 [11:43<07:19,  1.59s/it][1,8]<stderr>:#015 63%|██████▎   | 474/750 [11:44<07:06,  1.55s/it][1,0]<stderr>:#015 63%|██████▎   | 474/750 [11:44<07:02,  1.53s/it][1,8]<stderr>:#015 63%|██████▎   | 475/750 [11:45<06:46,  1.48s/it][1,0]<stderr>:#015 63%|██████▎   | 475/750 [11:45<06:43,  1.47s/it][1,0]<stderr>:#015 63%|██████▎   | 476/750 [11:47<06:35,  1.44s/it][1,8]<stderr>:#015 63%|██████▎   | 476/750 [11:47<06:54,  1.51s/it][1,8]<stderr>:#015 64%|██████▎   | 477/750 [11:48<06:45,  1.48s/it][1,0]<stderr>:#015 64%|██████▎   | 477/750 [11:48<06:52,  1.51s/it][1,0]<stderr>:#015 64%|██████▎   | 478/750 [11:50<06:52,  1.52s/it][1,8]<stderr>:#015 64%|██████▎   | 478/750 [11:50<06:53,  1.52s/it][1,8]<stderr>:#015 64%|██████▍   | 479/750 [11:51<06:41,  1.48s/it][1,0]<stderr>:#015 64%|██████▍   | 479/750 [11:51<06:44,  1.49s/it][1,8]<stderr>:#015 64%|██████▍   | 480/750 [11:53<06:53,  1.53s/it][1,0]<stderr>:#015 64%|██████▍   | 480/750 [11:53<07:03,  1.57s/it][1,0]<stderr>:#015 64%|██████▍   | 481/750 [11:54<06:21,  1.42s/it][1,8]<stderr>:#015 64%|██████▍   | 481/750 [11:54<06:27,  1.44s/it][1,0]<stderr>:#015 64%|██████▍   | 482/750 [11:56<06:15,  1.40s/it][1,8]<stderr>:#015 64%|██████▍   | 482/750 [11:56<06:22,  1.43s/it][1,0]<stderr>:#015 64%|██████▍   | 483/750 [11:57<06:49,  1.53s/it][1,8]<stderr>:#015 64%|██████▍   | 483/750 [11:57<06:49,  1.53s/it][1,0]<stderr>:#015 65%|██████▍   | 484/750 [11:59<06:46,  1.53s/it][1,8]<stderr>:#015 65%|██████▍   | 484/750 [11:59<06:46,  1.53s/it][1,8]<stderr>:#015 65%|██████▍   | 485/750 [12:00<06:41,  1.51s/it][1,0]<stderr>:#015 65%|██████▍   | 485/750 [12:00<06:46,  1.54s/it][1,8]<stderr>:#015 65%|██████▍   | 486/750 [12:02<06:41,  1.52s/it][1,0]<stderr>:#015 65%|██████▍   | 486/750 [12:02<06:44,  1.53s/it][1,0]<stderr>:#015 65%|██████▍   | 487/750 [12:03<06:20,  1.45s/it][1,8]<stderr>:#015 65%|██████▍   | 487/750 [12:03<06:25,  1.47s/it][1,0]<stderr>:#015 65%|██████▌   | 488/750 [12:04<06:00,  1.38s/it][1,8]<stderr>:#015 65%|██████▌   | 488/750 [12:04<06:02,  1.38s/it][1,8]<stderr>:#015 65%|██████▌   | 489/750 [12:06<05:52,  1.35s/it][1,0]<stderr>:#015 65%|██████▌   | 489/750 [12:06<05:59,  1.38s/it][1,8]<stderr>:#015 65%|██████▌   | 490/750 [12:08<07:42,  1.78s/it][1,0]<stderr>:#015 65%|██████▌   | 490/750 [12:09<07:43,  1.78s/it][1,0]<stderr>:#015 65%|██████▌   | 491/750 [12:11<09:05,  2.11s/it][1,8]<stderr>:#015 65%|██████▌   | 491/750 [12:11<09:13,  2.14s/it][1,8]<stderr>:#015 66%|██████▌   | 492/750 [12:13<08:48,  2.05s/it][1,0]<stderr>:#015 66%|██████▌   | 492/750 [12:13<08:50,  2.05s/it][1,8]<stderr>:#015 66%|██████▌   | 493/750 [12:16<09:14,  2.16s/it][1,0]<stderr>:#015 66%|██████▌   | 493/750 [12:16<09:14,  2.16s/it][1,8]<stderr>:#015 66%|██████▌   | 494/750 [12:18<08:47,  2.06s/it][1,0]<stderr>:#015 66%|██████▌   | 494/750 [12:18<08:48,  2.06s/it][1,8]<stderr>:#015 66%|██████▌   | 495/750 [12:20<08:49,  2.08s/it][1,0]<stderr>:#015 66%|██████▌   | 495/750 [12:20<08:54,  2.10s/it][1,0]<stderr>:#015 66%|██████▌   | 496/750 [12:21<08:20,  1.97s/it][1,8]<stderr>:#015 66%|██████▌   | 496/750 [12:21<08:23,  1.98s/it][1,8]<stderr>:#015 66%|██████▋   | 497/750 [12:23<07:20,  1.74s/it][1,0]<stderr>:#015 66%|██████▋   | 497/750 [12:23<07:22,  1.75s/it][1,0]<stderr>:#015 66%|██████▋   | 498/750 [12:24<06:56,  1.65s/it][1,8]<stderr>:#015 66%|██████▋   | 498/750 [12:24<06:58,  1.66s/it][1,8]<stderr>:#015 67%|██████▋   | 499/750 [12:26<06:44,  1.61s/it][1,0]<stderr>:#015 67%|██████▋   | 499/750 [12:26<06:49,  1.63s/it][1,8]<stderr>:#015 67%|██████▋   | 500/750 [12:27<06:43,  1.61s/it][1,8]<stderr>:#015                                                 [1,8]<stderr>:#015[1,8]<stderr>:#015 67%|██████▋   | 500/750 [12:27<06:43,  1.61s/it][1,0]<stderr>:#015 67%|██████▋   | 500/750 [12:27<06:41,  1.61s/it][1,0]<stderr>:#015                                                 [1,0]<stderr>:#015[1,0]<stderr>:#015 67%|██████▋   | 500/750 [12:27<06:41,  1.61s/it][1,0]<stderr>:Saving model checkpoint to /opt/ml/model/checkpoint-500\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:Configuration saved in /opt/ml/model/checkpoint-500/config.json\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:Model weights saved in /opt/ml/model/checkpoint-500/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:tokenizer config file saved in /opt/ml/model/checkpoint-500/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:Special tokens file saved in /opt/ml/model/checkpoint-500/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015 67%|██████▋   | 501/750 [12:36<16:06,  3.88s/it][1,8]<stderr>:#015 67%|██████▋   | 501/750 [12:37<16:17,  3.93s/it][1,8]<stderr>:#015 67%|██████▋   | 502/750 [12:38<13:06,  3.17s/it][1,0]<stderr>:#015 67%|██████▋   | 502/750 [12:38<13:11,  3.19s/it][1,8]<stderr>:#015 67%|██████▋   | 503/750 [12:39<10:42,  2.60s/it][1,0]<stderr>:#015 67%|██████▋   | 503/750 [12:39<10:56,  2.66s/it][1,8]<stderr>:#015 67%|██████▋   | 504/750 [12:41<09:22,  2.29s/it][1,0]<stderr>:#015 67%|██████▋   | 504/750 [12:41<09:20,  2.28s/it][1,8]<stderr>:#015 67%|██████▋   | 505/750 [12:42<08:38,  2.12s/it][1,0]<stderr>:#015 67%|██████▋   | 505/750 [12:43<08:39,  2.12s/it][1,8]<stderr>:#015 67%|██████▋   | 506/750 [12:44<07:33,  1.86s/it][1,0]<stderr>:#015 67%|██████▋   | 506/750 [12:44<07:30,  1.85s/it][1,0]<stderr>:#015 68%|██████▊   | 507/750 [12:45<06:44,  1.67s/it][1,8]<stderr>:#015 68%|██████▊   | 507/750 [12:45<06:48,  1.68s/it][1,0]<stderr>:#015 68%|██████▊   | 508/750 [12:47<06:38,  1.65s/it][1,8]<stderr>:#015 68%|██████▊   | 508/750 [12:47<06:46,  1.68s/it][1,0]<stderr>:#015 68%|██████▊   | 509/750 [12:48<06:19,  1.58s/it][1,8]<stderr>:#015 68%|██████▊   | 509/750 [12:48<06:22,  1.59s/it][1,0]<stderr>:#015 68%|██████▊   | 510/750 [12:49<05:56,  1.49s/it][1,8]<stderr>:#015 68%|██████▊   | 510/750 [12:49<05:55,  1.48s/it][1,8]<stderr>:#015 68%|██████▊   | 511/750 [12:51<05:54,  1.48s/it][1,0]<stderr>:#015 68%|██████▊   | 511/750 [12:51<06:04,  1.53s/it][1,8]<stderr>:#015 68%|██████▊   | 512/750 [12:52<06:00,  1.52s/it][1,0]<stderr>:#015 68%|██████▊   | 512/750 [12:52<06:02,  1.52s/it][1,0]<stderr>:#015 68%|██████▊   | 513/750 [12:54<06:01,  1.53s/it][1,8]<stderr>:#015 68%|██████▊   | 513/750 [12:54<06:05,  1.54s/it][1,0]<stderr>:#015 69%|██████▊   | 514/750 [12:55<05:44,  1.46s/it][1,8]<stderr>:#015 69%|██████▊   | 514/750 [12:55<05:48,  1.48s/it][1,8]<stderr>:#015 69%|██████▊   | 515/750 [12:57<05:39,  1.45s/it][1,0]<stderr>:#015 69%|██████▊   | 515/750 [12:57<05:44,  1.47s/it][1,8]<stderr>:#015 69%|██████▉   | 516/750 [12:58<05:38,  1.45s/it][1,0]<stderr>:#015 69%|██████▉   | 516/750 [12:58<05:42,  1.47s/it][1,0]<stderr>:#015 69%|██████▉   | 517/750 [13:00<05:37,  1.45s/it][1,8]<stderr>:#015 69%|██████▉   | 517/750 [13:00<05:51,  1.51s/it][1,8]<stderr>:#015 69%|██████▉   | 518/750 [13:01<05:45,  1.49s/it][1,0]<stderr>:#015 69%|██████▉   | 518/750 [13:01<05:50,  1.51s/it][1,0]<stderr>:#015 69%|██████▉   | 519/750 [13:03<05:43,  1.49s/it][1,8]<stderr>:#015 69%|██████▉   | 519/750 [13:03<05:50,  1.52s/it][1,0]<stderr>:#015 69%|██████▉   | 520/750 [13:04<05:08,  1.34s/it][1,8]<stderr>:#015 69%|██████▉   | 520/750 [13:04<05:14,  1.37s/it][1,8]<stderr>:#015 69%|██████▉   | 521/750 [13:05<05:05,  1.33s/it][1,0]<stderr>:#015 69%|██████▉   | 521/750 [13:05<05:13,  1.37s/it][1,0]<stderr>:#015 70%|██████▉   | 522/750 [13:07<05:14,  1.38s/it][1,8]<stderr>:#015 70%|██████▉   | 522/750 [13:07<05:23,  1.42s/it][1,0]<stderr>:#015 70%|██████▉   | 523/750 [13:08<05:20,  1.41s/it][1,8]<stderr>:#015 70%|██████▉   | 523/750 [13:08<05:24,  1.43s/it][1,8]<stderr>:#015 70%|██████▉   | 524/750 [13:10<05:25,  1.44s/it][1,0]<stderr>:#015 70%|██████▉   | 524/750 [13:10<05:40,  1.51s/it][1,0]<stderr>:#015 70%|███████   | 525/750 [13:11<05:17,  1.41s/it][1,8]<stderr>:#015 70%|███████   | 525/750 [13:11<05:18,  1.42s/it][1,8]<stderr>:#015 70%|███████   | 526/750 [13:12<05:22,  1.44s/it][1,0]<stderr>:#015 70%|███████   | 526/750 [13:13<05:25,  1.45s/it][1,0]<stderr>:#015 70%|███████   | 527/750 [13:14<05:21,  1.44s/it][1,8]<stderr>:#015 70%|███████   | 527/750 [13:14<05:21,  1.44s/it][1,8]<stderr>:#015 70%|███████   | 528/750 [13:15<05:05,  1.37s/it][1,0]<stderr>:#015 70%|███████   | 528/750 [13:15<05:14,  1.42s/it][1,8]<stderr>:#015 71%|███████   | 529/750 [13:17<05:13,  1.42s/it][1,0]<stderr>:#015 71%|███████   | 529/750 [13:17<05:19,  1.45s/it][1,0]<stderr>:#015 71%|███████   | 530/750 [13:18<05:06,  1.39s/it][1,8]<stderr>:#015 71%|███████   | 530/750 [13:18<05:22,  1.47s/it][1,8]<stderr>:#015 71%|███████   | 531/750 [13:20<05:09,  1.42s/it][1,0]<stderr>:#015 71%|███████   | 531/750 [13:20<05:18,  1.45s/it][1,8]<stderr>:#015 71%|███████   | 532/750 [13:21<05:17,  1.45s/it][1,0]<stderr>:#015 71%|███████   | 532/750 [13:21<05:18,  1.46s/it][1,0]<stderr>:#015 71%|███████   | 533/750 [13:23<05:26,  1.50s/it][1,8]<stderr>:#015 71%|███████   | 533/750 [13:23<05:29,  1.52s/it][1,8]<stderr>:#015 71%|███████   | 534/750 [13:24<05:21,  1.49s/it][1,0]<stderr>:#015 71%|███████   | 534/750 [13:24<05:23,  1.50s/it][1,8]<stderr>:#015 71%|███████▏  | 535/750 [13:26<05:36,  1.56s/it][1,0]<stderr>:#015 71%|███████▏  | 535/750 [13:26<05:37,  1.57s/it][1,8]<stderr>:#015 71%|███████▏  | 536/750 [13:27<05:23,  1.51s/it][1,0]<stderr>:#015 71%|███████▏  | 536/750 [13:27<05:26,  1.53s/it][1,8]<stderr>:#015 72%|███████▏  | 537/750 [13:29<05:07,  1.44s/it][1,0]<stderr>:#015 72%|███████▏  | 537/750 [13:29<05:12,  1.47s/it][1,8]<stderr>:#015 72%|███████▏  | 538/750 [13:30<05:01,  1.42s/it][1,0]<stderr>:#015 72%|███████▏  | 538/750 [13:30<05:05,  1.44s/it][1,0]<stderr>:#015 72%|███████▏  | 539/750 [13:32<05:07,  1.46s/it][1,8]<stderr>:#015 72%|███████▏  | 539/750 [13:32<05:17,  1.50s/it][1,0]<stderr>:#015 72%|███████▏  | 540/750 [13:33<05:03,  1.45s/it][1,8]<stderr>:#015 72%|███████▏  | 540/750 [13:33<05:10,  1.48s/it][1,0]<stderr>:#015 72%|███████▏  | 541/750 [13:34<04:55,  1.41s/it][1,8]<stderr>:#015 72%|███████▏  | 541/750 [13:35<05:09,  1.48s/it][1,0]<stderr>:#015 72%|███████▏  | 542/750 [13:36<05:11,  1.50s/it][1,8]<stderr>:#015 72%|███████▏  | 542/750 [13:36<05:09,  1.49s/it][1,0]<stderr>:#015 72%|███████▏  | 543/750 [13:37<04:57,  1.44s/it][1,8]<stderr>:#015 72%|███████▏  | 543/750 [13:37<04:58,  1.44s/it][1,8]<stderr>:#015 73%|███████▎  | 544/750 [13:39<04:44,  1.38s/it][1,0]<stderr>:#015 73%|███████▎  | 544/750 [13:39<04:47,  1.39s/it][1,0]<stderr>:#015 73%|███████▎  | 545/750 [13:40<04:59,  1.46s/it][1,8]<stderr>:#015 73%|███████▎  | 545/750 [13:40<05:03,  1.48s/it][1,0]<stderr>:#015 73%|███████▎  | 546/750 [13:42<04:47,  1.41s/it][1,8]<stderr>:#015 73%|███████▎  | 546/750 [13:42<04:47,  1.41s/it][1,0]<stderr>:#015 73%|███████▎  | 547/750 [13:43<04:44,  1.40s/it][1,8]<stderr>:#015 73%|███████▎  | 547/750 [13:43<04:46,  1.41s/it][1,8]<stderr>:#015 73%|███████▎  | 548/750 [13:44<04:29,  1.33s/it][1,0]<stderr>:#015 73%|███████▎  | 548/750 [13:44<04:36,  1.37s/it][1,0]<stderr>:#015 73%|███████▎  | 549/750 [13:46<04:33,  1.36s/it][1,8]<stderr>:#015 73%|███████▎  | 549/750 [13:46<04:35,  1.37s/it][1,0]<stderr>:#015 73%|███████▎  | 550/750 [13:47<04:42,  1.41s/it][1,8]<stderr>:#015 73%|███████▎  | 550/750 [13:47<04:42,  1.41s/it][1,0]<stderr>:#015 73%|███████▎  | 551/750 [13:49<04:44,  1.43s/it][1,8]<stderr>:#015 73%|███████▎  | 551/750 [13:49<04:46,  1.44s/it][1,0]<stderr>:#015 74%|███████▎  | 552/750 [13:50<04:40,  1.42s/it][1,8]<stderr>:#015 74%|███████▎  | 552/750 [13:50<04:45,  1.44s/it][1,8]<stderr>:#015 74%|███████▎  | 553/750 [13:51<04:44,  1.44s/it][1,0]<stderr>:#015 74%|███████▎  | 553/750 [13:52<04:52,  1.48s/it][1,0]<stderr>:#015 74%|███████▍  | 554/750 [13:53<04:37,  1.42s/it][1,8]<stderr>:#015 74%|███████▍  | 554/750 [13:53<04:41,  1.44s/it][1,0]<stderr>:#015 74%|███████▍  | 555/750 [13:54<04:35,  1.41s/it][1,8]<stderr>:#015 74%|███████▍  | 555/750 [13:54<04:36,  1.42s/it][1,8]<stderr>:#015 74%|███████▍  | 556/750 [13:56<04:30,  1.39s/it][1,0]<stderr>:#015 74%|███████▍  | 556/750 [13:56<04:40,  1.44s/it][1,0]<stderr>:#015 74%|███████▍  | 557/750 [13:57<04:21,  1.35s/it][1,8]<stderr>:#015 74%|███████▍  | 557/750 [13:57<04:23,  1.37s/it][1,8]<stderr>:#015 74%|███████▍  | 558/750 [13:58<04:18,  1.35s/it][1,0]<stderr>:#015 74%|███████▍  | 558/750 [13:58<04:24,  1.38s/it][1,8]<stderr>:#015 75%|███████▍  | 559/750 [14:00<04:17,  1.35s/it][1,0]<stderr>:#015 75%|███████▍  | 559/750 [14:00<04:26,  1.40s/it][1,8]<stderr>:#015 75%|███████▍  | 560/750 [14:01<04:15,  1.34s/it][1,0]<stderr>:#015 75%|███████▍  | 560/750 [14:01<04:13,  1.33s/it][1,0]<stderr>:#015 75%|███████▍  | 561/750 [14:02<04:11,  1.33s/it][1,8]<stderr>:#015 75%|███████▍  | 561/750 [14:02<04:20,  1.38s/it][1,8]<stderr>:#015 75%|███████▍  | 562/750 [14:04<04:19,  1.38s/it][1,0]<stderr>:#015 75%|███████▍  | 562/750 [14:04<04:19,  1.38s/it][1,8]<stderr>:#015 75%|███████▌  | 563/750 [14:05<04:31,  1.45s/it][1,0]<stderr>:#015 75%|███████▌  | 563/750 [14:05<04:30,  1.44s/it][1,8]<stderr>:#015 75%|███████▌  | 564/750 [14:07<04:33,  1.47s/it][1,0]<stderr>:#015 75%|███████▌  | 564/750 [14:07<04:41,  1.51s/it][1,8]<stderr>:#015 75%|███████▌  | 565/750 [14:09<04:45,  1.54s/it][1,0]<stderr>:#015 75%|███████▌  | 565/750 [14:09<04:47,  1.55s/it][1,8]<stderr>:#015 75%|███████▌  | 566/750 [14:10<04:38,  1.51s/it][1,0]<stderr>:#015 75%|███████▌  | 566/750 [14:10<04:35,  1.50s/it][1,8]<stderr>:#015 76%|███████▌  | 567/750 [14:12<05:00,  1.64s/it][1,0]<stderr>:#015 76%|███████▌  | 567/750 [14:12<05:00,  1.64s/it][1,8]<stderr>:#015 76%|███████▌  | 568/750 [14:14<04:52,  1.61s/it][1,0]<stderr>:#015 76%|███████▌  | 568/750 [14:14<04:51,  1.60s/it][1,0]<stderr>:#015 76%|███████▌  | 569/750 [14:15<04:36,  1.53s/it][1,8]<stderr>:#015 76%|███████▌  | 569/750 [14:15<04:39,  1.54s/it][1,0]<stderr>:#015 76%|███████▌  | 570/750 [14:16<04:28,  1.49s/it][1,8]<stderr>:#015 76%|███████▌  | 570/750 [14:16<04:35,  1.53s/it][1,0]<stderr>:#015 76%|███████▌  | 571/750 [14:18<04:28,  1.50s/it][1,8]<stderr>:#015 76%|███████▌  | 571/750 [14:18<04:30,  1.51s/it][1,0]<stderr>:#015 76%|███████▋  | 572/750 [14:19<04:18,  1.45s/it][1,8]<stderr>:#015 76%|███████▋  | 572/750 [14:19<04:19,  1.46s/it][1,0]<stderr>:#015 76%|███████▋  | 573/750 [14:21<04:24,  1.50s/it][1,8]<stderr>:#015 76%|███████▋  | 573/750 [14:21<04:24,  1.49s/it][1,8]<stderr>:#015 77%|███████▋  | 574/750 [14:22<04:11,  1.43s/it][1,0]<stderr>:#015 77%|███████▋  | 574/750 [14:22<04:22,  1.49s/it][1,8]<stderr>:#015 77%|███████▋  | 575/750 [14:24<04:17,  1.47s/it][1,0]<stderr>:#015 77%|███████▋  | 575/750 [14:24<04:14,  1.46s/it][1,0]<stderr>:#015 77%|███████▋  | 576/750 [14:25<04:15,  1.47s/it][1,8]<stderr>:#015 77%|███████▋  | 576/750 [14:25<04:19,  1.49s/it][1,8]<stderr>:#015 77%|███████▋  | 577/750 [14:27<04:15,  1.48s/it][1,0]<stderr>:#015 77%|███████▋  | 577/750 [14:27<04:15,  1.48s/it][1,8]<stderr>:#015 77%|███████▋  | 578/750 [14:28<04:25,  1.54s/it][1,0]<stderr>:#015 77%|███████▋  | 578/750 [14:28<04:24,  1.54s/it][1,0]<stderr>:#015 77%|███████▋  | 579/750 [14:30<04:12,  1.48s/it][1,8]<stderr>:#015 77%|███████▋  | 579/750 [14:30<04:20,  1.52s/it][1,0]<stderr>:#015 77%|███████▋  | 580/750 [14:31<04:20,  1.53s/it][1,8]<stderr>:#015 77%|███████▋  | 580/750 [14:31<04:22,  1.54s/it][1,0]<stderr>:#015 77%|███████▋  | 581/750 [14:33<04:05,  1.45s/it][1,8]<stderr>:#015 77%|███████▋  | 581/750 [14:33<04:09,  1.47s/it][1,0]<stderr>:#015 78%|███████▊  | 582/750 [14:34<04:01,  1.44s/it][1,8]<stderr>:#015 78%|███████▊  | 582/750 [14:34<03:59,  1.42s/it][1,0]<stderr>:#015 78%|███████▊  | 583/750 [14:35<04:02,  1.45s/it][1,8]<stderr>:#015 78%|███████▊  | 583/750 [14:36<04:02,  1.45s/it][1,0]<stderr>:#015 78%|███████▊  | 584/750 [14:37<04:02,  1.46s/it][1,8]<stderr>:#015 78%|███████▊  | 584/750 [14:37<04:03,  1.46s/it][1,0]<stderr>:#015 78%|███████▊  | 585/750 [14:38<04:00,  1.46s/it][1,8]<stderr>:#015 78%|███████▊  | 585/750 [14:38<03:59,  1.45s/it][1,0]<stderr>:#015 78%|███████▊  | 586/750 [14:40<04:21,  1.60s/it][1,8]<stderr>:#015 78%|███████▊  | 586/750 [14:40<04:19,  1.58s/it][1,8]<stderr>:#015 78%|███████▊  | 587/750 [14:42<04:12,  1.55s/it][1,0]<stderr>:#015 78%|███████▊  | 587/750 [14:42<04:16,  1.57s/it][1,0]<stderr>:#015 78%|███████▊  | 588/750 [14:43<04:16,  1.59s/it][1,8]<stderr>:#015 78%|███████▊  | 588/750 [14:44<04:20,  1.61s/it][1,0]<stderr>:#015 79%|███████▊  | 589/750 [14:45<04:05,  1.53s/it][1,8]<stderr>:#015 79%|███████▊  | 589/750 [14:45<04:04,  1.52s/it][1,8]<stderr>:#015 79%|███████▊  | 590/750 [14:47<04:11,  1.57s/it][1,0]<stderr>:#015 79%|███████▊  | 590/750 [14:47<04:16,  1.60s/it][1,8]<stderr>:#015 79%|███████▉  | 591/750 [14:48<04:00,  1.51s/it][1,0]<stderr>:#015 79%|███████▉  | 591/750 [14:48<04:04,  1.54s/it][1,8]<stderr>:#015 79%|███████▉  | 592/750 [14:50<04:19,  1.65s/it][1,0]<stderr>:#015 79%|███████▉  | 592/750 [14:50<04:20,  1.65s/it][1,0]<stderr>:#015 79%|███████▉  | 593/750 [14:52<04:19,  1.65s/it][1,8]<stderr>:#015 79%|███████▉  | 593/750 [14:52<04:29,  1.71s/it][1,8]<stderr>:#015 79%|███████▉  | 594/750 [14:53<04:06,  1.58s/it][1,0]<stderr>:#015 79%|███████▉  | 594/750 [14:53<04:09,  1.60s/it][1,0]<stderr>:#015 79%|███████▉  | 595/750 [14:55<04:02,  1.57s/it][1,8]<stderr>:#015 79%|███████▉  | 595/750 [14:55<04:08,  1.60s/it][1,0]<stderr>:#015 79%|███████▉  | 596/750 [14:56<04:00,  1.56s/it][1,8]<stderr>:#015 79%|███████▉  | 596/750 [14:56<04:04,  1.58s/it][1,8]<stderr>:#015 80%|███████▉  | 597/750 [14:58<03:51,  1.52s/it][1,0]<stderr>:#015 80%|███████▉  | 597/750 [14:58<03:56,  1.55s/it][1,0]<stderr>:#015 80%|███████▉  | 598/750 [14:59<03:59,  1.58s/it][1,8]<stderr>:#015 80%|███████▉  | 598/750 [14:59<03:58,  1.57s/it][1,0]<stderr>:#015 80%|███████▉  | 599/750 [15:01<03:42,  1.47s/it][1,8]<stderr>:#015 80%|███████▉  | 599/750 [15:00<03:42,  1.47s/it][1,0]<stderr>:#015 80%|████████  | 600/750 [15:02<03:33,  1.42s/it][1,8]<stderr>:#015 80%|████████  | 600/750 [15:02<03:35,  1.44s/it][1,8]<stderr>:#015 80%|████████  | 601/750 [15:03<03:38,  1.47s/it][1,0]<stderr>:#015 80%|████████  | 601/750 [15:03<03:43,  1.50s/it][1,8]<stderr>:#015 80%|████████  | 602/750 [15:05<03:36,  1.46s/it][1,0]<stderr>:#015 80%|████████  | 602/750 [15:05<03:38,  1.48s/it][1,8]<stderr>:#015 80%|████████  | 603/750 [15:06<03:33,  1.45s/it][1,0]<stderr>:#015 80%|████████  | 603/750 [15:06<03:42,  1.51s/it][1,8]<stderr>:#015 81%|██████�\u001b[0m\n",
      "\u001b[34m�█  | 604/750 [15:08<03:42,  1.52s/it][1,0]<stderr>:#015 81%|████████  | 604/750 [15:08<03:41,  1.52s/it][1,0]<stderr>:#015 81%|████████  | 605/750 [15:09<03:22,  1.40s/it][1,8]<stderr>:#015 81%|████████  | 605/750 [15:09<03:29,  1.44s/it][1,8]<stderr>:#015 81%|████████  | 606/750 [15:10<03:19,  1.39s/it][1,0]<stderr>:#015 81%|████████  | 606/750 [15:11<03:19,  1.38s/it][1,0]<stderr>:#015 81%|████████  | 607/750 [15:12<03:27,  1.45s/it][1,8]<stderr>:#015 81%|████████  | 607/750 [15:12<03:31,  1.48s/it][1,8]<stderr>:#015 81%|████████  | 608/750 [15:14<03:43,  1.58s/it][1,0]<stderr>:#015 81%|████████  | 608/750 [15:14<03:44,  1.58s/it][1,8]<stderr>:#015 81%|████████  | 609/750 [15:16<03:48,  1.62s/it][1,0]<stderr>:#015 81%|████████  | 609/750 [15:16<03:49,  1.62s/it][1,8]<stderr>:#015 81%|████████▏ | 610/750 [15:17<03:44,  1.61s/it][1,0]<stderr>:#015 81%|████████▏ | 610/750 [15:17<03:49,  1.64s/it][1,0]<stderr>:#015 81%|████████▏ | 611/750 [15:19<03:38,  1.57s/it][1,8]<stderr>:#015 81%|████████▏ | 611/750 [15:19<03:41,  1.59s/it][1,8]<stderr>:#015 82%|████████▏ | 612/750 [15:20<03:35,  1.56s/it][1,0]<stderr>:#015 82%|████████▏ | 612/750 [15:20<03:35,  1.56s/it][1,8]<stderr>:#015 82%|████████▏ | 613/750 [15:22<03:32,  1.55s/it][1,0]<stderr>:#015 82%|████████▏ | 613/750 [15:22<03:36,  1.58s/it][1,8]<stderr>:#015 82%|████████▏ | 614/750 [15:24<03:36,  1.59s/it][1,0]<stderr>:#015 82%|████████▏ | 614/750 [15:24<03:39,  1.61s/it][1,0]<stderr>:#015 82%|████████▏ | 615/750 [15:25<03:24,  1.52s/it][1,8]<stderr>:#015 82%|████████▏ | 615/750 [15:25<03:35,  1.60s/it][1,8]<stderr>:#015 82%|████████▏ | 616/750 [15:26<03:18,  1.48s/it][1,0]<stderr>:#015 82%|████████▏ | 616/750 [15:26<03:20,  1.49s/it][1,8]<stderr>:#015 82%|████████▏ | 617/750 [15:28<03:11,  1.44s/it][1,0]<stderr>:#015 82%|████████▏ | 617/750 [15:28<03:13,  1.46s/it][1,0]<stderr>:#015 82%|████████▏ | 618/750 [15:29<03:15,  1.48s/it][1,8]<stderr>:#015 82%|████████▏ | 618/750 [15:29<03:16,  1.49s/it][1,0]<stderr>:#015 83%|████████▎ | 619/750 [15:31<03:15,  1.49s/it][1,8]<stderr>:#015 83%|████████▎ | 619/750 [15:31<03:17,  1.51s/it][1,8]<stderr>:#015 83%|████████▎ | 620/750 [15:32<03:10,  1.47s/it][1,0]<stderr>:#015 83%|████████▎ | 620/750 [15:32<03:13,  1.49s/it][1,0]<stderr>:#015 83%|████████▎ | 621/750 [15:34<03:26,  1.60s/it][1,8]<stderr>:#015 83%|████████▎ | 621/750 [15:34<03:28,  1.62s/it][1,8]<stderr>:#015 83%|████████▎ | 622/750 [15:36<03:23,  1.59s/it][1,0]<stderr>:#015 83%|████████▎ | 622/750 [15:36<03:25,  1.61s/it][1,8]<stderr>:#015 83%|████████▎ | 623/750 [15:37<03:17,  1.56s/it][1,0]<stderr>:#015 83%|████████▎ | 623/750 [15:37<03:17,  1.55s/it][1,0]<stderr>:#015 83%|████████▎ | 624/750 [15:39<03:19,  1.59s/it][1,8]<stderr>:#015 83%|████████▎ | 624/750 [15:39<03:22,  1.60s/it][1,8]<stderr>:#015 83%|████████▎ | 625/750 [15:41<03:36,  1.73s/it][1,0]<stderr>:#015 83%|████████▎ | 625/750 [15:41<03:38,  1.74s/it][1,0]<stderr>:#015 83%|████████▎ | 626/750 [15:42<03:22,  1.64s/it][1,8]<stderr>:#015 83%|████████▎ | 626/750 [15:42<03:25,  1.65s/it][1,8]<stderr>:#015 84%|████████▎ | 627/750 [15:44<03:17,  1.60s/it][1,0]<stderr>:#015 84%|████████▎ | 627/750 [15:44<03:19,  1.62s/it][1,8]<stderr>:#015 84%|████████▎ | 628/750 [15:45<03:05,  1.52s/it][1,0]<stderr>:#015 84%|████████▎ | 628/750 [15:45<03:08,  1.55s/it][1,8]<stderr>:#015 84%|████████▍ | 629/750 [15:47<03:03,  1.52s/it][1,0]<stderr>:#015 84%|████████▍ | 629/750 [15:47<03:03,  1.52s/it][1,0]<stderr>:#015 84%|████████▍ | 630/750 [15:48<02:52,  1.44s/it][1,8]<stderr>:#015 84%|████████▍ | 630/750 [15:48<02:58,  1.49s/it][1,8]<stderr>:#015 84%|████████▍ | 631/750 [15:49<02:50,  1.43s/it][1,0]<stderr>:#015 84%|████████▍ | 631/750 [15:50<02:56,  1.49s/it][1,0]<stderr>:#015 84%|████████▍ | 632/750 [15:51<02:58,  1.51s/it][1,8]<stderr>:#015 84%|████████▍ | 632/750 [15:51<03:01,  1.54s/it][1,8]<stderr>:#015 84%|████████▍ | 633/750 [15:53<03:11,  1.63s/it][1,0]<stderr>:#015 84%|████████▍ | 633/750 [15:53<03:10,  1.62s/it][1,0]<stderr>:#015 85%|████████▍ | 634/750 [15:54<02:53,  1.50s/it][1,8]<stderr>:#015 85%|████████▍ | 634/750 [15:54<02:56,  1.52s/it][1,8]<stderr>:#015 85%|████████▍ | 635/750 [15:56<02:47,  1.46s/it][1,0]<stderr>:#015 85%|████████▍ | 635/750 [15:56<02:49,  1.47s/it][1,0]<stderr>:#015 85%|████████▍ | 636/750 [15:57<02:44,  1.44s/it][1,8]<stderr>:#015 85%|████████▍ | 636/750 [15:57<02:48,  1.48s/it][1,8]<stderr>:#015 85%|████████▍ | 637/750 [15:58<02:40,  1.42s/it][1,0]<stderr>:#015 85%|████████▍ | 637/750 [15:59<02:43,  1.44s/it][1,0]<stderr>:#015 85%|████████▌ | 638/750 [16:00<02:34,  1.38s/it][1,8]<stderr>:#015 85%|████████▌ | 638/750 [16:00<02:35,  1.39s/it][1,8]<stderr>:#015 85%|████████▌ | 639/750 [16:01<02:40,  1.45s/it][1,0]<stderr>:#015 85%|████████▌ | 639/750 [16:01<02:40,  1.45s/it][1,8]<stderr>:#015 85%|████████▌ | 640/750 [16:03<02:51,  1.56s/it][1,0]<stderr>:#015 85%|████████▌ | 640/750 [16:03<02:52,  1.57s/it][1,0]<stderr>:#015 85%|████████▌ | 641/750 [16:04<02:34,  1.42s/it][1,8]<stderr>:#015 85%|████████▌ | 641/750 [16:04<02:38,  1.45s/it][1,8]<stderr>:#015 86%|████████▌ | 642/750 [16:06<02:35,  1.44s/it][1,0]<stderr>:#015 86%|████████▌ | 642/750 [16:06<02:39,  1.47s/it][1,0]<stderr>:#015 86%|████████▌ | 643/750 [16:07<02:26,  1.37s/it][1,8]<stderr>:#015 86%|████████▌ | 643/750 [16:07<02:31,  1.42s/it][1,0]<stderr>:#015 86%|████████▌ | 644/750 [16:08<02:25,  1.37s/it][1,8]<stderr>:#015 86%|████████▌ | 644/750 [16:09<02:29,  1.41s/it][1,0]<stderr>:#015 86%|████████▌ | 645/750 [16:10<02:24,  1.37s/it][1,8]<stderr>:#015 86%|████████▌ | 645/750 [16:10<02:24,  1.37s/it][1,0]<stderr>:#015 86%|████████▌ | 646/750 [16:12<02:35,  1.50s/it][1,8]<stderr>:#015 86%|████████▌ | 646/750 [16:12<02:36,  1.50s/it][1,0]<stderr>:#015 86%|████████▋ | 647/750 [16:13<02:27,  1.43s/it][1,8]<stderr>:#015 86%|████████▋ | 647/750 [16:13<02:31,  1.47s/it][1,0]<stderr>:#015 86%|████████▋ | 648/750 [16:15<02:36,  1.53s/it][1,8]<stderr>:#015 86%|████████▋ | 648/750 [16:15<02:35,  1.52s/it][1,0]<stderr>:#015 87%|████████▋ | 649/750 [16:16<02:34,  1.53s/it][1,8]<stderr>:#015 87%|████████▋ | 649/750 [16:16<02:32,  1.51s/it][1,8]<stderr>:#015 87%|████████▋ | 650/750 [16:18<02:28,  1.48s/it][1,0]<stderr>:#015 87%|████████▋ | 650/750 [16:18<02:32,  1.52s/it][1,8]<stderr>:#015 87%|████████▋ | 651/750 [16:19<02:33,  1.55s/it][1,0]<stderr>:#015 87%|████████▋ | 651/750 [16:19<02:34,  1.56s/it][1,8]<stderr>:#015 87%|████████▋ | 652/750 [16:21<02:25,  1.49s/it][1,0]<stderr>:#015 87%|████████▋ | 652/750 [16:21<02:28,  1.52s/it][1,8]<stderr>:#015 87%|████████▋ | 653/750 [16:22<02:22,  1.47s/it][1,0]<stderr>:#015 87%|████████▋ | 653/750 [16:22<02:26,  1.51s/it][1,8]<stderr>:#015 87%|████████▋ | 654/750 [16:24<02:22,  1.48s/it][1,0]<stderr>:#015 87%|████████▋ | 654/750 [16:24<02:24,  1.50s/it][1,0]<stderr>:#015 87%|████████▋ | 655/750 [16:25<02:22,  1.50s/it][1,8]<stderr>:#015 87%|████████▋ | 655/750 [16:25<02:25,  1.53s/it][1,8]<stderr>:#015 87%|████████▋ | 656/750 [16:27<02:27,  1.57s/it][1,0]<stderr>:#015 87%|████████▋ | 656/750 [16:27<02:26,  1.56s/it][1,0]<stderr>:#015 88%|████████▊ | 657/750 [16:28<02:25,  1.56s/it][1,8]<stderr>:#015 88%|████████▊ | 657/750 [16:29<02:35,  1.68s/it][1,8]<stderr>:#015 88%|████████▊ | 658/750 [16:30<02:17,  1.49s/it][1,0]<stderr>:#015 88%|████████▊ | 658/750 [16:30<02:22,  1.55s/it][1,8]<stderr>:#015 88%|████████▊ | 659/750 [16:31<02:13,  1.47s/it][1,0]<stderr>:#015 88%|████████▊ | 659/750 [16:31<02:16,  1.50s/it][1,8]<stderr>:#015 88%|████████▊ | 660/750 [16:33<02:11,  1.47s/it][1,0]<stderr>:#015 88%|████████▊ | 660/750 [16:33<02:12,  1.47s/it][1,8]<stderr>:#015 88%|████████▊ | 661/750 [16:34<02:06,  1.42s/it][1,0]<stderr>:#015 88%|████████▊ | 661/750 [16:34<02:09,  1.46s/it][1,8]<stderr>:#015 88%|████████▊ | 662/750 [16:36<02:06,  1.44s/it][1,0]<stderr>:#015 88%|████████▊ | 662/750 [16:36<02:06,  1.43s/it][1,8]<stderr>:#015 88%|████████▊ | 663/750 [16:37<01:59,  1.38s/it][1,0]<stderr>:#015 88%|████████▊ | 663/750 [16:37<02:01,  1.40s/it][1,0]<stderr>:#015 89%|████████▊ | 664/750 [16:38<01:58,  1.37s/it][1,8]<stderr>:#015 89%|████████▊ | 664/750 [16:38<02:00,  1.40s/it][1,0]<stderr>:#015 89%|████████▊ | 665/750 [16:40<01:58,  1.39s/it][1,8]<stderr>:#015 89%|████████▊ | 665/750 [16:40<02:00,  1.41s/it][1,8]<stderr>:#015 89%|████████▉ | 666/750 [16:41<01:54,  1.37s/it][1,0]<stderr>:#015 89%|████████▉ | 666/750 [16:41<01:57,  1.40s/it][1,0]<stderr>:#015 89%|████████▉ | 667/750 [16:43<01:57,  1.42s/it][1,8]<stderr>:#015 89%|████████▉ | 667/750 [16:42<01:59,  1.43s/it][1,0]<stderr>:#015 89%|████████▉ | 668/750 [16:44<01:57,  1.43s/it][1,8]<stderr>:#015 89%|████████▉ | 668/750 [16:44<01:58,  1.44s/it][1,8]<stderr>:#015 89%|████████▉ | 669/750 [16:45<01:58,  1.46s/it][1,0]<stderr>:#015 89%|████████▉ | 669/750 [16:45<01:57,  1.46s/it][1,8]<stderr>:#015 89%|████████▉ | 670/750 [16:47<01:51,  1.39s/it][1,0]<stderr>:#015 89%|████████▉ | 670/750 [16:47<01:53,  1.41s/it][1,8]<stderr>:#015 89%|████████▉ | 671/750 [16:48<01:59,  1.51s/it][1,0]<stderr>:#015 89%|████████▉ | 671/750 [16:49<01:59,  1.51s/it][1,8]<stderr>:#015 90%|████████▉ | 672/750 [16:50<01:58,  1.52s/it][1,0]<stderr>:#015 90%|████████▉ | 672/750 [16:50<01:57,  1.51s/it][1,0]<stderr>:#015 90%|████████▉ | 673/750 [16:52<01:57,  1.53s/it][1,8]<stderr>:#015 90%|████████▉ | 673/750 [16:52<01:59,  1.56s/it][1,8]<stderr>:#015 90%|████████▉ | 674/750 [16:53<01:53,  1.49s/it][1,0]<stderr>:#015 90%|████████▉ | 674/750 [16:53<01:53,  1.50s/it][1,0]<stderr>:#015 90%|█████████ | 675/750 [16:54<01:48,  1.45s/it][1,8]<stderr>:#015 90%|█████████ | 675/750 [16:54<01:50,  1.47s/it][1,8]<stderr>:#015 90%|█████████ | 676/750 [16:56<01:43,  1.40s/it][1,0]<stderr>:#015 90%|█████████ | 676/750 [16:56<01:44,  1.42s/it][1,0]<stderr>:#015 90%|█████████ | 677/750 [16:57<01:44,  1.44s/it][1,8]<stderr>:#015 90%|█████████ | 677/750 [16:57<01:46,  1.46s/it][1,8]<stderr>:#015 90%|█████████ | 678/750 [16:58<01:39,  1.38s/it][1,0]<stderr>:#015 90%|█████████ | 678/750 [16:59<01:40,  1.40s/it][1,8]<stderr>:#015 91%|█████████ | 679/750 [17:00<01:37,  1.37s/it][1,0]<stderr>:#015 91%|█████████ | 679/750 [17:00<01:37,  1.37s/it][1,8]<stderr>:#015 91%|█████████ | 680/750 [17:01<01:32,  1.32s/it][1,0]<stderr>:#015 91%|█████████ | 680/750 [17:01<01:32,  1.32s/it][1,0]<stderr>:#015 91%|█████████ | 681/750 [17:03<01:40,  1.46s/it][1,8]<stderr>:#015 91%|█████████ | 681/750 [17:03<01:41,  1.47s/it][1,0]<stderr>:#015 91%|█████████ | 682/750 [17:04<01:35,  1.41s/it][1,8]<stderr>:#015 91%|█████████ | 682/750 [17:04<01:36,  1.42s/it][1,0]<stderr>:#015 91%|█████████ | 683/750 [17:06<01:37,  1.46s/it][1,8]<stderr>:#015 91%|█████████ | 683/750 [17:06<01:37,  1.46s/it][1,0]<stderr>:#015 91%|█████████ | 684/750 [17:07<01:38,  1.50s/it][1,8]<stderr>:#015 91%|█████████ | 684/750 [17:07<01:39,  1.51s/it][1,8]<stderr>:#015 91%|█████████▏| 685/750 [17:09<01:36,  1.49s/it][1,0]<stderr>:#015 91%|█████████▏| 685/750 [17:09<01:37,  1.50s/it][1,8]<stderr>:#015 91%|█████████▏| 686/750 [17:10<01:34,  1.47s/it][1,0]<stderr>:#015 91%|█████████▏| 686/750 [17:10<01:34,  1.48s/it][1,8]<stderr>:#015 92%|█████████▏| 687/750 [17:12<01:34,  1.51s/it][1,0]<stderr>:#015 92%|█████████▏| 687/750 [17:12<01:35,  1.51s/it][1,8]<stderr>:#015 92%|█████████▏| 688/750 [17:13<01:34,  1.53s/it][1,0]<stderr>:#015 92%|█████████▏| 688/750 [17:13<01:35,  1.54s/it][1,8]<stderr>:#015 92%|█████████▏| 689/750 [17:15<01:32,  1.51s/it][1,0]<stderr>:#015 92%|█████████▏| 689/750 [17:15<01:33,  1.53s/it][1,8]<stderr>:#015 92%|█████████▏| 690/750 [17:16<01:32,  1.55s/it][1,0]<stderr>:#015 92%|█████████▏| 690/750 [17:17<01:35,  1.59s/it][1,8]<stderr>:#015 92%|█████████▏| 691/750 [17:18<01:29,  1.51s/it][1,0]<stderr>:#015 92%|█████████▏| 691/750 [17:18<01:30,  1.53s/it][1,8]<stderr>:#015 92%|█████████▏| 692/750 [17:19<01:23,  1.44s/it][1,0]<stderr>:#015 92%|█████████▏| 692/750 [17:19<01:24,  1.45s/it][1,8]<stderr>:#015 92%|█████████▏| 693/750 [17:21<01:23,  1.47s/it][1,0]<stderr>:#015 92%|█████████▏| 693/750 [17:21<01:22,  1.44s/it][1,8]<stderr>:#015 93%|█████████▎| 694/750 [17:22<01:25,  1.53s/it][1,0]<stderr>:#015 93%|█████████▎| 694/750 [17:22<01:24,  1.51s/it][1,0]<stderr>:#015 93%|█████████▎| 695/750 [17:24<01:22,  1.50s/it][1,8]<stderr>:#015 93%|█████████▎| 695/750 [17:24<01:26,  1.57s/it][1,0]<stderr>:#015 93%|█████████▎| 696/750 [17:25<01:22,  1.53s/it][1,8]<stderr>:#015 93%|█████████▎| 696/750 [17:25<01:22,  1.54s/it][1,0]<stderr>:#015 93%|█████████▎| 697/750 [17:27<01:18,  1.48s/it][1,8]<stderr>:#015 93%|█████████▎| 697/750 [17:27<01:20,  1.51s/it][1,8]<stderr>:#015 93%|█████████▎| 698/750 [17:28<01:15,  1.46s/it][1,0]<stderr>:#015 93%|█████████▎| 698/750 [17:28<01:17,  1.50s/it][1,0]<stderr>:#015 93%|█████████▎| 699/750 [17:29<01:10,  1.37s/it][1,8]<stderr>:#015 93%|█████████▎| 699/750 [17:29<01:10,  1.39s/it][1,0]<stderr>:#015 93%|█████████▎| 700/750 [17:31<01:09,  1.40s/it][1,8]<stderr>:#015 93%|█████████▎| 700/750 [17:31<01:12,  1.45s/it][1,0]<stderr>:#015 93%|█████████▎| 701/750 [17:32<01:10,  1.43s/it][1,8]<stderr>:#015 93%|█████████▎| 701/750 [17:33<01:11,  1.46s/it][1,8]<stderr>:#015 94%|█████████▎| 702/750 [17:34<01:09,  1.45s/it][1,0]<stderr>:#015 94%|█████████▎| 702/750 [17:34<01:11,  1.49s/it][1,0]<stderr>:#015 94%|█████████▎| 703/750 [17:36<01:12,  1.55s/it][1,8]<stderr>:#015 94%|█████████▎| 703/\u001b[0m\n",
      "\u001b[34m750 [17:36<01:12,  1.55s/it][1,0]<stderr>:#015 94%|█████████▍| 704/750 [17:37<01:05,  1.43s/it][1,8]<stderr>:#015 94%|█████████▍| 704/750 [17:37<01:07,  1.47s/it][1,0]<stderr>:#015 94%|█████████▍| 705/750 [17:38<01:05,  1.45s/it][1,8]<stderr>:#015 94%|█████████▍| 705/750 [17:38<01:04,  1.44s/it][1,8]<stderr>:#015 94%|█████████▍| 706/750 [17:40<01:00,  1.38s/it][1,0]<stderr>:#015 94%|█████████▍| 706/750 [17:40<01:04,  1.46s/it][1,0]<stderr>:#015 94%|█████████▍| 707/750 [17:41<00:57,  1.35s/it][1,8]<stderr>:#015 94%|█████████▍| 707/750 [17:41<00:58,  1.35s/it][1,0]<stderr>:#015 94%|█████████▍| 708/750 [17:42<00:58,  1.39s/it][1,8]<stderr>:#015 94%|█████████▍| 708/750 [17:42<00:58,  1.39s/it][1,8]<stderr>:#015 95%|█████████▍| 709/750 [17:44<00:56,  1.39s/it][1,0]<stderr>:#015 95%|█████████▍| 709/750 [17:44<00:58,  1.43s/it][1,0]<stderr>:#015 95%|█████████▍| 710/750 [17:45<00:56,  1.42s/it][1,8]<stderr>:#015 95%|█████████▍| 710/750 [17:45<00:57,  1.44s/it][1,8]<stderr>:#015 95%|█████████▍| 711/750 [17:47<00:55,  1.43s/it][1,0]<stderr>:#015 95%|█████████▍| 711/750 [17:47<00:55,  1.43s/it][1,0]<stderr>:#015 95%|█████████▍| 712/750 [17:48<00:56,  1.50s/it][1,8]<stderr>:#015 95%|█████████▍| 712/750 [17:49<00:58,  1.54s/it][1,0]<stderr>:#015 95%|█████████▌| 713/750 [17:50<00:55,  1.49s/it][1,8]<stderr>:#015 95%|█████████▌| 713/750 [17:50<00:55,  1.49s/it][1,0]<stderr>:#015 95%|█████████▌| 714/750 [17:51<00:54,  1.50s/it][1,8]<stderr>:#015 95%|█████████▌| 714/750 [17:51<00:54,  1.51s/it][1,0]<stderr>:#015 95%|█████████▌| 715/750 [17:53<00:52,  1.50s/it][1,8]<stderr>:#015 95%|█████████▌| 715/750 [17:53<00:53,  1.53s/it][1,0]<stderr>:#015 95%|█████████▌| 716/750 [17:55<00:54,  1.61s/it][1,8]<stderr>:#015 95%|█████████▌| 716/750 [17:55<00:54,  1.59s/it][1,0]<stderr>:#015 96%|█████████▌| 717/750 [17:56<00:51,  1.56s/it][1,8]<stderr>:#015 96%|█████████▌| 717/750 [17:56<00:52,  1.59s/it][1,8]<stderr>:#015 96%|█████████▌| 718/750 [17:58<00:46,  1.46s/it][1,0]<stderr>:#015 96%|█████████▌| 718/750 [17:58<00:48,  1.52s/it][1,0]<stderr>:#015 96%|█████████▌| 719/750 [17:59<00:44,  1.45s/it][1,8]<stderr>:#015 96%|█████████▌| 719/750 [17:59<00:45,  1.46s/it][1,8]<stderr>:#015 96%|█████████▌| 720/750 [18:00<00:42,  1.43s/it][1,0]<stderr>:#015 96%|█████████▌| 720/750 [18:01<00:44,  1.48s/it][1,8]<stderr>:#015 96%|█████████▌| 721/750 [18:02<00:40,  1.38s/it][1,0]<stderr>:#015 96%|█████████▌| 721/750 [18:02<00:39,  1.38s/it][1,0]<stderr>:#015 96%|█████████▋| 722/750 [18:03<00:38,  1.39s/it][1,8]<stderr>:#015 96%|█████████▋| 722/750 [18:03<00:39,  1.41s/it][1,8]<stderr>:#015 96%|█████████▋| 723/750 [18:04<00:36,  1.37s/it][1,0]<stderr>:#015 96%|█████████▋| 723/750 [18:04<00:37,  1.38s/it][1,8]<stderr>:#015 97%|█████████▋| 724/750 [18:06<00:37,  1.46s/it][1,0]<stderr>:#015 97%|█████████▋| 724/750 [18:06<00:39,  1.50s/it][1,0]<stderr>:#015 97%|█████████▋| 725/750 [18:08<00:37,  1.50s/it][1,8]<stderr>:#015 97%|█████████▋| 725/750 [18:08<00:37,  1.51s/it][1,0]<stderr>:#015 97%|█████████▋| 726/750 [18:10<00:38,  1.61s/it][1,8]<stderr>:#015 97%|█████████▋| 726/750 [18:10<00:38,  1.62s/it][1,8]<stderr>:#015 97%|█████████▋| 727/750 [18:11<00:36,  1.58s/it][1,0]<stderr>:#015 97%|█████████▋| 727/750 [18:11<00:36,  1.59s/it][1,0]<stderr>:#015 97%|█████████▋| 728/750 [18:13<00:35,  1.60s/it][1,8]<stderr>:#015 97%|█████████▋| 728/750 [18:13<00:35,  1.63s/it][1,0]<stderr>:#015 97%|█████████▋| 729/750 [18:14<00:34,  1.63s/it][1,8]<stderr>:#015 97%|█████████▋| 729/750 [18:14<00:34,  1.65s/it][1,0]<stderr>:#015 97%|█████████▋| 730/750 [18:16<00:33,  1.65s/it][1,8]<stderr>:#015 97%|█████████▋| 730/750 [18:16<00:33,  1.67s/it][1,0]<stderr>:#015 97%|█████████▋| 731/750 [18:18<00:33,  1.75s/it][1,8]<stderr>:#015 97%|█████████▋| 731/750 [18:18<00:33,  1.75s/it][1,0]<stderr>:#015 98%|█████████▊| 732/750 [18:20<00:31,  1.78s/it][1,8]<stderr>:#015 98%|█████████▊| 732/750 [18:20<00:32,  1.78s/it][1,8]<stderr>:#015 98%|█████████▊| 733/750 [18:21<00:28,  1.68s/it][1,0]<stderr>:#015 98%|█████████▊| 733/750 [18:21<00:28,  1.70s/it][1,0]<stderr>:#015 98%|█████████▊| 734/750 [18:23<00:25,  1.60s/it][1,8]<stderr>:#015 98%|█████████▊| 734/750 [18:23<00:25,  1.61s/it][1,8]<stderr>:#015 98%|█████████▊| 735/750 [18:24<00:23,  1.58s/it][1,0]<stderr>:#015 98%|█████████▊| 735/750 [18:24<00:24,  1.61s/it][1,0]<stderr>:#015 98%|█████████▊| 736/750 [18:26<00:21,  1.57s/it][1,8]<stderr>:#015 98%|█████████▊| 736/750 [18:26<00:22,  1.57s/it][1,8]<stderr>:#015 98%|█████████▊| 737/750 [18:27<00:20,  1.56s/it][1,0]<stderr>:#015 98%|█████████▊| 737/750 [18:28<00:20,  1.58s/it][1,0]<stderr>:#015 98%|█████████▊| 738/750 [18:29<00:18,  1.50s/it][1,8]<stderr>:#015 98%|█████████▊| 738/750 [18:29<00:18,  1.50s/it][1,0]<stderr>:#015 99%|█████████▊| 739/750 [18:30<00:16,  1.50s/it][1,8]<stderr>:#015 99%|█████████▊| 739/750 [18:30<00:16,  1.52s/it][1,0]<stderr>:#015 99%|█████████▊| 740/750 [18:32<00:15,  1.53s/it][1,8]<stderr>:#015 99%|█████████▊| 740/750 [18:32<00:15,  1.55s/it][1,8]<stderr>:#015 99%|█████████▉| 741/750 [18:33<00:13,  1.48s/it][1,0]<stderr>:#015 99%|█████████▉| 741/750 [18:33<00:13,  1.50s/it][1,8]<stderr>:#015 99%|█████████▉| 742/750 [18:35<00:11,  1.45s/it][1,0]<stderr>:#015 99%|█████████▉| 742/750 [18:35<00:11,  1.50s/it][1,8]<stderr>:#015 99%|█████████▉| 743/750 [18:37<00:10,  1.56s/it][1,0]<stderr>:#015 99%|█████████▉| 743/750 [18:37<00:10,  1.55s/it][1,8]<stderr>:#015 99%|█████████▉| 744/750 [18:38<00:09,  1.53s/it][1,0]<stderr>:#015 99%|█████████▉| 744/750 [18:38<00:09,  1.53s/it][1,0]<stderr>:#015 99%|█████████▉| 745/750 [18:39<00:07,  1.45s/it][1,8]<stderr>:#015 99%|█████████▉| 745/750 [18:39<00:07,  1.49s/it][1,8]<stderr>:#015 99%|█████████▉| 746/750 [18:41<00:05,  1.44s/it][1,0]<stderr>:#015 99%|█████████▉| 746/750 [18:41<00:05,  1.48s/it][1,8]<stderr>:#015100%|█████████▉| 747/750 [18:42<00:04,  1.44s/it][1,0]<stderr>:#015100%|█████████▉| 747/750 [18:42<00:04,  1.44s/it][1,8]<stderr>:#015100%|█████████▉| 748/750 [18:44<00:02,  1.48s/it][1,0]<stderr>:#015100%|█████████▉| 748/750 [18:44<00:02,  1.49s/it][1,8]<stderr>:#015100%|█████████▉| 749/750 [18:45<00:01,  1.53s/it][1,0]<stderr>:#015100%|█████████▉| 749/750 [18:45<00:01,  1.54s/it][1,0]<stderr>:#015100%|██████████| 750/750 [18:47<00:00,  1.49s/it][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:Training completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015100%|██████████| 750/750 [18:47<00:00,  1.50s/it][1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:Training completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015                                                 [1,0]<stderr>:#015[1,8]<stderr>:#015                                                 [1,8]<stderr>:#015[1,0]<stderr>:#015100%|██████████| 750/750 [18:47<00:00,  1.49s/it][1,8]<stderr>:#015100%|██████████| 750/750 [18:47<00:00,  1.50s/it][1,0]<stderr>:#015100%|██████████| 750/750 [18:47<00:00,  1.50s/it][1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015100%|██████████| 750/750 [18:47<00:00,  1.50s/it]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  Num examples = 2000\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  Batch size = 4\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:***** train metrics *****\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  epoch                      =        3.0\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  init_mem_cpu_alloc_delta   =     -200MB\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  init_mem_cpu_peaked_delta  =      199MB\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  init_mem_gpu_alloc_delta   =     1165MB\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  init_mem_gpu_peaked_delta  =        0MB\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  train_mem_cpu_alloc_delta  =     1549MB\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  train_mem_cpu_peaked_delta =      516MB\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  train_mem_gpu_alloc_delta  =     4662MB\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  train_mem_gpu_peaked_delta =     5276MB\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  train_runtime              = 0:18:47.35\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  train_samples              =      16000\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  train_samples_per_second   =      0.665\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  Num examples = 2000\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  Batch size = 4\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:#015  0%|          | 0/32 [00:00<?, ?it/s][1,8]<stderr>:#015  0%|          | 0/32 [00:00<?, ?it/s][1,0]<stderr>:#015  6%|▋         | 2/32 [00:01<00:18,  1.61it/s][1,8]<stderr>:#015  6%|▋         | 2/32 [00:01<00:18,  1.61it/s][1,0]<stderr>:#015  9%|▉         | 3/32 [00:02<00:24,  1.18it/s][1,8]<stderr>:#015  9%|▉         | 3/32 [00:02<00:24,  1.18it/s][1,0]<stderr>:#015 12%|█▎        | 4/32 [00:04<00:28,  1.02s/it][1,8]<stderr>:#015 12%|█▎        | 4/32 [00:04<00:28,  1.02s/it][1,0]<stderr>:#015 16%|█▌        | 5/32 [00:05<00:29,  1.09s/it][1,8]<stderr>:#015 16%|█▌        | 5/32 [00:05<00:29,  1.09s/it][1,0]<stderr>:#015 19%|█▉        | 6/32 [00:06<00:29,  1.13s/it][1,8]<stderr>:#015 19%|█▉        | 6/32 [00:06<00:29,  1.13s/it][1,0]<stderr>:#015 22%|██▏       | 7/32 [00:07<00:29,  1.17s/it][1,8]<stderr>:#015 22%|██▏       | 7/32 [00:07<00:29,  1.17s/it][1,0]<stderr>:#015 25%|██▌       | 8/32 [00:09<00:28,  1.19s/it][1,8]<stderr>:#015 25%|██▌       | 8/32 [00:09<00:28,  1.19s/it][1,0]<stderr>:#015 28%|██▊       | 9/32 [00:10<00:28,  1.24s/it][1,8]<stderr>:#015 28%|██▊       | 9/32 [00:10<00:28,  1.24s/it][1,0]<stderr>:#015 31%|███▏      | 10/32 [00:11<00:28,  1.31s/it][1,8]<stderr>:#015 31%|███▏      | 10/32 [00:11<00:28,  1.31s/it][1,0]<stderr>:#015 34%|███▍      | 11/32 [00:13<00:27,  1.31s/it][1,8]<stderr>:#015 34%|███▍      | 11/32 [00:13<00:27,  1.31s/it][1,0]<stderr>:#015 38%|███▊      | 12/32 [00:14<00:25,  1.30s/it][1,8]<stderr>:#015 38%|███▊      | 12/32 [00:14<00:25,  1.30s/it][1,0]<stderr>:#015 41%|████      | 13/32 [00:15<00:24,  1.28s/it][1,8]<stderr>:#015 41%|████      | 13/32 [00:15<00:24,  1.28s/it][1,0]<stderr>:#015 44%|████▍     | 14/32 [00:16<00:23,  1.28s/it][1,8]<stderr>:#015 44%|████▍     | 14/32 [00:16<00:23,  1.28s/it][1,0]<stderr>:#015 47%|████▋     | 15/32 [00:18<00:21,  1.27s/it][1,8]<stderr>:#015 47%|████▋     | 15/32 [00:18<00:21,  1.27s/it][1,8]<stderr>:#015 50%|█████     | 16/32 [00:19<00:20,  1.27s/it][1,0]<stderr>:#015 50%|█████     | 16/32 [00:19<00:20,  1.27s/it][1,0]<stderr>:#015 53%|█████▎    | 17/32 [00:20<00:18,  1.24s/it][1,8]<stderr>:#015 53%|█████▎    | 17/32 [00:20<00:18,  1.24s/it][1,0]<stderr>:#015 56%|█████▋    | 18/32 [00:21<00:16,  1.21s/it][1,8]<stderr>:#015 56%|█████▋    | 18/32 [00:21<00:16,  1.21s/it][1,0]<stderr>:#015 59%|█████▉    | 19/32 [00:23<00:15,  1.22s/it][1,8]<stderr>:#015 59%|█████▉    | 19/32 [00:23<00:15,  1.22s/it][1,0]<stderr>:#015 62%|██████▎   | 20/32 [00:24<00:14,  1.23s/it][1,8]<stderr>:#015 62%|██████▎   | 20/32 [00:24<00:14,  1.23s/it][1,0]<stderr>:#015 66%|██████▌   | 21/32 [00:25<00:13,  1.22s/it][1,8]<stderr>:#015 66%|██████▌   | 21/32 [00:25<00:13,  1.22s/it][1,0]<stderr>:#015 69%|██████▉   | 22/32 [00:26<00:12,  1.21s/it][1,8]<stderr>:#015 69%|██████▉   | 22/32 [00:26<00:12,  1.21s/it][1,0]<stderr>:#015 72%|███████▏  | 23/32 [00:27<00:10,  1.17s/it][1,8]<stderr>:#015 72%|███████▏  | 23/32 [00:27<00:10,  1.17s/it][1,0]<stderr>:#015 75%|███████▌  | 24/32 [00:28<00:09,  1.20s/it][1,8]<stderr>:#015 75%|███████▌  | 24/32 [00:28<00:09,  1.20s/it][1,0]<stderr>:#015 78%|███████▊  | 25/32 [00:30<00:08,  1.17s/it][1,8]<stderr>:#015 78%|███████▊  | 25/32 [00:30<00:08,  1.17s/it][1,0]<stderr>:#015 81%|████████▏ | 26/32 [00:31<00:07,  1.20s/it][1,8]<stderr>:#015 81%|████████▏ | 26/32 [00:31<00:07,  1.20s/it][1,0]<stderr>:#015 84%|████████▍ | 27/32 [00:32<00:05,  1.18s/it][1,8]<stderr>:#015 84%|████████▍ | 27/32 [00:32<00:05,  1.18s/it][1,0]<stderr>:#015 88%|████████▊ | 28/32 [00:33<00:04,  1.16s/it][1,8]<stderr>:#015 88%|████████▊ | 28/32 [00:33<00:04,  1.16s/it][1,0]<stderr>:#015 91%|█████████ | 29/32 [00:34<00:03,  1.15s/it][1,8]<stderr>:#015 91%|█████████ | 29/32 [00:34<00:03,  1.15s/it][1,0]<stderr>:#015 94%|█████████▍| 30/32 [00:35<00:02,  1.17s/it][1,8]<stderr>:#015 94%|█████████▍| 30/32 [00:35<00:02,  1.17s/it][1,0]<stderr>:#015 97%|█████████▋| 31/32 [00:37<00:01,  1.21s/it][1,8]<stderr>:#015 97%|█████████▋| 31/32 [00:37<00:01,  1.21s/it][1,0]<stderr>:#015100%|██████████| 32/32 [00:38<00:00,  1.17s/it][1,8]<stderr>:#015100%|██████████| 32/32 [00:38<00:00,  1.17s/it][1,0]<stderr>:#015100%|██████████| 32/32 [00:42<00:00,  1.34s/it]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:***** eval metrics *****\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  epoch                     =        3.0\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  eval_gen_len              =       20.0\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  eval_loss                 =     2.2528\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  eval_mem_cpu_alloc_delta  =       35MB\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  eval_mem_cpu_peaked_delta =        1MB\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  eval_mem_gpu_alloc_delta  =        0MB\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  eval_mem_gpu_peaked_delta =      467MB\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  eval_rouge1               =    42.8812\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  eval_rouge2               =    22.7851\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  eval_rougeL               =    37.8441\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  eval_rougeLsum            =    37.8592\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  eval_runtime              = 0:00:44.06\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  eval_samples              =       2000\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  eval_samples_per_second   =     45.385\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:#015100%|██████████| 32/32 [00:43<00:00,  1.35s/it][1,8]<stderr>:\u001b[0m\n",
      "\u001b[35m2021-12-01 09:42:46,301 sagemaker-training-toolkit INFO     Orted process exited\u001b[0m\n",
      "\u001b[35m2021-12-01 09:43:16,324 sagemaker-training-toolkit INFO     MPI process finished.\u001b[0m\n",
      "\u001b[35m2021-12-01 09:43:16,324 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2021-12-01 09:43:35 Uploading - Uploading generated training model\n",
      "2021-12-01 09:53:38 Completed - Training job completed\n",
      "Training seconds: 4130\n",
      "Billable seconds: 4130\n"
     ]
    }
   ],
   "source": [
    "huggingface_estimator.fit({'datasets':f's3://{bucket}/summarization/data/'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "215b50bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_test = pd.read_csv('data/test.csv')\n",
    "ref_summaries = list(df_test['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29328766",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = list(df_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96c6ea25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"  Consider the blow-up $X$ of $\\\\mathbb{P}^3$ at 6 points in very general position and the 15 lines through the 6 points. We construct an infinite-order pseudo-automorphism $\\\\phi_X$ on $X$, induced by the complete linear system of a divisor of degree 13. The effective cone of $X$ has infinitely many extremal rays and hence, $X$ is not a Mori Dream Space. The threefold $X$ has a unique anticanonical section which is a Jacobian K3 Kummer surface $S$ of Picard number 17. The restriction of $\\\\phi_X$ on $S$ realizes one of Keum's 192 infinite-order automorphisms of Jacobian K3 Kummer surfaces. In general, we show the blow-up of $\\\\mathbb{P}^n$ ($n\\\\geq 3$) at $(n+3)$ very general points and certain 9 lines through them is not Mori Dream, with infinitely many extremal effective divisors. As an application, for $n\\\\geq 7$, the blow-up of $\\\\overline{M}_{0,n}$ at a very general point has infinitely many extremal effective divisors. \""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2710ea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Works great but not easy to install. You have to adjust from the top and with the small space I had to stand on the toilet to get it to the right height then get my head between the wall/counter and the toilet .... Like I said it works great'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\"inputs\": texts[0], \"max_length\":10}\n",
    "predictor.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8200d130",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_summaries = []\n",
    "\n",
    "for i, text in enumerate(texts):\n",
    "    if i % 50 == 0:\n",
    "        print(i)\n",
    "    data = {\"inputs\": texts[0]}\n",
    "    candidate = predictor.predict(data)\n",
    "    candidate_summaries.append(candidate[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8d781624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rouge_scores(candidates, references):\n",
    "    result = metric.compute(predictions=candidates, references=references, use_stemmer=True)\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "df87d3b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 18.962463067641274,\n",
       " 'rouge2': 10.93389662527964,\n",
       " 'rougeL': 17.206193741062908,\n",
       " 'rougeLsum': 17.241895755727892}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_rouge_scores(candidate_summaries, ref_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f619a9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"model-summaries.txt\", \"w\")\n",
    "for s in candidate_summaries:\n",
    "    file.write(s + \"\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44f0753",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b90d25b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ed916fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir inference_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e8aea1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing inference_code/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile inference_code/inference.py\n",
    "\n",
    "# This is the script that will be used in the inference container\n",
    "import json \n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"\n",
    "    Load the model and tokenizer for inference \n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_dir).to(device).eval()\n",
    "    \n",
    "    model_dict = {'model':model, 'tokenizer':tokenizer}\n",
    "    \n",
    "    return model_dict \n",
    "\n",
    "\n",
    "def predict_fn(input_data, model_dict):\n",
    "    \"\"\"\n",
    "    Make a prediction with the model\n",
    "    \"\"\"\n",
    "    text = input_data.pop('inputs')\n",
    "    parameters_list = input_data.pop('parameters_list', None)\n",
    "    \n",
    "    tokenizer = model_dict['tokenizer']\n",
    "    model = model_dict['model']\n",
    "\n",
    "    # Parameters may or may not be passed    \n",
    "    input_ids = tokenizer(text, truncation=True, padding='longest', return_tensors=\"pt\").input_ids.to(device)\n",
    "    \n",
    "    if parameters_list:\n",
    "        predictions = []\n",
    "        for parameters in parameters_list:\n",
    "            output = model.generate(input_ids, **parameters)\n",
    "            predictions.append(tokenizer.batch_decode(output, skip_special_tokens=True))\n",
    "    else:\n",
    "        output = model.generate(input_ids)\n",
    "        predictions = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    \"\"\"\n",
    "    Transform the input request to a dictionary\n",
    "    \"\"\"\n",
    "    request = json.loads(request_body)\n",
    "\n",
    "    return request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29f78014",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c318dbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('database or disk is full')).History will not be written to the database.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-905847418383/huggingface-pytorch-training-2021-12-01-09-15-45-087/output/model.tar.gz'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_estimator.model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2612b3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_for_deployment = HuggingFaceModel(entry_point='inference.py',\n",
    "                                        source_dir='inference_code',\n",
    "                                        model_data='s3://sagemaker-us-east-1-905847418383/huggingface-pytorch-training-2021-12-01-09-15-45-087/output/model.tar.gz',\n",
    "                                        role=role,\n",
    "                                        pytorch_version='1.7.1',\n",
    "                                        py_version='py36',\n",
    "                                        transformers_version='4.6.1',\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a19bf907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------!"
     ]
    }
   ],
   "source": [
    "predictor = model_for_deployment.deploy(initial_instance_count=1,\n",
    "                                        instance_type='ml.g4dn.xlarge',\n",
    "                                        serializer=sagemaker.serializers.JSONSerializer(),\n",
    "                                        deserializer=sagemaker.deserializers.JSONDeserializer()\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e04a4e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The blow-up of $\\\\mathbb{P}^3$ at 6 points']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\"inputs\":texts[0], \"parameters_list\":[{\"min_length\": 5, \"max_length\": 20}]}\n",
    "predictor.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19893f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Birational geometry of blow-ups of projective spaces along points and   lines'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_summaries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4685b36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n"
     ]
    }
   ],
   "source": [
    "candidate_summaries = []\n",
    "\n",
    "for i, text in enumerate(texts):\n",
    "    if i % 50 == 0:\n",
    "        print(i)\n",
    "    data = {\"inputs\":text, \"parameters_list\":[{\"min_length\": 5, \"max_length\": 20}]}\n",
    "    candidate = predictor.predict(data)\n",
    "    candidate_summaries.append(candidate[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90db6e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"model-summaries.txt\", \"w\")\n",
    "for s in candidate_summaries:\n",
    "    file.write(s + \"\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "986e026d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from rouge_score) (1.21.4)\n",
      "Collecting absl-py\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "     |████████████████████████████████| 126 kB 16.9 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from rouge_score) (3.6.5)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from nltk->rouge_score) (8.0.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from nltk->rouge_score) (2021.11.2)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from nltk->rouge_score) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from nltk->rouge_score) (4.49.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from click->nltk->rouge_score) (4.8.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from importlib-metadata->click->nltk->rouge_score) (3.10.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from importlib-metadata->click->nltk->rouge_score) (3.6.0)\n",
      "\u001b[33mWARNING: Error parsing requirements for pyyaml: [Errno 2] No such file or directory: '/home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/PyYAML-6.0.dist-info/METADATA'\u001b[0m\n",
      "Installing collected packages: absl-py, rouge-score\n",
      "Successfully installed absl-py-1.0.0 rouge-score-0.0.4\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "200e3613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28045b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rouge_scores(candidates, references):\n",
    "    result = metric.compute(predictions=candidates, references=references, use_stemmer=True)\n",
    "    result = {key: round(value.mid.fmeasure * 100, 1) for key, value in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "551a37f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 44.5, 'rouge2': 24.5, 'rougeL': 39.4, 'rougeLsum': 39.4}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_rouge_scores(candidate_summaries, ref_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a553bae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n"
     ]
    }
   ],
   "source": [
    "candidate_summaries_topk = []\n",
    "\n",
    "for i, text in enumerate(texts):\n",
    "    if i % 50 == 0:\n",
    "        print(i)\n",
    "    data = {\"inputs\":text, \"parameters_list\":[{\"min_length\": 5, \"max_length\": 20, \"num_beams\": 50, \"top_p\": 0.9, \"do_sample\": True}]}\n",
    "    candidate = predictor.predict(data)\n",
    "    candidate_summaries_topk.append(candidate[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c84d316b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 31.0, 'rouge2': 20.0, 'rougeL': 29.2, 'rougeLsum': 29.1}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_rouge_scores(candidate_summaries_topk, ref_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2b7a08e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"model-summaries-top_p.txt\", \"w\")\n",
    "for s in candidate_summaries_topk:\n",
    "    file.write(s + \"\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db66b69f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2dff899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Didn’t work for me. Second brand I’ve tried',\n",
       " 'The graphics were not centered and placed more towards the handle than what the Amazon image',\n",
       " 'Just right for us, towels, first aid kit, tools, personal emergency toiletries',\n",
       " '3.5 STARS,... When you purchase instead of rent... you really want',\n",
       " 'Only 6 of the 12 lights were included. Only 6 of these lights are']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_summaries[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06f522d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting datasets\n",
      "  Downloading datasets-1.15.1-py3-none-any.whl (290 kB)\n",
      "     |████████████████████████████████| 290 kB 21.6 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from datasets) (4.8.2)\n",
      "Requirement already satisfied: dill in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from datasets) (0.3.4)\n",
      "Collecting huggingface-hub<1.0.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n",
      "     |████████████████████████████████| 59 kB 10.3 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from datasets) (3.7.4.post0)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from datasets) (1.3.4)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from datasets) (6.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from datasets) (1.21.4)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from datasets) (4.62.3)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from datasets) (0.70.12.2)\n",
      "Collecting fsspec[http]>=2021.05.0\n",
      "  Downloading fsspec-2021.11.0-py3-none-any.whl (132 kB)\n",
      "     |████████████████████████████████| 132 kB 30.5 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from datasets) (2.26.0)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from datasets) (21.2)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
      "     |████████████████████████████████| 243 kB 47.6 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.3.2)\n",
      "Collecting pyparsing<3,>=2.0.2\n",
      "  Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: chardet<5.0,>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from aiohttp->datasets) (3.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from aiohttp->datasets) (5.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/PyYAML-6.0.dist-info/METADATA'\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b5837a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: rouge_score in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (0.0.4)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from rouge_score) (1.21.4)\n",
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from rouge_score) (3.6.5)\n",
      "Requirement already satisfied: absl-py in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from rouge_score) (1.0.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from nltk->rouge_score) (8.0.3)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from nltk->rouge_score) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from nltk->rouge_score) (2021.11.2)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from nltk->rouge_score) (4.49.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from click->nltk->rouge_score) (4.8.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from importlib-metadata->click->nltk->rouge_score) (3.10.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from importlib-metadata->click->nltk->rouge_score) (3.6.0)\n",
      "\u001b[33mWARNING: Error parsing requirements for pyyaml: [Errno 2] No such file or directory: '/home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/PyYAML-6.0.dist-info/METADATA'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2139a84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ff3a1607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rouge_scores(candidates, references):\n",
    "    result = metric.compute(predictions=candidates, references=references, use_stemmer=True)\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8a78823e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d58ede4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('./model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "21e45df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained('./model/').to('cpu').eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b2606f7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"  Consider the blow-up $X$ of $\\\\mathbb{P}^3$ at 6 points in very general position and the 15 lines through the 6 points. We construct an infinite-order pseudo-automorphism $\\\\phi_X$ on $X$, induced by the complete linear system of a divisor of degree 13. The effective cone of $X$ has infinitely many extremal rays and hence, $X$ is not a Mori Dream Space. The threefold $X$ has a unique anticanonical section which is a Jacobian K3 Kummer surface $S$ of Picard number 17. The restriction of $\\\\phi_X$ on $S$ realizes one of Keum's 192 infinite-order automorphisms of Jacobian K3 Kummer surfaces. In general, we show the blow-up of $\\\\mathbb{P}^n$ ($n\\\\geq 3$) at $(n+3)$ very general points and certain 9 lines through them is not Mori Dream, with infinitely many extremal effective divisors. As an application, for $n\\\\geq 7$, the blow-up of $\\\\overline{M}_{0,n}$ at a very general point has infinitely many extremal effective divisors. \""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aacd4086",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(texts[0], truncation=True, padding='longest', return_tensors=\"pt\").input_ids.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4936b2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(input_ids)\n",
    "predictions = tokenizer.batch_decode(output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "49ae3035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The blow-up of $\\\\mathbb{P}^3$ at 6 points and certain 9 lines through   them is not Mori Dream, with infinitely many extremal effective divisors and applications to the blow-ups of $\\\\overline{M}_{0,n}$ at a very general point']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba49693",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cc2dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec06bb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05cbe123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-1-905847418383/huggingface-pytorch-training-2021-12-01-09-15-45-087/output/model.tar.gz to model/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://sagemaker-us-east-1-905847418383/huggingface-pytorch-training-2021-12-01-09-15-45-087/output/model.tar.gz model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1c6da57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tar -xf model/model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeda3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar -xvf model/model.tar.gz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p37",
   "language": "python",
   "name": "conda_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
