{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d4c5b44",
   "metadata": {},
   "source": [
    "# Part 3 - Training (aka *fine-tuning*) a Transformer model\n",
    "\n",
    "In this part we will finally train our very own Transformers model. We saw that the zer-shot model didn't produce great results, and that's probably because the model was trained on summarising news articles, not academic papers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5676581a",
   "metadata": {},
   "source": [
    "These lines of code are typical setup for Sagemaker, we require them for training jobs: https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c208b2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "print(f\"IAM role arn used for running training: {role}\")\n",
    "print(f\"S3 bucket used for storing artifacts: {sess.default_bucket()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d888348",
   "metadata": {},
   "source": [
    "We are in the great position that we don't have to write our own training script. Instead we will use a script from the transformers library in Github: https://github.com/huggingface/transformers/blob/v4.6.1/examples/pytorch/summarization/run_summarization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b660184f",
   "metadata": {},
   "outputs": [],
   "source": [
    "git_config = {'repo': 'https://github.com/huggingface/transformers.git','branch': 'v4.6.1'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e9fd86",
   "metadata": {},
   "source": [
    "These rae the parameters for training, and this is one of the most important levers we can leverage once we are in the experimentation phase. Changing these parameters can influence the model performance and there will be a component of trial & error to find the best model. Also check out https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html for automated hyperparameter tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda99089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={'per_device_train_batch_size': 4,\n",
    "                 'per_device_eval_batch_size': 4,\n",
    "                 'model_name_or_path': 'sshleifer/distilbart-cnn-12-6',\n",
    "                 'train_file': '/opt/ml/input/data/datasets/train.csv',\n",
    "                 'validation_file': '/opt/ml/input/data/datasets/val.csv',\n",
    "                 'do_train': True,\n",
    "                 'do_eval': True,\n",
    "                 'do_predict': False,\n",
    "                 'predict_with_generate': True,\n",
    "                 'output_dir': '/opt/ml/model',\n",
    "                 'num_train_epochs': 3,\n",
    "                 'learning_rate': 5e-5,\n",
    "                 'seed': 7,\n",
    "                 'fp16': True,\n",
    "                 'val_max_target_length': 20,\n",
    "                 'text_column': 'text',\n",
    "                 'summary_column': 'summary',\n",
    "                 }\n",
    "\n",
    "# configuration for running training on smdistributed Data Parallel\n",
    "# distribution = {'smdistributed':{'dataparallel':{ 'enabled': True }}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7e9b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point='run_summarization.py',\n",
    "    source_dir='./examples/pytorch/summarization',\n",
    "    git_config=git_config,\n",
    "    instance_type='ml.p3.16xlarge',\n",
    "    instance_count=1,\n",
    "    transformers_version='4.6',\n",
    "    pytorch_version='1.7',\n",
    "    py_version='py36',\n",
    "    role=role,\n",
    "    hyperparameters=hyperparameters,\n",
    "#     distribution=distribution,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f59de9b",
   "metadata": {},
   "source": [
    "This will kick off the training job which should take around 1 hour. There is also the option to use distributed training with more instances, see here:https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html. Running this training with 2 distributed instances should take ~40 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9d4588",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "huggingface_estimator.fit({'datasets':f's3://{bucket}/summarization/data/'}, wait=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p37",
   "language": "python",
   "name": "conda_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
